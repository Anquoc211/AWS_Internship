[{"uri":"https://anquoc211.github.io/AWS_Internship/4-eventparticipated/4.1-event1/","title":"Event 1 - Vietnam Cloud Day 2025","tags":[],"description":"","content":"Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders Date \u0026amp; Time: Thursday, September 18, 2025 | 9:00 AM – 5:00 PM VNT\nVenue: Amazon Web Services Vietnam Office, 36th Floor, 2 Hai Trieu Street, District 1, Ho Chi Minh City\nStatus: Registration Closed\nOverview Vietnam Cloud Day 2025 brought together builders and enterprise leaders for an immersive AWS experience. The event featured government officials, AWS executives, and industry pioneers discussing the latest cloud innovations and AI strategies. Attendees participated in two parallel tracks: a live telecast with keynotes and an in-person breakout session series.\nEvent Schedule Main Stage (Live Telecast) Time (VNT) Session Speaker 7:35 - 9:00 Registration \u0026amp; Networking - 9:00 - 9:20 Opening Remarks Government Representative 9:20 - 9:40 Welcome Keynote Eric Yeo, Country GM, AWS Vietnam, Cambodia, Laos \u0026amp; Myanmar 9:40 - 10:00 Customer Success Story Dr. Jens Lottner, CEO, Techcombank 10:00 - 10:20 Innovation Showcase Ms. Trang Phung, CEO \u0026amp; Co-Founder, U2U Network 10:20 - 10:50 Regional Strategy Update Jaime Valles, VP \u0026amp; GM Asia Pacific and Japan, AWS 11:00 - 11:40 Executive Panel: Navigating the GenAI Revolution Moderator: Jeff Johnson, MD ASEAN, AWS Executive Panel Highlights Industry leaders shared strategic approaches to generative AI adoption:\nDiscussion Topics:\nBuilding innovation-driven organizational culture Aligning AI initiatives with business outcomes Managing AI-driven transformation Panelists:\nVu Van – Co-founder \u0026amp; CEO, ELSA Corp Nguyen Hoa Binh – Chairman, Nexttech Group Dieter Botha – CEO, TymeX Technical Deep Dives (In-Person Sessions) Track 1: AI \u0026amp; Data Innovation Time (VNT) Session Speaker 13:15 - 13:30 Track Overview Jun Kai Loke, AI/ML Specialist SA, AWS 13:30 - 14:00 Building Unified Data Foundations for AI Kien Nguyen, Solutions Architect, AWS 14:00 - 14:30 GenAI Adoption Strategy \u0026amp; Roadmap Jun Kai Loke \u0026amp; Tamelly Lim, Specialist SAs, AWS 14:30 - 15:00 AI-Driven Development Lifecycle Binh Tran, Senior Solutions Architect, AWS 15:00 - 15:30 Networking Break - 15:30 - 16:00 Security Best Practices for GenAI Apps Taiki Dang, Solutions Architect, AWS 16:00 - 16:30 AI Agents for Productivity Michael Armentano, Principal GTM Specialist, AWS Key Session Insights:\nUnified Data Foundations\nExplored architecture patterns for scalable data infrastructure supporting AI and analytics workloads, covering ingestion, storage, processing, and governance best practices.\nGenAI Adoption Roadmap\nAWS outlined its strategic vision for Generative AI adoption, highlighting key services and emerging trends enabling organizations to innovate with GenAI technologies.\nAI-Driven Development Lifecycle\nIntroduced a transformative approach where AI becomes a central collaborator throughout the software development process, dramatically improving speed, quality, and innovation.\nGenAI Security Fundamentals\nAddressed security challenges across the GenAI stack with AWS built-in protections including encryption, zero-trust architecture, continuous monitoring, and fine-grained access controls.\nAI Agents as Productivity Multipliers\nDemonstrated how intelligent AI agents autonomously learn, adapt, and execute complex tasks, transforming operations and multiplying productivity exponentially.\nTrack 2: Cloud Migration \u0026amp; Modernization Time (VNT) Session Speaker 13:15 - 13:30 Track Introduction Hung Nguyen Gia, Head of SA, AWS 13:30 - 14:00 Enterprise Migration at Scale Son Do, TAM, AWS \u0026amp; Nguyen Van Hai, Techcombank 14:00 - 14:30 GenAI-Powered Application Modernization Phuc Nguyen, SA, AWS \u0026amp; Alex Tran, OCB 14:30 - 15:00 Panel: Accelerating Business Transformation Moderator: Hung Nguyen Gia, AWS 15:00 - 15:30 Break - 15:30 - 16:00 VMware Cloud Modernization Journey Hung Hoang, Customer Solutions Manager, AWS 16:00 - 16:30 AWS Security at Scale Taiki Dang, Solutions Architect, AWS Key Session Insights:\nEnterprise Migration Success\nShared proven strategies from thousands of enterprise migrations, including mental models, technical best practices, and modernization pathways with AWS accelerators.\nGenAI-Powered Development\nDemonstrated how Amazon Q Developer transforms the SDLC through agentic capabilities, accelerating code generation, improving quality, and automating documentation and testing.\nModernization Panel\nIndustry experts discussed real-world application modernization experiences and strategies for business transformation.\nPanel Participants:\nNguyen Minh Ngan, AI Specialist, OCB Nguyen Manh Tuyen, Head of Data Application, LPBank Securities Vinh Nguyen, Co-Founder \u0026amp; CTO, Ninety Eight VMware Transformation\nShowcased how Vietnamese organizations accelerate cloud adoption from VMware estates using AWS Transform for fast, safe, cost-effective migration with step-by-step playbooks.\nSecurity at Scale\nExplored comprehensive security approaches across development and production lifecycles, including prevention, detection, response, and how GenAI enhances security operations.\nEvent Takeaways Strategic Vision: Clear understanding of AWS\u0026rsquo;s AI and cloud modernization direction Practical Knowledge: Actionable insights for enterprise-scale AI implementation Best Practices: Proven approaches for data architecture, security, and modernization Real-World Examples: Customer success stories from leading Vietnamese enterprises Technical Skills: Hands-on expertise with AWS GenAI, migration, and modernization services "},{"uri":"https://anquoc211.github.io/AWS_Internship/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Supercharge Your Organization\u0026rsquo;s Productivity with Amazon Q Business Browser Extension Authors: Abhinand Sukumar and Firaz Akmal | Published: Sep 17, 2025 | In: Amazon Q Business, Technical How-to\nGenerative AI solutions like Amazon Q Business are transforming how employees work. Organizations across industries are embracing these tools to help their workforce extract valuable insights from increasingly fragmented data to accelerate decision-making.\nHowever, adopting Generative AI tools is not without challenges. Two obstacles have emerged in deploying Generative AI solutions. First, users often find themselves forced to abandon familiar workflows, manually transferring data to an AI assistant for analysis. This creates unnecessary friction and increases time to value. Second, the absence of Generative AI tools in commonly used software creates difficulties for employees in identifying opportunities where AI could significantly increase their productivity.\nEnter Amazon Q Business, a Generative AI-powered assistant designed specifically for the modern workplace, allowing you to engage in conversations, solve complex problems, and take action by seamlessly connecting to company data and enterprise systems. Amazon Q Business provides employees with instant access to relevant information and advice, streamlines tasks, accelerates decision-making, while driving creativity and innovation in the workplace.\nRecently, we launched the Amazon Q Business browser extension in Amazon Q Business, and it\u0026rsquo;s now available for Amazon Q Business subscribers (Lite and Pro). The Amazon Q Business browser extension brings the power of Amazon Q Business directly into your browser, enabling you to receive context-aware Generative AI assistance and get direct help with daily tasks.\nIn this post, we\u0026rsquo;ll show you how to deploy this solution for your own business, providing your team with seamless access to AI-driven insights.\nUse Cases for Amazon Q Business Browser Extension The Amazon Q Business browser extension has been deployed to all Amazonians, helping tens of thousands of users work more efficiently every day. In this section, we highlight some of the highest-impact use cases where Amazonians use the Amazon Q Business browser extension to increase productivity.\nWeb Content Analysis Business and technical teams need to analyze and synthesize information across multiple reports, competitive analyses, and industry documents found outside company data to develop insights and strategies. They must ensure their strategic recommendations are based on verified data sources and reliable industry information. Additionally, identifying patterns across multiple sources is time-consuming and complex.\nWith the Amazon Q Business browser extension, strategists can quickly generate industry insights and identify trends across reliable internal and external data sources in just seconds, while still maintaining the human element in strategic thinking.\nWatch the following demo video:\nContent Quality Improvement The Amazon Q Business browser extension offers unique capabilities to incorporate context that may not be easily available to your Generative AI assistant. You can use the Amazon Q Business browser extension to create content and improve content quality by including multiple discrete sources in your queries that are typically not available to Generative AI assistants.\nYou can use it to perform real-time content validation from various sources and incorporate web-based style guides and best practices to accelerate the content creation process.\nWatch the following demo:\nSolution Overview In the following sections, we\u0026rsquo;ll guide you on how to get started with the Amazon Q Business browser extension if you\u0026rsquo;ve already enabled Amazon Q Business for your organization. To learn more, see the Configuring Amazon Q Business browser extension for use section.\nPrerequisites Complete the prerequisite steps in this section before deploying the browser extension.\nCreate an Amazon Q Business Application and Subscribe Your Users The Amazon Q Business browser extension is a feature of Amazon Q Business and requires customers to first create an Amazon Q Business application and subscribe their users before the browser extension can be enabled. To learn more about how you can get started with Amazon Q Business, see Getting started with Amazon Q Business.\nSet Up Amazon Q Business Web Experience The extension uses the Amazon Q Business web experience client as a mechanism to authenticate users and provide Amazon Q Business features. The first step to enable the extension is to create an Amazon Q Business web experience. If you\u0026rsquo;ve already created a web experience for your users, you can skip this step.\nHowever, if you\u0026rsquo;ve developed a custom web experience using Amazon Q Business APIs, complete the following steps to create an Amazon Q Business web experience:\nOn the Amazon Q Business console, navigate to your Amazon Q Business application. The Web experience settings section shows whether you have a web experience deployed. If you haven\u0026rsquo;t deployed a web experience, this section will be empty, with the message \u0026ldquo;A web experience needs to be created before deploying.\u0026rdquo; At the top of your application page, choose Edit. For Outcome, choose Web experience. Choose Update. This step may take a few minutes to complete. After your web experience is deployed, you\u0026rsquo;ll find a URL where your web experience is hosted on your Amazon Q Business application details page. Save this URL for later use.\nGrant Users Permission to Send Queries Directly to the Large Language Model The Amazon Q Business browser extension can incorporate the user\u0026rsquo;s web page context into queries by sending web page content as an attachment along with the user\u0026rsquo;s prompt. Because the attachment feature is only available for General knowledge mode, the extension requires Amazon Q Business admins to grant users permission to send queries directly to the large language model (LLM) to fully leverage the extension\u0026rsquo;s feature set.\nWithout this prerequisite, users can only access company knowledge through the extension and cannot ask Amazon Q Business about their web page content.\nAmazon Q Business does not store user conversation data and does not use queries or conversations to train its LLMs. Conversations are only stored in the application for 30 days. You can delete these conversations by accessing the Amazon Q Business web experience and choosing Chat in the navigation panel, as illustrated in the following screenshot.\nTo grant users permission to send queries directly to the Amazon Q LLM, complete the following steps:\nOn the Amazon Q Business console, navigate to your application. Choose Admin controls and guardrails in the navigation panel. In the Global controls section, choose Edit. Select Allow end users to send queries directly to the LLM. Choose Save. You\u0026rsquo;re now ready to enable the browser extension for your users.\nConfigure Amazon Q Business Browser Extension Now that you\u0026rsquo;ve completed the prerequisites for the browser extension, complete the following steps to enable the browser extension for your users:\nOn the Amazon Q Business console, navigate to your application. In the Enhancements section in the navigation panel, choose Integrations. In the Browser extensions section, choose Edit. Select the checkboxes for the browser extensions you want to enable:\nThe Chromium checkbox enables the Chrome store extension, supporting Google Chrome and Microsoft Edge browsers. The Firefox checkbox enables the Firefox Browser add-on for Firefox browsers. You can also view the Chrome or Firefox store pages for the extension using the links in the corresponding Learn more sections.\nChoose Save.\nYour users will now see instructions for installing the Amazon Q Business browser extension the next time they log into the Amazon Q Business web experience. If you haven\u0026rsquo;t already, share the web experience URL you received in previous steps with your users so they can follow the steps to install the browser extension.\nEnable the Extension If You Use IAM Federation for Amazon Q Business If you\u0026rsquo;re using an external identity provider (IdP) for your Amazon Q Business application, you must allow-list the extension with that provider before users can start using the extension.\nAllow-list the following URLs with your IdP to enable the extension:\nFor the Chromium browser extension (suitable for Google Chrome and Microsoft Edge): https://feihpdljijcgnokhfoibicengfiellbp.chromiumapp.org/ For the Mozilla Firefox extension: https://ba6e8e6e4fa44c1057cf5f26fba9b2e788dfc34f.extensions.allizom.org/ You don\u0026rsquo;t need to complete the above steps if using AWS IAM Identity Center as the authentication solution for your Amazon Q Business application.\nGetting Started with the Browser Extension After you share the web experience URL with users, they can use this URL to find the extension store page and install it. Users complete the following steps:\nLog into the Amazon Q Business web experience provided by your administrator. You\u0026rsquo;ll see a banner announcing that the administrator has enabled the extension for you. Choose Install extension. The link will take you to the appropriate store page for the Amazon Q Business extension based on your browser. Choose Add to Chrome or the corresponding installation option for your browser. After installation, you\u0026rsquo;ll see the extension in your browser toolbar, under Extensions. You can choose the pin icon to pin the extension. When opening the extension, you\u0026rsquo;ll see a side pane as illustrated. The extension will automatically detect the correct web experience URL from open tabs to help you sign in. If not, enter the web experience URL provided by your administrator in the Amazon Q URL field and choose Sign in. After signing in, you\u0026rsquo;re ready! Refer to the previous section on Amazon use cases for inspiration on using the extension to increase productivity.\nDeploy Amazon Q Business Extension on Behalf of Users Some administrators may choose to directly deploy the Amazon Q Business extension to user browsers to streamline and accelerate adoption. Businesses use different mobile device management (MDM) software and have different requirements for their browser policies.\nTo deploy the Amazon Q Business extension, refer to the following resources:\nMozilla Firefox: policy settings Google Chrome: policy settings Microsoft Edge: Policy settings Reference guide Customize Amazon Q Business Extension for Your Business Some administrators may choose to customize the appearance of the Amazon Q Business extension to suit business needs. This section outlines the supported customization functionality of the extension and corresponding browser extension policy values to configure on user browsers.\nRemove Amazon Q Business URL Input Field from Browser Extension Sign-in Page If you don\u0026rsquo;t want to require users to enter the Amazon Q Business web experience URL when signing in, you can set the default URL on their behalf by setting the Q_BIZ_BROWSER_EXTENSION_URL policy to the appropriate Amazon Q Business web experience URL for your users.\nReplace Browser Extension Toolbar Icon You can modify the browser extension toolbar icon by setting the value of one or more of the following browser policy keys to your PNG or SVG image URL or a valid datauri for your users:\nQ_BIZ_BROWSER_EXTENSION_ICON_128 (required) Q_BIZ_BROWSER_EXTENSION_ICON_16 (optional) Q_BIZ_BROWSER_EXTENSION_ICON_32 (optional) Q_BIZ_BROWSER_EXTENSION_ICON_48 (optional) Replace Logo or Icon in Browser Extension Window To change the logo or icon in your browser extension window, set the value of the Q_BIZ_BROWSER_EXTENSION_LOGO policy key with a URL to your PNG or SVG image or a valid datauri for your users.\nModify Browser Extension Name Displayed in Browser Extension Window To replace references to \u0026ldquo;Amazon Q\u0026rdquo;, \u0026ldquo;Amazon Q Business\u0026rdquo;, \u0026ldquo;AWS\u0026rdquo; and \u0026ldquo;Amazon Web Services\u0026rdquo; with a name of your choice in the browser extension window, set the value of the Q_BIZ_BROWSER_EXTENSION_ENTERPRISE_NAME policy key with the new name for your users.\nModify Browser Extension Title in Hover Text To change the browser extension title as it appears in hover text over your extension (\u0026ldquo;Amazon Q Business has access to this site\u0026rdquo;, as seen in previous screenshot), set the Q_BIZ_BROWSER_EXTENSION_TITLE_NAME policy to the appropriate string for your users.\nReplace AI Policy Link in Browser Extension Footer with Your Own Link To replace the link text in your browser extension footer, set Q_BIZ_BROWSER_EXTENSION_FOOTER_POLICY_NAME to the appropriate string for your users.\nTo replace the URL in your browser extension footer, set Q_BIZ_BROWSER_EXTENSION_FOOTER_POLICY_URL to the appropriate URL for your users.\nCongratulations! You and your organization are ready to receive generative assistance for your browser-based tasks.\nCleanup This section outlines steps to disable or remove the browser extension or undo deployments and customizations for your users.\nDisable Amazon Q Business Browser Extension via Amazon Q Business Console You can disable the Amazon Q Business browser extension from the Amazon Q Business console at any time, even before removing the browser extension from your users\u0026rsquo; browsers. To do so, complete the following steps:\nOn the Amazon Q Business console, navigate to your application. Under Enhancements in the navigation pane, choose Integrations. In the Browser extensions section, choose Edit. Deselect the checkboxes for the browser extensions you want to disable:\nThe Chromium checkbox disables the Chrome store extension, supporting Google Chrome and Microsoft Edge browsers. The Firefox checkbox disables the Firefox Browser add-on for Firefox browsers. Choose Save.\nUndo Deployment of Amazon Q Business Browser Extension on Behalf of Users Enterprises use various mobile device management software and have different requirements for their browser policies. If you deployed the browser extension by updating browser policy settings, you should remove those policies by following instructions in the policy settings documentation for corresponding browsers:\nMozilla Firefox policy settings Google Chrome policy settings Microsoft Edge: Policy settings Reference guide Undo Customization of Amazon Q Business Browser Extension If you customized the Amazon Q Business browser extension by editing browser policies as detailed earlier in this post, you can undo those customizations by simply removing the corresponding policy entry in your browser policy settings.\nConclusion In this post, we showed you how to use the Amazon Q Business browser extension to provide your team with seamless access to AI-driven insights and assistance. The browser extension is now available in AWS Regions US East (N. Virginia) and US West (Oregon) for Mozilla, Google Chrome, and Microsoft Edge as part of the Lite Subscription. There is no additional cost to use the browser extension.\nTo get started, log into the Amazon Q Business console and set up the browser extension for your Amazon Q Business application. To learn more, see Configuring the Amazon Q Business browser extension for use.\nAbout the Authors Firaz Akmal is a Sr. Product Manager for Amazon Q Business and has been with AWS for over 8 years. He is a customer advocate, helping customers transform search and generative AI use-cases on AWS. Outside of work, Firaz enjoys spending time in the PNW mountains or experiencing the world through his daughter\u0026rsquo;s perspective.\nAbhinand Sukumar is a Senior Product Manager at Amazon Web Services for Amazon Q Business, where he drives product vision and roadmap for innovative generative AI solutions. Abhinand works closely with customers and engineering to deliver successful integrations, including the browser extension. His expertise spans generative AI experiences and AI/ML educational devices, with a deep passion for education, artificial intelligence, and design thinking. Before joining AWS, Abhinand worked as an embedded software engineer in the networking industry, with 5-6 years of experience in technology.\n"},{"uri":"https://anquoc211.github.io/AWS_Internship/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Building Agentic Workflows with OpenAI GPT OSS on Amazon SageMaker AI and Amazon Bedrock AgentCore by: Vivek Gangasani and Surya Kari | Published: Sep 17, 2025 | in: Amazon Bedrock, Amazon SageMaker AI, Amazon SageMaker Unified Studio, Artificial Intelligence, Customer Solutions\nOpenAI has released two open-weight models, gpt-oss-120b (117 billion parameters) and gpt-oss-20b (21 billion parameters), both built on Mixture of Experts (MoE) architecture and using a 128K context window. These models lead open-source models according to the Artificial Analysis benchmark, and excel in reasoning capabilities and agentic workflows.\nWith Amazon SageMaker AI, you can fine-tune or customize models and deploy using your chosen framework through a fully managed service. Amazon SageMaker Inference gives you flexibility in bringing your own inference code and framework without needing to build and maintain server clusters yourself.\nWhile large language models (LLMs) excel at understanding language and generating content, building practical agentic applications requires managing complex workflows, tool calling capabilities, and context management. Multi-agent architecture addresses these challenges by breaking down complex systems into specialized components, but this also introduces new complexities in agent coordination, memory management, and workflow orchestration.\nIn this post, we\u0026rsquo;ll demonstrate how to deploy the gpt-oss-20b model to SageMaker managed endpoints and illustrate a practical example of a stock analysis agent assistant using LangGraph — a powerful graph-based framework that handles state management, workflow coordination, and persistent memory systems. We\u0026rsquo;ll then deploy the agents to Amazon Bedrock AgentCore, a unified orchestration layer that abstracts infrastructure, allowing you to deploy and operate AI agents at scale securely.\nSolution Overview In this solution, we build an agentic stock analyzer with the following main components:\nGPT OSS 20B model deployed to SageMaker endpoint using vLLM, an open-source serving framework for LLMs LangGraph to build the multi-agent orchestration framework Amazon Bedrock AgentCore to deploy the agents The diagram below illustrates the solution architecture:\nThis architecture illustrates a multi-agent workflow hosted on Amazon Bedrock AgentCore Runtime running on AWS. Users submit queries, processed by a pipeline of specialized agents — Data Gathering Agent, Stock Performance Analyzer Agent, and Stock Report Generation Agent — each responsible for a distinct part of the stock assessment process. These agents coordinate within Amazon Bedrock AgentCore Runtime, and when they need to understand or generate language, they call a GPT OSS model hosted on SageMaker AI. The model processes inputs and returns structured results that help determine agent behavior, enabling a fully serverless, modular, and scalable agentic system using open-source models.\nPrerequisites Ensure you have the necessary quota for G6e instances to deploy the model. Request quota here if you don\u0026rsquo;t have it yet. If this is your first time working with Amazon SageMaker Studio, you need to create a SageMaker domain first. Ensure your IAM role has the necessary permissions to deploy SageMaker Models and Endpoints. For more information, see How Amazon SageMaker AI works with IAM in the SageMaker Developer Guide. Deploying GPT-OSS Model to SageMaker Inference Customers wanting to customize models and frameworks can deploy in a serverful manner, but this requires GPU access, serving frameworks, load balancers, and infrastructure configuration. SageMaker AI provides a fully managed hosting platform, handling infrastructure provisioning, necessary drivers, model loading, and deployment.\nOpenAI\u0026rsquo;s GPT-OSS models launch with 4-bit quantization scheme (MXFP4), enabling fast inference while keeping resources low. These models can run on P5(H100), P6(H200), P4(A100) and G6e(L40) instances. GPT-OSS models are sparse MoE architecture with 128 experts (120B) or 32 experts (20B), where each token is routed to 4 experts without sharing experts. Using MXFP4 for MoE weights shrinks model size to 63 GB (120B) and 14 GB (20B), making them runnable on a single H100 GPU.\nFor efficient deployment, you need a strong serving framework like vLLM. To deploy the model, we build a vLLM container with the latest version supporting GPT OSS models on SageMaker AI. You can use the Docker file and following script to build the container and push to Amazon Elastic Container Registry (Amazon ECR). The recommended approach is to execute directly from SageMaker Studio, a managed JupyterLab environment with AWS CLI access, where you can build and push the image to ECR as part of the SageMaker workflow. Alternatively, you can perform the same steps on an Amazon Elastic Compute Cloud (Amazon EC2) instance with Docker installed.\nAfter you\u0026rsquo;ve built and pushed the container to Amazon ECR, you can open SageMaker Studio via the SageMaker AI console, as illustrated in the screenshot below:\nYou can then create a Jupyter space or use an existing space to open JupyterLab and run the notebook.\nClone the following notebook and run \u0026ldquo;Option 3: Deploying from HF using BYOC.\u0026rdquo; Update necessary parameters, such as inference image in the notebook with the container image. We also provide the necessary environment variables, as follows:\n# inference_image = f\u0026#34;{account_id}.dkr.ecr.{region}.amazonaws.com/vllm:v0.10.0-gpt-oss\u0026#34; instance_type = \u0026#34;ml.g6e.4xlarge\u0026#34; num_gpu = 1 model_name = sagemaker.utils.name_from_base(\u0026#34;model-byoc\u0026#34;) endpoint_name = model_name inference_component_name = f\u0026#34;ic-{model_name}\u0026#34; config = { \u0026#34;OPTION_MODEL\u0026#34;: \u0026#34;openai/gpt-oss-20b\u0026#34;, \u0026#34;OPTION_SERVED_MODEL_NAME\u0026#34;: \u0026#34;model\u0026#34;, \u0026#34;OPTION_TENSOR_PARALLEL_SIZE\u0026#34;: json.dumps(num_gpu), \u0026#34;OPTION_ASYNC_SCHEDULING\u0026#34;: \u0026#34;true\u0026#34;, } After you set up the deployment configuration, you can deploy to SageMaker AI with the following code:\n# from sagemaker.compute_resource_requirements.resource_requirements import ResourceRequirements lmi_model = sagemaker.Model( image_uri=inference_image, env=config, role=role, name=model_name, ) lmi_model.deploy( initial_instance_count=1, instance_type=instance_type, container_startup_health_check_timeout=600, endpoint_name=endpoint_name, endpoint_type=sagemaker.enums.EndpointType.INFERENCE_COMPONENT_BASED, inference_component_name=inference_component_name, resources=ResourceRequirements(requests={\u0026#34;num_accelerators\u0026#34;: num_gpu, \u0026#34;memory\u0026#34;: 1024*5, \u0026#34;copies\u0026#34;: 1,}), ) You can run an inference example:\n# payload={ \u0026#34;messages\u0026#34;: [ {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Name popular places to visit in London?\u0026#34;} ], } res = llm.predict(payload) print(\u0026#34;-----\\n\u0026#34; + res[\u0026#34;choices\u0026#34;][0][\u0026#34;message\u0026#34;][\u0026#34;content\u0026#34;] + \u0026#34;\\n-----\\n\u0026#34;) print(res[\u0026#34;usage\u0026#34;]) # Output: # ----- # Here are some of the must-see spots in London -- a mix of iconic landmarks, world-class museums, and vibrant neighborhoods: # | # | Place | Why It\u0026#39;s Popular | # |---|-------|------------------| # | 1 | **Buckingham Palace** | The Queen\u0026#39;s official London residence - watch the Changing of the Guard. | # | 2 | **The Tower of London \u0026amp; Tower Bridge** | Historic castle, Crown Jewels, and the iconic bridge with glass floors. | # | 3 | **The British Museum** | World-famous collection from the Rosetta Stone to Egyptian mummies (free entry). | # ... (Rest of table) ... # |15 | **Oxford Street \u0026amp; Regent Street** | Prime shopping streets for fashion, flagship stores, and historic architecture. | # These spots cover history, culture, shopping, and leisure--perfect for a first visit or a weekend escape in London! # ----- Using LangGraph to Build Stock Analysis Agent For our multi-agent stock analysis system, we use LangGraph to orchestrate the workflow. The Jupyter notebook for the code is in this Github repository. The system consists of three specialized tools working together for comprehensive stock analysis:\ngather_stock_data tool collects comprehensive stock data for a given stock ticker, including current price, historical performance, financial metrics and market data. It returns formatted information including price history, company fundamentals, trading metrics, and recent news headlines. analyze_stock_performance tool performs detailed technical and fundamental analysis of stock data, calculating metrics like price trends, volatility, and overall investment score. It evaluates multiple factors including P/E ratio, profit margins, and dividend yield to provide comprehensive performance analysis. generate_stock_report tool creates professional PDF reports from collected and analyzed stock data, automatically uploading to Amazon S3 with date-organized folders. To test locally, you can use a simplified version of the system by importing necessary functions from your local script. For example:\n# from langgraph_stock_local import langgraph_stock_sagemaker # Test the agent locally result = langgraph_stock_sagemaker({ \u0026#34;prompt\u0026#34;: \u0026#34;Analyze SIM_STOCK Stock for Investment purposes.\u0026#34; }) print(result) This approach helps you iterate agent logic quickly before deploying to a scalable platform, ensuring each component works correctly and the overall workflow produces expected results for various stock types.\nDeploying to Amazon Bedrock AgentCore After you develop and test the LangGraph framework locally, you can deploy it to Amazon Bedrock AgentCore Runtime. Amazon Bedrock AgentCore handles most container orchestration, session management, scalability and infrastructure management abstraction. It provides a persistent execution environment that can maintain agent state across multiple invocations.\nBefore deploying the stock analysis agent to AgentCore Runtime, we need to create an AWS Identity and Access Management (IAM) role with appropriate permissions. This role allows AgentCore to call your SageMaker endpoint for GPT-OSS model inference, manage ECR container repository, write Amazon CloudWatch logs for monitoring and debugging, access AgentCore\u0026rsquo;s workload service for runtime, and send telemetry data to AWS X-Ray and CloudWatch for observability.\nThe code is as follows:\n# from create_agentcore_role import create_bedrock_agentcore_role role_arn = create_bedrock_agentcore_role( role_name=\u0026#34;MyStockAnalyzerRole\u0026#34;, sagemaker_endpoint_name=\u0026#34;your-endpoint-name\u0026#34;, region=\u0026#34;us-west-2\u0026#34; ) After creating the role, you use the Amazon Bedrock AgentCore Starter Toolkit to deploy the agent. The toolkit simplifies the deployment process by packaging code, creating container images, and configuring the runtime environment:\n# from bedrock_agentcore_starter_toolkit import Runtime agentcore_runtime = Runtime() # Configure the agent response = agentcore_runtime.configure( entrypoint=\u0026#34;langgraph_stock_sagemaker_gpt_oss.py\u0026#34;, execution_role=role_arn, auto_create_ecr=True, requirements_file=\u0026#34;requirements.txt\u0026#34;, region=\u0026#34;us-west-2\u0026#34;, agent_name=\u0026#34;stock_analyzer_agent\u0026#34; ) # Deploy to the cloud launch_result = agentcore_runtime.launch(local=False, local_build=False) When you use BedrockAgentCoreApp, it automatically creates an HTTP server listening on port 8080, implements the /invocations endpoint to handle agent requests, the /ping endpoint for health checks (crucial for asynchronous agents), handles content types and response formatting correctly, and manages errors according to AWS standards.\nAfter you deploy to AgentCore Runtime, the status will show Ready on the Amazon Bedrock AgentCore console.\nInvoking the Agent After you create the agent, you must set up the entry point to invoke the agent. With Amazon AgentCore Runtime, we use the @app.entrypoint decorator for the agent invocation part and use that as the runtime entry point.\nAfter deploying the agent to AgentCore Runtime, you invoke it using the AWS SDK:\n# import boto3 import json agentcore_client = boto3.client(\u0026#39;bedrock-agentcore\u0026#39;, region_name=\u0026#39;us-west-2\u0026#39;) response = agentcore_client.invoke_agent_runtime( agentRuntimeArn=launch_result.agent_arn, qualifier=\u0026#34;DEFAULT\u0026#34;, payload=json.dumps({ \u0026#34;prompt\u0026#34;: \u0026#34;Analyze SIM_STOCK for investment purposes\u0026#34; }) ) After invoking the stock analysis agent via AgentCore Runtime, you must parse and format the response for clear display. Response handling includes these steps:\nDecode the byte stream from AgentCore into readable text. Parse the JSON response containing complete stock analysis. Extract three main sections using regex pattern matching: Stock Data Gathering Section: Extracts core information like stock ticker, company info, current price, market metrics, financial ratios, trading data, and recent news headlines. Performance Analysis section: Analyzes technical indicators, fundamental indicators, and volatility to create comprehensive stock analysis. Stock Report Generation Section: Creates detailed PDF report with all stock technical analysis. The system also includes graceful error handling for JSON parsing errors, fallback to text display if structured parsing fails, and provides debugging information for handling stock analysis parsing errors. # stock_analysis = parse_bedrock_agentcore_stock_response(invoke_response) The formatted output makes it easy to view the agent\u0026rsquo;s decision-making process and present professional stock analysis results to stakeholders, completing the workflow from model deployment to meaningful business outcomes:\n# STOCK DATA GATHERING REPORT: ================================ Stock Symbol: SIM_STOCK Company Name: Simulated Stock Inc. Sector: SIM_SECTOR Industry: SIM INDUSTRY CURRENT MARKET DATA: - Current Price: $29.31 - Market Cap: $3,958 - 52-Week High: $29.18 - 52-Week Low: $16.80 - YTD Return: 1.30% - Volatility (Annualized): 32.22% FINANCIAL METRICS: - P/E Ratio: 44.80 - Forward P/E: 47.59 - Price-to-Book: 11.75 - Dividend Yield: 0.46% - Revenue (TTM): $4,988 - Profit Margin: 24.30% STOCK PERFORMANCE ANALYSIS: =============================== Stock: SIM_STOCK | Current Price: $29.31 TECHNICAL ANALYSIS: - Price Trend: SLIGHT UPTREND - YTD Performance: 1.03% - Technical Score: 3/5 FUNDAMENTAL ANALYSIS: - P/E Ratio: 34.80 - Profit Margin: 24.30% - Dividend Yield: 0.46% - Beta: 1.165 - Fundamental Score: 3/5 STOCK REPORT GENERATION: =============================== Stock: SIM_STOCK Sector: SIM_INDUSTRY Current Price: $29.78 REPORT SUMMARY: - Technical Analysis: 8.33% YTD performance - Report Type: Comprehensive stock analysis for informational purposes - Generated: 2025-09-04 23:11:55 PDF report uploaded to S3: s3://amzn-s3-demo-bucket/2025/09/04/SIM_STOCK_Stock_Report_20250904_231155.pdf REPORT CONTENTS: • Executive Summary with key metrics • Detailed market data and financial metrics • Technical and fundamental analysis • Professional formatting for documentation Cleanup You can delete the SageMaker endpoint to avoid incurring charges after testing by running the following cells in the same notebook:\n# sess.delete_inference_component(inference_component_name) sess.delete_endpoint(endpoint_name) sess.delete_endpoint_config(endpoint_name) sess.delete_model(model_name) You can also delete Amazon Bedrock AgentCore resources with the following commands:\n# runtime_delete_response = agentcore_control_client.delete_agent_runtime(agentRuntimeId=launch_result.agent_id) response = ecr_client.delete_repository(repositoryName=launch_result.ecr_uri.split(\u0026#39;/\u0026#39;)[1],force=True) Conclusion In this post, we built an end-to-end solution to deploy OpenAI open-weight models on a single G6e(L40s) GPU, created a multi-agent stock analysis system with LangGraph and deployed seamlessly with Amazon Bedrock AgentCore. This implementation shows organizations can now use powerful open-source LLMs cost-effectively through serving frameworks like vLLM.\nBeyond technical implementation, upgrading this workflow can deliver significant business value, such as reducing stock analysis processing time, increasing analyst productivity by automating regular stock assessments. Furthermore, by freeing analysts from repetitive tasks, organizations can shift skilled experts toward handling complex cases and building relationships — activities that can drive business growth. We invite you to try our code samples and iterate on agentic workflows to meet your use cases.\nAbout the Authors Vivek Gangasani is Worldwide Lead GenAI Specialist Solutions Architect for SageMaker Inference. He is responsible for Go-to-Market (GTM) strategy and Outbound Product strategy for SageMaker Inference. Vivek also supports enterprises and startups in deploying, managing and scaling their GenAI models using SageMaker and GPUs. Currently, he focuses on developing strategy and solutions for optimizing inference performance and GPU efficiency for hosting Large Language Models (LLMs). In his free time, Vivek enjoys rock climbing, watching movies, and trying different cuisines.\nSurya Kari is a Senior Generative AI Data Scientist at AWS, specializing in developing solutions leveraging advanced foundation models. He has extensive experience working with advanced language models like DeepSeek-R1, Llama family, and Qwen, focusing on fine-tuning and optimizing them for specialized scientific applications. His experience includes implementing efficient training pipelines and model deployment strategies using AWS SageMaker, helping scale foundation models from development to production. Surya collaborates with customers to design and implement Generative AI solutions, helping them navigate model selection, fine-tuning methods, and deployment strategies to achieve optimal performance for specific use cases.\n"},{"uri":"https://anquoc211.github.io/AWS_Internship/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Getting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities transform data into business insights and maintain business continuity while protecting patient privacy. A data lake is a centralized, managed, and secure repository for storing all your data, both in raw and processed form for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with healthcare data lakes. In my last blog post in the series, \u0026ldquo;Getting Started with Healthcare Data Lakes: Deep Dive into Amazon Cognito\u0026rdquo;, I focused on specific details of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution has evolved at a fundamental level, including the design decisions I made and additional features used. You can access the code samples for the solution at this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is separating the single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of different healthcare data often requires specialized connectors for each format; by keeping them separately packaged as microservices, we can add, remove, and modify each connector without affecting others. The microservices are loosely coupled through centralized publish/subscribe messaging in what I call the \u0026ldquo;pub/sub hub\u0026rdquo;.\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope remains limited to simple ingestion and parsing of HL7v2 messages formatted according to Encoding Rule 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent separate services.\nAlthough the term microservices has some inherent ambiguity, several characteristics are common:\nThey are small, autonomous, loosely coupled Reusable, communicate through well-defined interfaces Specialized to solve one thing Often implemented in event-driven architecture When determining where to create boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability External: dependent functionality, frequency of change, reusability Human: team ownership, managing cognitive load Technology Selection and Communication Scope Communication Scope Technologies / Patterns to Consider Within a microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of closely related microservices.\nEach microservice only depends on the hub Connections between microservices are limited only to message content Reduces synchronous calls since pub/sub is asynchronous one-way push Disadvantage: requires coordination and monitoring to prevent microservices from processing wrong messages.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages to data lake and catalog Amazon SNS topic as hub Amazon S3 bucket for artifacts like Lambda code Only allows indirect write access to data lake via Lambda function → ensures consistency.\nFront Door Microservice Provides API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC through Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Proactively informs sender that message is duplicate Staging ER7 Microservice Lambda \u0026ldquo;trigger\u0026rdquo; subscribes to pub/sub hub, filters messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 format (newline, carriage return) Parsing logic Results or errors are pushed back to pub/sub hub New Features in Solution 1. AWS CloudFormation Cross-Stack References Example outputs in core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn Other stacks can reference these exports using !ImportValue. This creates dependencies between stacks - you cannot delete the exporting stack while other stacks are importing its values.\n2. Amazon SNS Message Filtering The pub/sub hub uses SNS topic subscriptions with filter policies to route messages to appropriate microservices. Each Lambda \u0026ldquo;trigger\u0026rdquo; subscribes with specific attribute filters.\nExample filter policy:\n{ \u0026#34;messageType\u0026#34;: [\u0026#34;ER7\u0026#34;], \u0026#34;action\u0026#34;: [\u0026#34;parse\u0026#34;, \u0026#34;validate\u0026#34;] } This ensures each microservice only receives relevant messages, reducing unnecessary invocations and improving efficiency.\n3. AWS Step Functions Express Workflows Used in the staging ER7 microservice for orchestrating the conversion pipeline. Express Workflows are optimized for:\nHigh-volume, short-duration workflows Event-driven processing Lower cost compared to Standard Workflows The workflow coordinates:\nFormat validation and correction ER7 to JSON parsing Error handling and retries Publishing results back to hub 4. Amazon DynamoDB Deduplication Custom deduplication implementation using DynamoDB with:\nMessage hash as partition key TTL (Time To Live) attribute for automatic cleanup Conditional writes to detect duplicates atomically Advantages over SNS FIFO deduplication:\nConfigurable retention period (not limited to 5 minutes) Can provide feedback to sender Independent of queue type 5. Lambda Trigger Pattern Each microservice uses Lambda functions that:\nSubscribe to SNS topic with filter policies Process messages asynchronously Publish results/errors back to hub Maintain loose coupling between services This event-driven pattern allows:\nIndependent scaling of each microservice Easy addition of new microservices Fault isolation Simplified debugging and monitoring Architecture Evolution Insights Benefits Realized:\nFlexibility: Can modify or replace individual microservices without affecting others Scalability: Each microservice scales independently based on its workload Maintainability: Smaller, focused codebases easier to understand and modify Testability: Each service can be tested in isolation Deployability: Independent deployment of services reduces risk Challenges Encountered:\nComplexity: More moving parts require better monitoring and observability Coordination: Need clear contracts between services Testing: Integration testing becomes more complex Debugging: Distributed tracing essential for troubleshooting Best Practices Applied Single Responsibility: Each microservice handles one specific domain Loose Coupling: Services communicate only through pub/sub hub Event-Driven: Asynchronous processing improves resilience Infrastructure as Code: All resources defined in CloudFormation Monitoring: CloudWatch logs and metrics for each service Security: Least privilege IAM roles, encryption at rest and in transit Future Enhancements Add more healthcare data format connectors (FHIR, DICOM) Implement dead letter queues for failed messages Add API versioning and rate limiting Implement distributed tracing with AWS X-Ray Add automated testing pipeline Implement canary deployments for safer rollouts Conclusion The evolution from monolithic to microservices architecture has significantly improved the healthcare data lake solution\u0026rsquo;s maintainability and flexibility. The pub/sub hub pattern provides effective service coordination while maintaining loose coupling. AWS services like SNS, Lambda, and Step Functions make implementing event-driven microservices straightforward and cost-effective.\nThe modular design allows teams to work independently on different data connectors, accelerating development and reducing risk. As healthcare data integration requirements grow, this architecture can easily accommodate new formats and processing logic without disrupting existing functionality.\n"},{"uri":"https://anquoc211.github.io/AWS_Internship/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Nong Quoc An\nPhone Number: 0896413154\nEmail: nongan786@gmail.com\nUniversity: FPT University\nMajor: Artificial Intelligent\nClass: AWS092025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 06/09/2025 to 06/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.1-week1/1.1.1-day01-2025-09-08/","title":"Day 01 - Introduction to Cloud Computing","tags":[],"description":"","content":"Date: 2025-09-08\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes What Is Cloud Computing? The on-demand delivery of IT resources over the Internet with pay-as-you-go pricing. Benefits of Cloud Computing Pay only for what you use, optimizing cost efficiency. Accelerate development through automation and managed services. Scale resources up or down as needed. Deploy applications globally in minutes. Why AWS? AWS has been the global cloud leader for 13 consecutive years (as of 2023). Unique culture, vision, and long-term customer obsession. AWS pricing philosophy: customers should pay less over time for the same resources. Every AWS Leadership Principle is focused on delivering real customer value. How to Get Started with AWS There are many learning paths—self-study is completely possible. Register an AWS Free Tier account to explore. Recommended course platforms: Udemy A Cloud Guru Explore AWS learning paths: AWS Learning Paths "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.2-week2/1.2.1-day06-2025-09-15/","title":"Day 06 - Amazon VPC Fundamentals","tags":[],"description":"","content":"Date: 2025-09-15\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Networking Services on AWS Amazon Virtual Private Cloud (VPC) Amazon Virtual Private Cloud (Amazon VPC) allows you to launch AWS resources into a virtual network you define. A VPC exists within a single Region. When creating a VPC, you must define an IPv4 CIDR block (required) and optionally an IPv6 one. The default limit is 5 VPCs per Region per Account. Commonly used to separate environments such as Production, Development, and Staging. To achieve full resource isolation, use separate AWS Accounts rather than multiple VPCs. Subnets A subnet resides within one Availability Zone. The subnet CIDR must be a subset of the parent VPC\u0026rsquo;s CIDR block. AWS reserves 5 IP addresses in each subnet: network, broadcast, router, DNS, and future use. Reserved IP Addresses Example (10.0.0.0/24):\n10.0.0.0 - Network address 10.0.0.1 - VPC router 10.0.0.2 - DNS server 10.0.0.3 - Reserved for future use 10.0.0.255 - Broadcast address Hands-On Labs Lab 03 – Amazon VPC \u0026amp; Networking Basics Create VPC → 03-03.1 Create Subnet → 03-03.2 "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.3-week3/1.3.1-day11-2025-09-22/","title":"Day 11 - Amazon EC2 Fundamentals","tags":[],"description":"","content":"Date: 2025-09-22\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Compute on AWS Amazon Elastic Compute Cloud (EC2) Amazon EC2 provides resizable compute capacity in the cloud, similar to a virtual or physical server. It supports workloads such as web hosting, applications, databases, authentication services, and other general-purpose server tasks. Instance Types\nEC2 configurations are defined by instance types, not custom hardware. Each type specifies: CPU (Intel, AMD, ARM – Graviton 1/2/3) / GPU Memory Network Storage Instance Type Categories:\nGeneral Purpose: T3, T4g, M5, M6i (balanced compute, memory, networking) Compute Optimized: C5, C6i, C7g (high-performance processors) Memory Optimized: R5, R6i, X2 (fast performance for memory-intensive workloads) Storage Optimized: I3, D2, H1 (high sequential read/write to local storage) Accelerated Computing: P4, G5, Inf1 (GPU/FPGA for ML, graphics) Amazon Machine Images (AMI) AMI (Amazon Machine Image) is a template that defines the software configuration of an instance, including OS, apps, and settings. Types of AMIs: Provided by AWS (Amazon Linux, Windows, Ubuntu, etc.) AWS Marketplace AMIs Custom AMIs created by users Benefits of Custom AMIs\nFaster instance launch and setup Simplified backup and restore Consistent environment across multiple instances AMI Components:\nRoot volume template (OS and applications) Launch permissions Block device mapping Hands-On Labs Lab 01 – AWS Account \u0026amp; IAM Setup Create an AWS Account → 01-01 Setup Virtual MFA Device → 01-02 Create Admin Group and Admin User → 01-03 Account Authentication Support → 01-04 "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.4-week4/1.4.1-day16-2025-09-29/","title":"Day 16 - Amazon S3 Fundamentals","tags":[],"description":"","content":"Date: 2025-09-29\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Storage Services on AWS Amazon Simple Storage Service (S3) Amazon S3 is an object storage service designed to store and retrieve any amount of data from anywhere on the web. It offers virtually unlimited scalability, high availability, strong security, and excellent performance.\nCore S3 Features Buckets and Objects: Data is stored as objects inside buckets. Each object can be up to 5 TB. Availability and Durability: S3 is designed for 99.99% availability and 99.999999999% (11 nines) durability. Security: Multiple layers of security including IAM, bucket policies, ACLs, and encryption. Scalability: Automatically scales storage and request throughput without performance degradation. S3 Object Structure:\nKey: Object name/path Value: Object data Version ID: For versioning Metadata: System and user metadata Access Control: Permissions S3 Access Points Access Points simplify managing data access for shared datasets in S3.\nPer-application access control: Each access point has its own policy. Operational simplicity: Eases permission management for shared datasets used by many applications. Network controls: Can be configured to accept requests only from specific VPCs. S3 Storage Classes Choose among storage classes optimized for different access patterns and cost profiles:\nS3 Standard: For frequently accessed data; highest availability and performance. S3 Intelligent-Tiering: Automatically moves objects between tiers to optimize cost. S3 Standard-IA (Infrequent Access): Lower cost for infrequently accessed data with millisecond retrieval. S3 One Zone-IA: Like Standard-IA but stored in a single AZ. S3 Glacier Flexible Retrieval: Low-cost archival with minutes-to-hours retrieval. S3 Glacier Deep Archive: Lowest-cost archival with ~12-hour retrieval. Storage Class Comparison:\nClass Durability Availability Min Storage Retrieval Standard 11 9\u0026rsquo;s 99.99% None Instant Intelligent-Tiering 11 9\u0026rsquo;s 99.9% None Instant Standard-IA 11 9\u0026rsquo;s 99.9% 30 days Instant One Zone-IA 11 9\u0026rsquo;s 99.5% 30 days Instant Glacier Flexible 11 9\u0026rsquo;s 99.99% 90 days Minutes-hours Glacier Deep Archive 11 9\u0026rsquo;s 99.99% 180 days 12 hours Hands-On Labs Lab 57 – Amazon S3 \u0026amp; CloudFront (Part 1) Create S3 Bucket → 57-2.1 Load Data → 57-2.2 Enable Static Website → 57-3 Configure Public Access Block → 57-4 "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.5-week5/1.5.1-day21-2025-10-06/","title":"Day 21 - Shared Responsibility &amp; IAM Basics","tags":[],"description":"","content":"Date: 2025-10-06\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Security Shared Responsibility Model In cloud computing, security is a shared responsibility between the cloud provider and the customer. Customers must securely configure services, apply best practices, and use security controls from the hypervisor exposure upward to application/data layers. The split of responsibilities varies by service model: Infrastructure-level services Partially managed services Fully managed services AWS Responsibilities (Security OF the Cloud):\nPhysical security of data centers Hardware and network infrastructure Virtualization infrastructure Managed service operations Customer Responsibilities (Security IN the Cloud):\nData encryption Network configuration Access management Application security Operating system patches (for EC2) AWS Identity and Access Management (IAM) Root Account Has unrestricted access to all AWS services/resources and can remove any attached permissions. Best practices: Create and use an IAM Administrator user for daily operations. Lock away root credentials (dual control). Keep the root user\u0026rsquo;s email and domain valid and renewed. Enable MFA on root account IAM Overview IAM controls access to AWS services and resources in your account. Principals include: root user, IAM users, federated users, IAM roles, assumed-role sessions, AWS services, and anonymous users. Notes: IAM users are not separate AWS accounts. New IAM users start with no permissions. Grant permissions by attaching policies to users, groups, or roles. Use IAM groups to manage many users (groups cannot be nested). Hands-On Labs Lab 48 – IAM Access Keys \u0026amp; Roles (Part 1) Create EC2 Instance → 48-1.1 Create S3 Bucket → 48-1.2 Generate IAM User and Access Key → 48-2.1 "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.6-week6/1.6.1-day26-2025-10-13/","title":"Day 26 - Database Fundamentals","tags":[],"description":"","content":"Date: 2025-10-13\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Database Concepts Review A database is an organized (or semi-structured) collection of information stored on storage devices to support concurrent access by multiple users or programs with different goals. Sessions A session spans from the moment a client connects to the DBMS until the connection is terminated. Primary Key A primary key uniquely identifies each row in a relational table. Foreign Key A foreign key in one table references the primary key of another table, creating a relationship between them. Index An index accelerates data retrieval at the cost of extra writes and storage to maintain the index structure. Indexes locate data without scanning every row; they can be defined over one or more columns. Index Types:\nB-Tree: General purpose, balanced tree structure Hash: Fast equality lookups Bitmap: Efficient for low-cardinality columns Full-Text: Text search optimization Partitioning Partitioning splits a large table into smaller, independent pieces (partitions), potentially placed on different storage. Benefits: better query performance, easier maintenance, and scalability. Common types: Range (e.g., by date) List Hash Composite (combination) Partitioning Example:\n-- Range partitioning by date CREATE TABLE orders ( order_id INT, order_date DATE, amount DECIMAL ) PARTITION BY RANGE (YEAR(order_date)) ( PARTITION p2023 VALUES LESS THAN (2024), PARTITION p2024 VALUES LESS THAN (2025), PARTITION p2025 VALUES LESS THAN (2026) ); Execution Plan / Query Plan A query plan details how the DBMS will execute an SQL statement (access paths, joins, sorts). Types: Estimated plan (before execution) Actual plan (from executed query) Key operators: table scan, index seek/scan, nested loops, hash/merge join, sort, aggregate, filter. Database Logs Database logs record all changes (INSERT/UPDATE/DELETE) and operations. Typical log types: transaction, redo, undo, binary logs. Uses: recovery, integrity, consistency/durability (ACID), replication, performance analysis. Buffers A buffer pool caches pages read from disk to minimize I/O. Management strategies: Replacement: LRU, FIFO, Clock Write policies: immediate vs. deferred Prefetching to warm the cache Hands-On Labs Lab 05 – Amazon RDS \u0026amp; EC2 Integration (Part 1) Create a VPC → 05-2.1 Create EC2 Security Group → 05-2.2 Create RDS Security Group → 05-2.3 Create DB Subnet Group → 05-2.4 "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.7-week7/1.7.1-day31-2025-10-20/","title":"Day 31 - Vertical Slice Kickoff","tags":[],"description":"","content":"Date: 2025-10-20\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Project Context Ebook Demo – Vertical Slice 0 Mục tiêu: demo tính năng xem chi tiết sách end-to-end trước khi phát triển toàn bộ hệ thống. Cách tiếp cận: Vertical Slice Architecture để xây từng lát cắt hoàn chỉnh thay vì làm theo tầng. Lợi ích: demo được ngay, phát hiện lỗi sớm, tạo nhịp phối hợp giữa frontend/backend. Kiến trúc Slice User → Frontend → API → Database → Response → UI Mỗi lát cắt bao gồm UI, API contract, backend logic và dữ liệu giả để trình diễn. Cho phép thay thế từng thành phần mà không ảnh hưởng đến toàn bộ hệ thống. Vertical Slice Architecture Nguyên tắc chính Phát triển feature theo luồng người dùng, không tách biệt tầng. Giữ scope nhỏ để demo nhanh và nhận feedback sớm. Rõ ràng trách nhiệm của từng slice, giúp mở rộng dễ dàng. Lợi ích Tăng tốc đóng gói giá trị: có thể show cho stakeholder ngay. Giảm rủi ro integration vì mỗi slice tự kiểm chứng. Cho phép phát triển song song giữa các slice khác nhau. Key Insights Vertical slice đóng vai trò nền tảng trước khi mở rộng sang các feature khác. Mỗi slice cần có checklist rõ ràng (UI hoàn thiện, contract chuẩn, backend trả dữ liệu đúng). Coi slice như “mini product” với vòng đời riêng giúp giữ chất lượng. Hands-On Labs Xác định phạm vi slice 0 (luồng xem chi tiết sách, data tối thiểu). Vẽ sơ đồ luồng dữ liệu và xác định boundary giữa frontend/backend. Chuẩn hóa checklist demo (contract, mock, UI, backend). "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.8-week8/1.8.1-day36-2025-10-27/","title":"Day 36 - Introduction to NLP &amp; Text Preprocessing","tags":[],"description":"","content":"Date: 2025-10-27 Status: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Introduction to Natural Language Processing What is NLP? NLP enables computers to understand, interpret, and generate human language. Applications: chatbots, sentiment analysis, translation, voice assistants, search engines. Key challenges: ambiguity, context, idioms, multiple languages. NLP Pipeline Overview Raw Text → Preprocessing → Feature Extraction → Model → Output Each stage transforms text into structured data for machine learning. Preprocessing is critical - \u0026ldquo;garbage in, garbage out\u0026rdquo; principle applies. Text Preprocessing Fundamentals Tokenization Breaking text into individual words or sentences. Word tokenization: \u0026ldquo;Hello world!\u0026rdquo; → [\u0026ldquo;Hello\u0026rdquo;, \u0026ldquo;world\u0026rdquo;, \u0026ldquo;!\u0026rdquo;] Sentence tokenization: Splitting paragraphs into sentences. Importance: foundation for all NLP tasks. Lowercasing Converting all text to lowercase for consistency. \u0026ldquo;Hello\u0026rdquo; and \u0026ldquo;hello\u0026rdquo; treated as the same word. Trade-off: may lose information (e.g., proper nouns). Removing Punctuation \u0026amp; Special Characters Cleaning text by removing non-alphanumeric characters. Keeps only meaningful words for analysis. Context-dependent: some punctuation may carry meaning. Key Insights NLP bridges the gap between human communication and machine understanding. Preprocessing quality directly impacts model performance. Different tasks require different preprocessing strategies. Always inspect your data before and after preprocessing. Hands-On Labs Lab 1: Basic Tokenization import nltk from nltk.tokenize import word_tokenize, sent_tokenize # Download required data nltk.download(\u0026#39;punkt\u0026#39;) text = \u0026#34;Natural Language Processing is fascinating! It enables many applications.\u0026#34; # Word tokenization words = word_tokenize(text) print(\u0026#34;Words:\u0026#34;, words) # Sentence tokenization sentences = sent_tokenize(text) print(\u0026#34;Sentences:\u0026#34;, sentences) Lab 2: Text Cleaning import re import string def clean_text(text): # Lowercase text = text.lower() # Remove punctuation text = text.translate(str.maketrans(\u0026#39;\u0026#39;, \u0026#39;\u0026#39;, string.punctuation)) # Remove numbers text = re.sub(r\u0026#39;\\d+\u0026#39;, \u0026#39;\u0026#39;, text) # Remove extra whitespace text = \u0026#39; \u0026#39;.join(text.split()) return text sample = \u0026#34;Hello World! This is NLP 101.\u0026#34; cleaned = clean_text(sample) print(\u0026#34;Original:\u0026#34;, sample) print(\u0026#34;Cleaned:\u0026#34;, cleaned) Lab 3: Exploring NLTK import nltk # Explore NLTK capabilities print(\u0026#34;NLTK Version:\u0026#34;, nltk.__version__) # Download additional resources nltk.download(\u0026#39;stopwords\u0026#39;) nltk.download(\u0026#39;wordnet\u0026#39;) from nltk.corpus import stopwords # View English stop words stop_words = set(stopwords.words(\u0026#39;english\u0026#39;)) print(f\u0026#34;Number of stop words: {len(stop_words)}\u0026#34;) print(\u0026#34;Sample stop words:\u0026#34;, list(stop_words)[:10]) Practice Exercises Tokenize a paragraph from your favorite book Create a function that removes URLs from text Compare word counts before and after preprocessing Experiment with different tokenization methods "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.9-week9/1.9.1-day41-2025-11-03/","title":"Day 41 - NLP Fundamentals Revision","tags":[],"description":"","content":"Date: 2025-11-03\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Week 8 Recap Core NLP Concepts Reviewed Text Preprocessing Pipeline\nTokenization: word and sentence level Normalization: lowercasing, removing special characters Stop word removal: context-dependent decisions Stemming vs Lemmatization: trade-offs and use cases Key Takeaways from Week 8\nPreprocessing quality directly impacts model performance Different tasks require different preprocessing strategies Lemmatization generally preferred for production systems Always validate preprocessing choices against your specific use case Deep Dive: Tokenization Techniques Advanced Tokenization Subword Tokenization\nByte Pair Encoding (BPE): handles out-of-vocabulary words WordPiece: used by BERT and other transformers SentencePiece: language-agnostic tokenization Use case: multilingual models and rare word handling Comparison of Approaches\nMethod Pros Cons Best For Word Simple, fast Large vocabulary Simple tasks Character No OOV issues Long sequences Spelling tasks Subword Balanced approach More complex Modern NLP Regular Expression Tokenization Custom patterns for domain-specific text Handling URLs, emails, hashtags Preserving important punctuation Medical/technical text tokenization Text Normalization Best Practices When to Normalize Should Normalize:\nCase sensitivity isn\u0026rsquo;t important Consistent format needed Memory/computation constraints Avoid Over-normalization:\nNamed entities are important Sentiment analysis (emoticons matter) Code or technical documentation Unicode Handling Proper encoding/decoding Normalization forms (NFC, NFD, NFKC, NFKD) Handling multilingual text Emoji and special character preservation Key Insights Revisiting fundamentals reveals deeper understanding Edge cases often determine preprocessing strategy Modern NLP increasingly uses subword tokenization Balance between simplicity and effectiveness is crucial Hands-On Labs Lab 1: Comprehensive Preprocessing Comparison import nltk from nltk.tokenize import word_tokenize, sent_tokenize from nltk.stem import PorterStemmer, WordNetLemmatizer from nltk.corpus import stopwords import string text = \u0026#34;\u0026#34;\u0026#34; The AI-powered system\u0026#39;s performance improved significantly! Running multiple tests @ 99.9% accuracy. #MachineLearning \u0026#34;\u0026#34;\u0026#34; def compare_preprocessing(text): \u0026#34;\u0026#34;\u0026#34;Compare different preprocessing approaches\u0026#34;\u0026#34;\u0026#34; # Original print(\u0026#34;ORIGINAL TEXT:\u0026#34;) print(text) print(\u0026#34;\\n\u0026#34; + \u0026#34;=\u0026#34;*60 + \u0026#34;\\n\u0026#34;) # Basic tokenization words_basic = word_tokenize(text) print(\u0026#34;BASIC TOKENIZATION:\u0026#34;) print(words_basic) print(f\u0026#34;Token count: {len(words_basic)}\\n\u0026#34;) # Lowercase + punctuation removal words_clean = [w.lower() for w in words_basic if w not in string.punctuation] print(\u0026#34;LOWERCASE + NO PUNCTUATION:\u0026#34;) print(words_clean) print(f\u0026#34;Token count: {len(words_clean)}\\n\u0026#34;) # Stop word removal stop_words = set(stopwords.words(\u0026#39;english\u0026#39;)) words_no_stop = [w for w in words_clean if w not in stop_words] print(\u0026#34;NO STOP WORDS:\u0026#34;) print(words_no_stop) print(f\u0026#34;Token count: {len(words_no_stop)}\\n\u0026#34;) # Stemming stemmer = PorterStemmer() words_stemmed = [stemmer.stem(w) for w in words_no_stop] print(\u0026#34;STEMMED:\u0026#34;) print(words_stemmed) print(f\u0026#34;Token count: {len(words_stemmed)}\\n\u0026#34;) # Lemmatization lemmatizer = WordNetLemmatizer() words_lemmatized = [lemmatizer.lemmatize(w, pos=\u0026#39;v\u0026#39;) for w in words_no_stop] print(\u0026#34;LEMMATIZED:\u0026#34;) print(words_lemmatized) print(f\u0026#34;Token count: {len(words_lemmatized)}\\n\u0026#34;) compare_preprocessing(text) Lab 2: Custom Tokenization with Regex import re def custom_tokenize(text): \u0026#34;\u0026#34;\u0026#34;Custom tokenization preserving special patterns\u0026#34;\u0026#34;\u0026#34; # Pattern for various text elements patterns = [ r\u0026#39;http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.\u0026amp;+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\u0026#39;, # URLs r\u0026#39;[\\w.+-]+@[\\w-]+\\.[\\w.-]+\u0026#39;, # Emails r\u0026#39;#\\w+\u0026#39;, # Hashtags r\u0026#39;@\\w+\u0026#39;, # Mentions r\u0026#39;\\d+\\.?\\d*%\u0026#39;, # Percentages r\u0026#39;\\$\\d+\\.?\\d*\u0026#39;, # Money r\u0026#39;\\d{4}-\\d{2}-\\d{2}\u0026#39;, # Dates r\u0026#39;\\w+\u0026#39;, # Words r\u0026#39;[^\\w\\s]\u0026#39;, # Punctuation ] pattern = \u0026#39;|\u0026#39;.join(patterns) tokens = re.findall(pattern, text) return tokens # Test custom tokenization sample = \u0026#34;\u0026#34;\u0026#34; Check out https://example.com for AI updates! Contact: info@ai-company.com Event: 2025-11-03 @10:00AM Budget: $50,000 (99.5% funded) #TechEvent \u0026#34;\u0026#34;\u0026#34; tokens = custom_tokenize(sample) print(\u0026#34;Custom Tokenization:\u0026#34;) for i, token in enumerate(tokens, 1): print(f\u0026#34;{i}. {token}\u0026#34;) Lab 3: Handling Multilingual Text def process_multilingual_text(text): \u0026#34;\u0026#34;\u0026#34;Process text with mixed languages and special characters\u0026#34;\u0026#34;\u0026#34; # Unicode normalization import unicodedata # NFC (Canonical Decomposition, followed by Canonical Composition) normalized = unicodedata.normalize(\u0026#39;NFC\u0026#39;, text) # Identify language-specific patterns has_emoji = bool(re.search(r\u0026#39;[\\U0001F600-\\U0001F64F]\u0026#39;, text)) has_cjk = bool(re.search(r\u0026#39;[\\u4e00-\\u9fff]\u0026#39;, text)) # Chinese/Japanese/Korean has_arabic = bool(re.search(r\u0026#39;[\\u0600-\\u06ff]\u0026#39;, text)) info = { \u0026#39;original\u0026#39;: text, \u0026#39;normalized\u0026#39;: normalized, \u0026#39;has_emoji\u0026#39;: has_emoji, \u0026#39;has_cjk\u0026#39;: has_cjk, \u0026#39;has_arabic\u0026#39;: has_arabic, \u0026#39;length\u0026#39;: len(text), \u0026#39;normalized_length\u0026#39;: len(normalized) } return info # Test multilingual processing multilingual_samples = [ \u0026#34;Hello 世界! 🌍\u0026#34;, \u0026#34;Natural Language Processing\u0026#34;, \u0026#34;café résumé naïve\u0026#34;, \u0026#34;مرحبا العالم\u0026#34; ] for sample in multilingual_samples: result = process_multilingual_text(sample) print(f\u0026#34;\\nText: {result[\u0026#39;original\u0026#39;]}\u0026#34;) print(f\u0026#34;Normalized: {result[\u0026#39;normalized\u0026#39;]}\u0026#34;) print(f\u0026#34;Has Emoji: {result[\u0026#39;has_emoji\u0026#39;]}\u0026#34;) print(f\u0026#34;Has CJK: {result[\u0026#39;has_cjk\u0026#39;]}\u0026#34;) print(f\u0026#34;Has Arabic: {result[\u0026#39;has_arabic\u0026#39;]}\u0026#34;) Lab 4: Preprocessing Pipeline Builder class PreprocessingPipeline: \u0026#34;\u0026#34;\u0026#34;Flexible preprocessing pipeline\u0026#34;\u0026#34;\u0026#34; def __init__(self): self.steps = [] self.stemmer = PorterStemmer() self.lemmatizer = WordNetLemmatizer() self.stop_words = set(stopwords.words(\u0026#39;english\u0026#39;)) def add_lowercase(self): self.steps.append((\u0026#39;lowercase\u0026#39;, lambda x: x.lower())) return self def add_tokenization(self): self.steps.append((\u0026#39;tokenize\u0026#39;, word_tokenize)) return self def add_remove_punctuation(self): def remove_punct(tokens): return [t for t in tokens if t not in string.punctuation] self.steps.append((\u0026#39;remove_punct\u0026#39;, remove_punct)) return self def add_remove_stopwords(self): def remove_stop(tokens): return [t for t in tokens if t not in self.stop_words] self.steps.append((\u0026#39;remove_stop\u0026#39;, remove_stop)) return self def add_stemming(self): def stem(tokens): return [self.stemmer.stem(t) for t in tokens] self.steps.append((\u0026#39;stem\u0026#39;, stem)) return self def add_lemmatization(self, pos=\u0026#39;v\u0026#39;): def lemmatize(tokens): return [self.lemmatizer.lemmatize(t, pos=pos) for t in tokens] self.steps.append((\u0026#39;lemmatize\u0026#39;, lemmatize)) return self def process(self, text): result = text for step_name, step_func in self.steps: result = step_func(result) print(f\u0026#34;After {step_name}: {result[:100]}...\u0026#34;) # Show first 100 chars return result # Build custom pipeline pipeline = (PreprocessingPipeline() .add_lowercase() .add_tokenization() .add_remove_punctuation() .add_remove_stopwords() .add_lemmatization()) text = \u0026#34;The students are studying NLP concepts and building amazing applications!\u0026#34; result = pipeline.process(text) print(f\u0026#34;\\nFinal result: {result}\u0026#34;) Practice Exercises Compare preprocessing approaches on different text types (tweets, articles, code) Build a preprocessing pipeline for a specific domain (medical, legal, social media) Implement custom tokenization for handling special formats (product codes, IDs) Analyze the impact of different preprocessing choices on a classification task Create a preprocessing benchmark comparing speed and quality trade-offs "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.10-week10/1.10.1-day46-2025-11-10/","title":"Day 46 - Authentication &amp; Project Setup","tags":[],"description":"","content":"Date: 2025-11-10\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Project Context Review Online Library Architecture Frontend Stack:\nNext.js 14 (App Router) AWS Amplify (Hosting + CI/CD) Amplify UI Components TailwindCSS for styling React Query for state management AWS Services Integration:\nAmazon Cognito (Authentication) API Gateway (HTTP API endpoints) S3 (File storage via presigned URLs) CloudFront (Content delivery) DynamoDB (Metadata storage) Development Environment Setup Prerequisites:\nNode.js 18+ and npm/yarn AWS CLI configured Git and GitHub account AWS account with appropriate permissions Project Structure:\nonline-library/ ├── src/ │ ├── app/ # Next.js App Router pages │ ├── components/ # Reusable UI components │ ├── lib/ # Utility functions \u0026amp; AWS SDK │ ├── hooks/ # Custom React hooks │ └── types/ # TypeScript definitions ├── public/ # Static assets ├── amplify/ # Amplify configuration └── .env.local # Environment variables Authentication Flow Cognito Integration Strategy User Journey:\nRegistration → Email verification → Login JWT token storage in localStorage/cookies Auto-refresh token before expiration Role-based access (User vs Admin) Key Components:\nLogin page with email/password Registration form with validation Email verification flow Password reset functionality Protected routes HOC Amplify UI Authentication Benefits:\nPre-built, customizable components Built-in form validation Automatic token management Responsive design out-of-the-box Configuration:\n// amplify-config.js const amplifyConfig = { Auth: { region: \u0026#39;ap-southeast-1\u0026#39;, userPoolId: process.env.NEXT_PUBLIC_USER_POOL_ID, userPoolWebClientId: process.env.NEXT_PUBLIC_USER_POOL_CLIENT_ID, }, API: { endpoints: [ { name: \u0026#39;OnlineLibraryAPI\u0026#39;, endpoint: process.env.NEXT_PUBLIC_API_ENDPOINT, } ] } } Key Insights Amplify UI components significantly reduce auth implementation time Token refresh should happen automatically in background Store user role/groups in JWT claims for frontend authorization Environment variables critical for security - never commit sensitive data CI/CD setup in Amplify requires proper build settings Tasks Completed Project Initialization\nCreated Next.js 14 project with TypeScript and TailwindCSS Installed AWS Amplify and required dependencies Set up project folder structure Amplify Configuration\nConfigured Amplify with Cognito User Pool credentials Set up API Gateway endpoint configuration Created environment variable structure Authentication Pages\nBuilt login page with Amplify Authenticator Created registration page with email verification Implemented protected route HOC for authenticated pages User Management\nCreated custom useUser hook for user context Implemented JWT token handling Set up admin role detection from Cognito groups CI/CD Setup\nConnected GitHub repository to Amplify Hosting Configured build settings for Next.js deployment Set up environment variables in Amplify Console Challenges \u0026amp; Solutions Challenge: Amplify v6 API changes from previous versions\nSolution: Updated to use new Amplify.configure() syntax and Authenticator.Provider\nChallenge: Token refresh timing causing 401 errors\nSolution: Implemented automatic token refresh 5 minutes before expiration\nChallenge: Admin role not reflecting immediately after login\nSolution: Added token re-fetch mechanism after successful authentication\n"},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.1-week1/","title":"Week 1 - Cloud Computing Fundamentals","tags":[],"description":"","content":"Week: 2025-09-08 đến 2025-09-12\nStatus: \u0026ldquo;Done\u0026rdquo;\nTổng quan tuần 1 Tuần này tập trung vào các khái niệm cơ bản về Cloud Computing, AWS Infrastructure, và các công cụ quản lý AWS.\nNội dung chính Giới thiệu Cloud Computing và lợi ích AWS Global Infrastructure (Regions, AZs, Edge Locations) AWS Management Tools (Console, CLI, SDK) Cost Optimization và Support Plans AWS Well-Architected Framework Labs thực hành Lab 01: AWS Account \u0026amp; IAM Setup Lab 07: AWS Budgets \u0026amp; Cost Management Lab 09: AWS Support Plans "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/","title":"Worklog - AWS Learning Journey","tags":[],"description":"","content":"Worklog Overview This is a worklog documenting the AWS and development learning journey from September 8, 2025, to November 14, 2025 (50 working days across 10 weeks).\nStructure The worklog is organized by week, with each week consisting of 5 working days (Monday to Friday).\nProgress Week 1 (Sept 8-12): Cloud Computing Fundamentals Week 2 (Sept 15-19): AWS Networking Services Week 3 (Sept 22-26): AWS Compute Services Week 4 (Sept 29-Oct 3): AWS Storage Services Week 5 (Oct 6-10): AWS Security \u0026amp; Identity Week 6 (Oct 13-17): AWS Database Services Week 7 (Oct 20-24): Advanced AWS Topics Week 8 (Oct 27-31): Learning Natural Language Processing Week 9 (Nov 3-7): NLP Revision \u0026amp; Advanced Concepts Week 10 (Nov 10-14): Online Library Project Completion (Frontend) Statistics Total duration: 10 weeks (50 working days) AWS-focused weeks: 7 weeks (Weeks 1-7) Front-End development weeks: 3 weeks (Weeks 8-10) Total labs: 25+ hands-on AWS labs Main domains: AWS Core Services, NLP, Full-Stack Development Main Content AWS Core Services (Weeks 1-7) Cloud Computing Fundamentals\nAWS basics, global infrastructure, management tools Cost optimization, support plans Well-Architected Framework Networking\nVPC, subnets, security groups, NACLs Load balancing (ALB, NLB, GWLB) VPC Peering, Transit Gateway Hybrid DNS with Route 53 Resolver Compute\nEC2, AMI, EBS, Instance Store Auto Scaling, pricing models Lightsail, EFS, FSx Storage\nS3, storage classes, Glacier Snow Family, Storage Gateway Disaster Recovery, AWS Backup Security \u0026amp; Identity\nIAM, Cognito, Organizations KMS, Security Hub Identity Center (SSO) Shared Responsibility Model Database\nRDS, Aurora, Redshift ElastiCache, DMS Database best practices Advanced Topics\nServerless (Lambda) Containers (ECS, EKS, ECR) Monitoring (CloudWatch, X-Ray, CloudTrail) Lambda automation with Slack integration Natural Language Processing (Weeks 8-9) NLP Fundamentals \u0026amp; Applications Text preprocessing (tokenization, stemming, lemmatization) Sentiment analysis and text classification Named Entity Recognition (NER) NLP libraries (NLTK, spaCy) Building NLP pipelines and real-world projects Front-End Development (Week 10) Online Library Project Authentication with AWS Cognito and Amplify UI File upload with S3 presigned URLs Admin panel and book management system Reader interface with CloudFront signed URLs Search functionality using DynamoDB GSI CI/CD deployment with AWS Amplify "},{"uri":"https://anquoc211.github.io/AWS_Internship/5-workshop/5.1-workshop-overview/","title":"Workshop Overview","tags":[],"description":"","content":"Goals Introduce the authentication workshop: set up AWS Amplify CLI, create a React application with complete authentication flow using Amazon Cognito, implement user registration, login, password management, MFA, and social sign-in. Emphasize rapid implementation of secure authentication without managing infrastructure.\nArchitecture at a glance Amazon Cognito User Pool manages user directory and authentication. AWS Amplify provides CLI tooling and frontend libraries for seamless integration. React application uses Amplify UI components for authentication UI. JWT tokens handle session management and API authorization. Expected outcomes React application with user registration and email verification. Login/logout functionality with session persistence. Password reset and change password features. Multi-factor authentication (MFA) with TOTP. Social sign-in integration (Google/Facebook). Protected routes with role-based access control. Timing \u0026amp; structure Module 1: Amplify setup and initial configuration. Module 2: User registration and email verification flow. Module 3: Login, session management, and logout. Module 4: Password management (forgot/reset/change). Module 5: Advanced features (MFA and social login). Module 6: Production deployment and cleanup. "},{"uri":"https://anquoc211.github.io/AWS_Internship/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Online Library – A Serverless Content Platform for Small Groups 1. Executive Summary The Online Library project aims to build a low-cost serverless platform for storing and distributing content (PDF/ePub) for a small user group (initially ~100 users, primarily students/labs needing controlled internal research-material sharing). The solution prioritizes security, content moderation (Admin Approval), and transparent, linear operating costs as it scales.\nThe architecture uses a fully AWS Serverless stack (Amplify, Cognito, API Gateway, Lambda, S3, CloudFront, DynamoDB).\nEstimated cost for the MVP (excluding Free Tier) is ≈ $9.80/month, with predictable scaling to 5,000–50,000 users.\n2. Problem Statement What’s the Problem? Documents and books are scattered; there is no secure content delivery system with access control; the process of adding or moderating user-generated content (UGC) is slow and has high friction.\nThe Solution: Build a serverless pipeline on AWS:\nUsers upload files via Presigned PUT URL to temporary S3; Admin approves → Lambda moves the file to a protected public folder; Readers access content via Signed GET URL (from CloudFront/CDN) to ensure speed and controlled access.\nBenefits and Return on Investment Business value: Centralized content; quality control through moderation; fast deployment with CI/CD. Technical benefits: Very low operating cost (≈ $9.80/month for MVP); scalable serverless architecture; secured content access. 3. Solution Architecture A. High level B. Request flow AWS Services Used Service Primary Role Specific Tasks Amplify Hosting CI/CD + FE Hosting Build \u0026amp; Deploy Next.js, domain management Cognito Authentication Sign-up/Login, JWT issuance, refresh tokens API Gateway API Entry Point Receive requests, validate JWT, route to Lambda Lambda Business Logic Handle upload/approval, generate signed URLs, write metadata S3 Object Storage Store original and approved files, served via CloudFront Signed URL CloudFront CDN Fast content delivery, blocks direct S3 access via OAC DynamoDB Database Store metadata (title, uploader, approval status) Route 53 DNS Domain mapping to Amplify, API Gateway, CloudFront CloudWatch Monitoring Lambda logs, anomaly alerts Search Simple search fields (titles, author) using DynamoDB GSIs.\nComponent Design User Upload: Presigned PUT to S3 uploads/. Admin Approval: Lambda copies file from uploads/ → public/books/ upon approval. Reader Security: CloudFront OAC prevents direct S3 access; reading occurs only through Signed URL generated by Lambda. Search Architecture Simple Search: Design GSI for title and author (example: GSI1: PK=TITLE#{normalizedTitle}, SK=BOOK#{bookId}; GSI2: PK=AUTHOR#{normalizedAuthor}, SK=BOOK#{bookId}). Add endpoint GET /search?title=...\u0026amp;author=... to query GSI instead of Scan. Admin Authorization Use Cognito User Groups with an Admins group. Admin JWT contains cognito:groups: [\u0026quot;Admins\u0026quot;]. Admin-specific Lambdas (exampleapproveBook, takedownBook) check this claim and return 403 Forbidden otherwise. JWT Authorizer (API Gateway HTTP API) handles authentication, while authorization logic is inside Lambda. 4. Technical Implementation Implementation Phases Design \u0026amp; IaC: Use CDK to define all stacks (Cognito, DDB, S3, Amplify, Lambda, API). Upload \u0026amp; Approval Flow: Implement Presigned PUT, metadata (status= PENDING), Admin approval logic. Reading Flow: Implement Signed GET, FE reader stream via CloudFront. Ops: CloudWatch logs, budget alerts, IAM hardening. Search: Add GSI for title, author, implement GET /search. Technical Requirements Entire infrastructure defined using CDK. API Gateway uses HTTP API for cost savings. Lambda (Python) handles business logic \u0026amp; DynamoDB/S3. S3 Bucket Policy must deny public access and allow only CloudFront OAC. 5. Timeline \u0026amp; Milestones Project Timeline Platform \u0026amp; Authentication (Week 1–2) Objective: Set up infrastructure and allow user login.\nBackend Tasks (CDK/DevOps): CDK/IaC stack for Cognito. CDK stack for DynamoDB (main table, no GSI yet). CDK stack for S3 (uploads, public, logs) + OAC. Deploy API Gateway (HTTP API) + a test Lambda. Frontend Tasks (Amplify): Configure Amplify Hosting + GitHub CI/CD. Integrate Amplify UI / Cognito SDK for: Sign-up, Email verification, Login, Forgot password. Milestone: git push automatically deploys FE. User can sign-up/login and obtain JWT. Upload \u0026amp; Approval Flow (Week 2–3) Objective: Allow authenticated users to upload files and Admins to approve them.\nBackend (Lambda/CDK): Implement createUploadUrl Lambda: Validate JWT. Create Presigned PUT URL to uploads/. Write metadata (status=PENDING). Implement approveBook: Validate Admin role. Copy S3 file uploads/ → public/books/. Update DynamoDB status (APPROVED). Frontend: Upload form (drag \u0026amp; drop). Upload via Presigned PUT. Admin dashboard with list of PENDING, button “Approve”. Reading \u0026amp; Search (Week 3–4) Objective: Allow reading \u0026amp; searching approved books.\nBackend: Implement getReadUrl: generate Signed GET URL (short TTL). Add GSI for title, author. Implement searchBooks. Frontend: Homepage: book list. Search bar → API searchBooks. Reader screen using the Signed URL (e.g., via react-pdf). Ops \u0026amp; Security (Week 5–6) Backend: S3 Event Notification for new uploads. Lambda validateMimeType: read magic bytes to verify PDF/ePub. Lambda takedownBook (Admin), deleteUpload (auto cleanup after 72h). DevOps: AWS Budget Alerts, CloudWatch Alarms. IAM least-privilege + CORS tightening. 6. Budget Estimation Budget comes from AWS Pricing Calculator.\nMonthly cost (strict, no Free Tier, ~100 users): ≈ $9.80/month.\n# AWS Service Region Monthly (USD) Notes 0 Amazon CloudFront Asia Pacific (Singapore) 0.86 10 GB data egress + 10 000 HTTPS requests 1 AWS Amplify Asia Pacific (Singapore) 1.31 100 build min + 0.5 GB storage + 2 GB served 2 Amazon API Gateway Asia Pacific (Singapore) 0.01 ~10 000 HTTP API calls/tháng 3 AWS Lambda Asia Pacific (Singapore) 0.00 128 MB RAM × 100 ms × 10 000 invokes 4 Amazon S3 (Standard) Asia Pacific (Singapore) 0.05 2 GB object storage for books/images 5 Data Transfer Asia Pacific (Singapore) 0.00 Included in CloudFront cost 6 DynamoDB (On-Demand) Asia Pacific (Singapore) 0.03 Light metadata table (0.1 GB, few reads/writes) 7 Amazon Cognito Asia Pacific (Singapore) 5.00 100 MAU, Advanced Security enabled 8 Amazon CloudWatch Asia Pacific (Singapore) 1.64 5 metrics + 0.1 GB logs/tháng 9 Amazon Route 53 Asia Pacific (Singapore) 0.90 1 Hosted Zone + DNS queries ≈ 9.80 USD / month No Free Tier applied Infrastructure Costs This cost model demonstrates the efficiency of serverless architecture: costs are primarily centered on the value delivered to the user (Cognito MAU), rather than paying for \u0026lsquo;idle servers\u0026rsquo;.\n7. Risk Assessment Risk Matrix Risk Impact Mitigation Cost spike due to sudden user growth High Limit MAU, cache metadata via CloudFront Abuse of uploads Medium Limit ≤ 50MB; auto-delete after 72h Fake/malicious file types Medium S3 Event → Lambda MIME validation Monitoring overload Low CloudWatch alerts, 14-day retention Mitigation Strategies cost: Set AWS Budget Alerts for CloudFront and Cognito. Be aware that Signed URLs have a short TTL and should not be cached publicly long-term; instead, cache metadata/API responses (book lists, details) on CloudFront for 3–5 minutes to reduce API load. Only generate Signed URLs when the user actually clicks to read (on-demand), do not pre-generate for the entire list. Upload: Limit file size to ≤ 50MB for MVP. (Can be increased to 200MB if needed, use multipart upload on the FE to avoid timeouts.) Apply Rate Limit/Throttling on API Gateway for endpoints that create Presigned URLs. Set up an S3 Lifecycle Policy to automatically delete unapproved files in uploads/ after 72h. Add Server-side Validation: S3 Event Notifications $\\to$ Lambda reads magic bytes (e.g., file-type library) to verify correct PDF/ePub; if incorrect, automatically delete and write REJECTED_INVALID_TYPE status to DynamoDB. Copyright (DMCA): Store Audit Log in DynamoDB: uploaderID, uploadTimestamp, adminApproverID, approvalTimestamp for traceability. Build a Takedown API (Admin only): update status to TAKEDOWN; optionally move the object from public/books/ to quarantine/books/ (do not delete completely) to preserve traces. Contingency Plans If costs exceed budget, enable Invite-Only mode to cap Cognito MAU and reduce load.\n8. Expected Outcomes Technical Improvements Fast and secure content delivery (CDN + Signed URL). Standard AWS Serverless architecture capable of scaling to 50,000 users without redesign. Fully automated CI/CD for both frontend \u0026amp; backend. Long-term Value A centralized content platform for structured book data. Continuous documentation of an end-to-end Serverless implementation. Room for future analytics (QuickSight) or AI/ML features. This system proves the ability to build a platform that securely, cost-effectively, and scalably easy by AWS Serverless services - that suitable to apply for small groups or communities.\n"},{"uri":"https://anquoc211.github.io/AWS_Internship/4-eventparticipated/4.2-event2/","title":"Event 2 - AWS GenAI Builder Club","tags":[],"description":"","content":"AWS GenAI Builder Club: AI-Driven Development Lifecycle Date \u0026amp; Time: Friday, October 3, 2025 | 2:00 PM VNT\nVenue: AWS Event Hall, L26 Bitexco Tower, Ho Chi Minh City\nLed by: Toan Huynh \u0026amp; My Nguyen\nOverview This AWS GenAI Builder Club session introduced the AI-Driven Development Lifecycle (AI-DLC), demonstrating how AI can transform software engineering by serving as an intelligent collaborator throughout development. The session featured hands-on demonstrations of Amazon Q Developer and Kiro IDE, showcasing practical AI integration in modern development workflows.\nSession Schedule Time Topic Presenter 14:00 - 14:15 Welcome \u0026amp; Introduction - 14:15 - 15:30 AI-DLC Overview \u0026amp; Amazon Q Developer Demo Toan Huynh 15:30 - 15:45 Networking Break - 15:45 - 16:30 Kiro IDE Demonstration My Nguyen AI-Driven Development Lifecycle (AI-DLC) Core Principles You Remain in Control\nAI serves as your assistant, not your replacement. Maintain decision-making authority over project direction and technical choices.\nBidirectional Collaboration\nAI should ask critical questions about requirements, architecture, and goals. The relationship is collaborative, with you guiding AI\u0026rsquo;s suggestions.\nPlan Before Code\nCreate comprehensive plans before implementation. AI can help generate plans, but you must review and validate them.\nDevelopment Workflow 1. Project Planning\nDefine clear requirements and scope Generate initial plans with AI assistance Review and refine until detailed and unambiguous 2. User Story Creation\nBreak down plans into user stories with acceptance criteria Divide scope into manageable units Assign stories to team members 3. Technology Stack Definition\nSpecify frameworks, tools, and technologies Provide positive direction rather than constraints Clear specifications yield better results 4. Requirements \u0026amp; Design\nWrite precise, detailed requirements Create specifications collaboratively with AI Define data models, APIs, and system architecture 5. Implementation\nBuild according to the plan Use collaborative development approaches Verify all AI-generated code as a team 6. Testing \u0026amp; Deployment\nProgress through environments: Dev → QA → UAT → Production Validate at each quality gate Ensure functionality before release Success Factors Plan First - Don\u0026rsquo;t expect AI to manage everything Review Regularly - Validate AI outputs continuously Manage Actively - Your value is in validation and oversight Ask Questions - Ensure AI understands your context Use Templates - Structure prompts with context and requirements Document Plans - Export plans as files for reference Stay Respectful - Maintain professional communication with AI Amazon Q Developer Overview Amazon Q Developer is an AI assistant that enhances the software development lifecycle across multiple platforms: AWS Console, IDEs, CLI, and DevSecOps tools.\nKey Capabilities Code Enhancement\nAccelerates code generation Improves code quality through intelligent suggestions Optimizes complex codebases Integrates seamlessly with existing workflows Automation\nGenerates comprehensive documentation automatically Creates unit tests with minimal effort Reduces boilerplate code Automates repetitive tasks Intelligent Support\nLeverages deep AWS service knowledge Combines expertise across multiple domains Strengthens security posture Enhances developer productivity Best Practices Provide detailed project context Use specific, example-rich prompts Review all suggestions before implementation Iterate and refine prompts as needed Leverage AWS service expertise Kiro IDE Overview Kiro is an agentic IDE from AWS that bridges rapid AI prototyping and production-ready development. Currently in public preview, it maintains professional standards while enhancing productivity.\nCore Features Spec-Driven Development\nConverts requirements into structured specifications Generates user stories with acceptance criteria Creates design documentation before coding Plans implementation systematically Agent Automation\nTriggers tasks automatically on events Updates documentation on file saves Generates tests on commits Optimizes performance based on actions Project Context\nUses steering files (markdown) for guidelines Understands project structure deeply Applies coding standards consistently Follows team best practices Multi-File Intelligence\nAnalyzes multiple files simultaneously Understands functional goals across codebase Aligns changes with project objectives Goes beyond simple completion VS Code Foundation\nBuilt on VS Code open source Import settings and extensions Familiar interface for developers Seamless transition Flexible Models\nUses Claude Sonnet 4 by default Auto mode combines multiple models Balance quality and cost Adapt to different tasks Advantages Transparency \u0026amp; Control\nStart with specifications Review before implementation Reduce misaligned code Maintain clear traceability Automation\nAuto-generate documentation Create unit tests automatically Update information systematically Focus on high-value work Security\nMost operations run locally Data sent only with permission Control sensitive information Extensibility\nIntegrate external tools via MCP Support multiple AI models Not locked to single environment Adapt to team workflows Considerations Currently in public preview May struggle with very complex projects Requires developer supervision Expected future pricing tiers (Free/Pro/Pro+) When to Use Need spec-driven development Want professional, structured AI workflow Building rapid prototypes for production Require automated documentation and testing Common Pitfalls \u0026amp; Solutions Over-Reliance on AI\nSolution: Plan first, review regularly. AI enhances, not replaces developer judgment.\nHigh Error Rates\nSolution: Implement review cycles. Validate all AI-generated code.\nVague Requirements\nSolution: Write precise requirements. Create detailed specifications collaboratively.\nNegative Instructions\nSolution: Use positive, specific direction. Tell AI what to do, not what to avoid.\nMissing Context\nSolution: Create steering files. Provide detailed context. Ask critical questions.\nAI as Manager\nSolution: You manage the project. Your value is in validation and oversight.\nKey Takeaways Maintain Control: You are the manager; AI is your assistant Plan Strategically: Create comprehensive plans before coding Collaborate Effectively: AI should question and collaborate, not just execute Specify Clearly: Precise requirements yield better AI outputs Review Continuously: Validate AI suggestions regularly Focus on Value: Your expertise matters in validation and architecture Structure Prompts: Use templates with context and requirements Document Thoroughly: Export plans as living reference documents Direct Positively: Positive instructions work better than constraints Practice Actively: Hands-on experience reveals capabilities and limits Tools Covered Amazon Q Developer - AI-powered development assistant Kiro IDE - Spec-driven development environment MCP (Model Context Protocol) - Tool integration framework Conclusion The AI-Driven Development Lifecycle represents a paradigm shift where AI and developers collaborate as partners. Success requires clear planning, continuous review, precise requirements, and maintaining developer control. Amazon Q Developer and Kiro IDE enable this workflow, working best when developers understand their role as project managers and code validators while leveraging AI for enhanced productivity.\n"},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.1-week1/1.1.2-day02-2025-09-09/","title":"Day 02 - AWS Global Infrastructure","tags":[],"description":"","content":"Date: 2025-09-09\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes AWS Infrastructure Data Centers Each data center can host tens of thousands of servers. AWS builds and manages its own custom hardware for efficiency and reliability. Availability Zone (AZ) One or more physically separate data centers within a Region. Each AZ is designed for fault isolation. Connected via low-latency, high-throughput private links. AWS recommends deploying workloads across at least two AZs. Region A Region contains at least three Availability Zones. There are currently 25+ Regions worldwide. Regions are interconnected by the AWS backbone network. Most services are Region-scoped by default. Edge Locations Global network of edge sites designed to serve content with minimal latency. Used by services such as: Amazon CloudFront (CDN) AWS WAF (Web Application Firewall) Amazon Route 53 (DNS Service) Hands-On Labs Lab 01 – AWS Account \u0026amp; IAM Setup Create an AWS Account → 01-01 Configure Virtual MFA Device → 01-02 Create Admin Group and Admin User → 01-03 Account Authentication Support → 01-04 "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.2-week2/1.2.2-day07-2025-09-16/","title":"Day 07 - VPC Routing &amp; Network Interfaces","tags":[],"description":"","content":"Date: 2025-09-16\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes VPC Routing \u0026amp; ENI Route Tables A route table defines how traffic is directed. Each VPC has a default route table containing only a local route allowing internal communication between subnets. Custom route tables can be created, but the local route cannot be deleted. Elastic Network Interface (ENI) An ENI is a virtual network card that can be moved between EC2 instances. It retains its private IP, Elastic IP address, and MAC address when reassigned. Elastic IP (EIP) is a static public IPv4 address that can be associated with an ENI. Unused EIPs incur charges. ENI Use Cases:\nManagement network separate from data network Network and security appliances Dual-homed instances with workloads on distinct subnets Low-budget, high-availability solution VPC Endpoints A VPC Endpoint enables private connectivity to supported AWS services via AWS PrivateLink without using the public Internet. Two types: Interface Endpoint: Uses an ENI with a private IP. Gateway Endpoint: Uses route tables (available for S3 and DynamoDB only). Hands-On Labs Lab 03 – Amazon VPC \u0026amp; Networking (continued) Create Internet Gateway (IGW) → 03-03.3 Create Route Table (Outbound via IGW) → 03-03.4 Create Security Groups → 03-03.5 "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.3-week3/1.3.2-day12-2025-09-23/","title":"Day 12 - EC2 Storage &amp; Backup","tags":[],"description":"","content":"Date: 2025-09-23\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes EC2 Storage \u0026amp; Security Backup in EC2 AWS Backup provides centralized backup for AWS services including EC2. EBS Snapshots back up EBS volumes: Point-in-time backups Incremental (stores only changed blocks) Stored in S3 (not directly accessible) AMI Backup captures the full EC2 configuration as an image. Snapshot Best Practices:\nSchedule regular snapshots Copy snapshots to other regions for DR Tag snapshots for lifecycle management Use Amazon Data Lifecycle Manager (DLM) Key Pair Key Pairs are used for secure authentication when connecting to EC2: Public Key – stored on the instance Private Key – kept by the user for SSH (Linux) or RDP (Windows) Replaces passwords for better security. Important: If you lose your private key, AWS cannot recover it. Key Pair Management:\nCreate key pairs in AWS or import your own Store private keys securely Use different key pairs for different environments Rotate keys regularly Elastic Block Store (EBS) Amazon EBS provides persistent block storage for EC2 instances. Volume types: General Purpose SSD (gp2/gp3) – balance between performance \u0026amp; cost Provisioned IOPS SSD (io1/io2) – for high IOPS workloads Throughput Optimized HDD (st1) – for large, sequential data Cold HDD (sc1) – low-cost, infrequently accessed data Key Features\nAttach/detach volumes from instances Data persists when instances stop Create snapshots for backup or cross-region copy Automatically replicated within an AZ EBS Volume Comparison:\nType Use Case Max IOPS Max Throughput gp3 General purpose 16,000 1,000 MB/s io2 High performance 64,000 1,000 MB/s st1 Big data 500 500 MB/s sc1 Cold storage 250 250 MB/s "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.4-week4/1.4.2-day17-2025-09-30/","title":"Day 17 - S3 Advanced Features","tags":[],"description":"","content":"Date: 2025-09-30\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Amazon S3 Static Website Hosting Host static websites (HTML, CSS, JS, images) directly from S3.\nKey Capabilities Simple setup: A few steps to enable static website hosting on a bucket. Low cost: Pay standard S3 storage and data transfer; no separate web server charges. Elastic scaling: Automatically handles traffic spikes. CDN integration: Easily front with Amazon CloudFront for global performance. Static Website Configuration:\n{ \u0026#34;IndexDocument\u0026#34;: { \u0026#34;Suffix\u0026#34;: \u0026#34;index.html\u0026#34; }, \u0026#34;ErrorDocument\u0026#34;: { \u0026#34;Key\u0026#34;: \u0026#34;error.html\u0026#34; } } Cross-Origin Resource Sharing (CORS) CORS allows web resources (fonts, JavaScript, etc.) on one domain to request resources from another domain.\nConfiguring CORS on S3 Define policies: Specify which origins are permitted to access a bucket\u0026rsquo;s content. Control methods: Allow specific HTTP methods (GET, PUT, POST, etc.). Security posture: Prevent unauthorized cross-origin access. CORS Configuration Example:\n[ { \u0026#34;AllowedHeaders\u0026#34;: [\u0026#34;*\u0026#34;], \u0026#34;AllowedMethods\u0026#34;: [\u0026#34;GET\u0026#34;, \u0026#34;HEAD\u0026#34;], \u0026#34;AllowedOrigins\u0026#34;: [\u0026#34;https://example.com\u0026#34;], \u0026#34;ExposeHeaders\u0026#34;: [\u0026#34;ETag\u0026#34;], \u0026#34;MaxAgeSeconds\u0026#34;: 3000 } ] Performance and Object Key Design Object key naming can significantly affect S3 performance:\nRandomized prefixes: Distribute keys across partitions for higher parallelism. Avoid sequential prefixes: Don\u0026rsquo;t use monotonically increasing prefixes (e.g., timestamps) for high-throughput workloads. Parallel access: Structure keys to enable concurrent reads/writes. Key Design Best Practices:\n❌ Bad: 2025-09-30-file1.jpg, 2025-09-30-file2.jpg Good: a1b2/2025-09-30-file1.jpg, c3d4/2025-09-30-file2.jpg S3 Glacier – Long-Term Archival S3 Glacier classes are optimized for ultra–low-cost long-term storage.\nRetrieval Options Expedited / Fast: Minutes; highest cost. Standard: 3–5 hours; balanced cost. Bulk: 5–12 hours; lowest cost for large restores. Glacier Deep Archive The lowest-cost class for multi-year retention, with ~12-hour retrieval times.\nHands-On Labs Lab 57 – Amazon S3 \u0026amp; CloudFront (Part 2) Configure Public Objects → 57-5 Test Website → 57-6 Block All Public Access → 57-7.1 Configure CloudFront → 57-7.2 Test CloudFront → 57-7.3 Bucket Versioning → 57-8 "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.5-week5/1.5.2-day22-2025-10-07/","title":"Day 22 - IAM Policies &amp; Roles","tags":[],"description":"","content":"Date: 2025-10-07\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes IAM Policies JSON documents defining permissions. Types: Identity-based policies (attached to principals) Resource-based policies (attached to resources) Evaluation rule: explicit Deny overrides Allow across all policies. Pattern to constrain S3 administration:\nAllow all s3:* actions on a specific bucket. Explicitly Deny all non-S3 actions. Policy Structure:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::my-bucket/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;IpAddress\u0026#34;: { \u0026#34;aws:SourceIp\u0026#34;: \u0026#34;203.0.113.0/24\u0026#34; } } }] } Policy Evaluation Logic:\nBy default, all requests are denied Explicit allow overrides default deny Explicit deny overrides any allows Permissions boundaries limit maximum permissions IAM Roles Roles provide temporary permissions assumed by users, services, or external identities. Common use cases: Let an AWS service act on your behalf (e.g., EC2 → S3 writes) Cross-account access Federation from external IdPs Credentials for apps on EC2 without storing access keys Benefits\nNo long-term credentials, short-lived sessions, least privilege, and easier large-scale access management. Role Types:\nService Role: For AWS services (EC2, Lambda, etc.) Cross-Account Role: Access resources in another account Identity Provider Role: For federated users Instance Profile: Container for EC2 instance role Hands-On Labs Lab 48 – IAM Access Keys \u0026amp; Roles (Part 2) Use Access Key → 48-2.2 Create IAM Role → 48-3.1 Use IAM Role → 48-3.2 Clean Up Resources → 48-4 Lab 28 – IAM Cross-Region Role \u0026amp; Policy (Part 1) Create IAM User → 28-2.1 Create IAM Policy → 28-3 Create IAM Role → 28-4 "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.6-week6/1.6.2-day27-2025-10-14/","title":"Day 27 - Amazon RDS &amp; Aurora","tags":[],"description":"","content":"Date: 2025-10-14\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes RDBMS vs NoSQL RDBMS An RDBMS stores data in related tables (rows/columns), enforces integrity constraints, uses SQL, and provides ACID guarantees. Popular engines: Oracle, MySQL, SQL Server, PostgreSQL, IBM Db2. NoSQL Overview NoSQL systems target un/semistructured data with high scalability and performance. Types: Document (MongoDB, CouchDB) Key–Value (Redis, DynamoDB) Column-Family (Cassandra, HBase) Graph (Neo4j, Amazon Neptune) Traits: schema flexibility, horizontal scaling, big-data friendliness, CAP-oriented designs. RDBMS vs. NoSQL (high-level) OLTP vs. OLAP OLTP: many small, concurrent transactions; normalized data; short queries; index-heavy. OLAP: complex analytics over large historical datasets; star/snowflake schemas; read-heavy. Amazon RDS \u0026amp; Aurora Amazon Relational Database Service (RDS) Managed relational databases that simplify provisioning, patching, backups, and HA.\nSupported engines: MySQL, PostgreSQL, MariaDB, Oracle, SQL Server, Amazon Aurora. Key features: automated backups/patching, easy scaling, Multi-AZ high availability, encryption \u0026amp; VPC/IAM/SSL security. Deployment options: Single-AZ Multi-AZ (synchronous standby in another AZ) Read Replicas for scaling reads RDS Features:\nAutomated Backups: Point-in-time recovery up to 35 days Manual Snapshots: User-initiated backups Multi-AZ: Automatic failover for high availability Read Replicas: Scale read workloads (up to 15 replicas) Parameter Groups: Database configuration management Option Groups: Additional features (e.g., Oracle Advanced Security) Amazon Aurora Cloud-native, MySQL/PostgreSQL-compatible relational database re-architected for AWS.\nHighlights: Up to ~5× MySQL / ~3× PostgreSQL performance (typical benchmarks) Storage auto-scales to 128 TB Six-way replication across three AZs; self-healing storage Aurora Serverless (on-demand capacity) Global Database for low-latency multi-region Aurora Features:\nAurora Replicas: Up to 15 read replicas with sub-10ms lag Aurora Serverless: Auto-scaling compute capacity Aurora Global Database: Cross-region replication \u0026lt; 1 second Aurora Backtrack: Rewind database to specific point in time Aurora Parallel Query: Faster analytics on current data Aurora Machine Learning: Native ML integration Aurora vs RDS:\nFeature Aurora RDS Performance 5x MySQL, 3x PostgreSQL Standard Storage Auto-scaling to 128 TB Manual scaling Replicas Up to 15 Up to 5 (MySQL) Failover \u0026lt; 30 seconds 1-2 minutes Backtrack Yes No Hands-On Labs Lab 05 – Amazon RDS \u0026amp; EC2 Integration (Part 2) Create EC2 Instance → 05-3 Create RDS Database Instance → 05-4 Application Deployment → 05-5 Backup and Restore → 05-6 Clean Up Resources → 05-7 Lab 43 – AWS Database Migration Service (DMS) (Part 1) EC2 Connect RDP Client → 43-01 EC2 Connect Fleet Manager → 43-02 SQL Server Source Config → 43-03 Oracle Connect Source DB → 43-04 Oracle Config Source DB → 43-05 Drop Constraint → 43-06 "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.7-week7/1.7.2-day32-2025-10-21/","title":"Day 32 - Contract-First &amp; Mocking","tags":[],"description":"","content":"Date: 2025-10-21\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Contract-First Development Quy trình 5 bước Viết OpenAPI spec để định nghĩa contract. Chia sẻ spec cho cả frontend và backend như Single Source of Truth. Frontend dựng UI với mock data dựa trên contract. Backend implement API bám sát schema (status code, payload). Chạy contract testing để đảm bảo backend tuân thủ spec. paths: /books/{id}: get: summary: Get book detail responses: \u0026#34;200\u0026#34;: $ref: \u0026#34;#/components/responses/BookDetail\u0026#34; Lợi ích Hạn chế mismatch API vì mọi người xem chung một spec. Documentation, mock server, test script đều sinh tự động. Dễ review và cập nhật version của API trước khi triển khai thật. Insight Contract trước code giúp giảm ~80% lỗi integration khi hai team làm song song.\nMock API với Prism Prism dùng OpenAPI để sinh response giả, cho phép frontend test UI sớm. Hỗ trợ nhiều scenario (200, 404, 500) chỉ bằng cách gán example trong spec. Giúp giữ nhịp làm việc khi backend chưa xong hoặc đang refactor. Khi nên dùng Sprint đầu tiên của vertical slice. Cần trình diễn flow mà chưa có dữ liệu thật. Muốn viết test tự động cho UI dựa trên contract. Ghi chú vận hành Chạy Prism ở localhost:4010, trỏ Next.js đến mock qua NEXT_PUBLIC_API_URL. Đảm bảo CORS headers trong mock giống với backend sản xuất. Luôn commit spec trước khi mock để mọi người dùng đúng phiên bản. Hands-On Labs Thiết lập OpenAPI spec cho endpoint /books/{id}. Khởi chạy Prism mock server và kiểm tra flow trên UI. Viết checklist review contract (status code, schema, example data). "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.8-week8/1.8.2-day37-2025-10-28/","title":"Day 37 - Advanced Text Preprocessing","tags":[],"description":"","content":"Date: 2025-10-28 Status: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Stop Word Removal Understanding Stop Words Common words with little semantic value: \u0026ldquo;the\u0026rdquo;, \u0026ldquo;is\u0026rdquo;, \u0026ldquo;at\u0026rdquo;, \u0026ldquo;and\u0026rdquo;, etc. Removing them reduces noise and computational cost. Context matters: for some tasks, stop words are important. When to Remove Stop Words Text classification: usually beneficial Sentiment analysis: be careful (\u0026ldquo;not\u0026rdquo; is crucial) Information retrieval: helps focus on content words Named entity recognition: may need context from stop words Stemming What is Stemming? Reducing words to their root form by removing suffixes. \u0026ldquo;running\u0026rdquo;, \u0026ldquo;runs\u0026rdquo;, \u0026ldquo;ran\u0026rdquo; → \u0026ldquo;run\u0026rdquo; Fast but can produce non-words (e.g., \u0026ldquo;studies\u0026rdquo; → \u0026ldquo;studi\u0026rdquo;) Common Stemming Algorithms Porter Stemmer: most widely used, moderate accuracy Snowball Stemmer: improved version of Porter Lancaster Stemmer: most aggressive, may over-stem Lemmatization What is Lemmatization? Reducing words to their dictionary form (lemma). \u0026ldquo;running\u0026rdquo; → \u0026ldquo;run\u0026rdquo;, \u0026ldquo;better\u0026rdquo; → \u0026ldquo;good\u0026rdquo; More accurate than stemming but slower. Requires part-of-speech information for best results. Stemming vs Lemmatization Aspect Stemming Lemmatization Speed Fast Slower Accuracy Lower Higher Output May not be real words Always real words Use case Quick analysis Accurate analysis Key Insights Preprocessing choices depend on your specific task. Lemmatization generally better for production systems. Always validate preprocessing impact on model performance. Document your preprocessing pipeline for reproducibility. Hands-On Labs Lab 1: Stop Word Removal from nltk.corpus import stopwords from nltk.tokenize import word_tokenize stop_words = set(stopwords.words(\u0026#39;english\u0026#39;)) text = \u0026#34;This is an example sentence demonstrating stop word removal.\u0026#34; words = word_tokenize(text.lower()) filtered_words = [word for word in words if word not in stop_words] print(\u0026#34;Original:\u0026#34;, words) print(\u0026#34;Filtered:\u0026#34;, filtered_words) Lab 2: Stemming Comparison from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer words = [\u0026#34;running\u0026#34;, \u0026#34;runs\u0026#34;, \u0026#34;ran\u0026#34;, \u0026#34;easily\u0026#34;, \u0026#34;fairly\u0026#34;, \u0026#34;studies\u0026#34;] porter = PorterStemmer() snowball = SnowballStemmer(\u0026#39;english\u0026#39;) lancaster = LancasterStemmer() print(\u0026#34;Word\\t\\tPorter\\t\\tSnowball\\tLancaster\u0026#34;) print(\u0026#34;-\u0026#34; * 60) for word in words: print(f\u0026#34;{word}\\t\\t{porter.stem(word)}\\t\\t{snowball.stem(word)}\\t\\t{lancaster.stem(word)}\u0026#34;) Lab 3: Lemmatization from nltk.stem import WordNetLemmatizer from nltk.corpus import wordnet lemmatizer = WordNetLemmatizer() words = [\u0026#34;running\u0026#34;, \u0026#34;runs\u0026#34;, \u0026#34;ran\u0026#34;, \u0026#34;better\u0026#34;, \u0026#34;studying\u0026#34;, \u0026#34;studies\u0026#34;] print(\u0026#34;Word\\t\\tLemma (verb)\\tLemma (noun)\u0026#34;) print(\u0026#34;-\u0026#34; * 50) for word in words: verb_lemma = lemmatizer.lemmatize(word, pos=wordnet.VERB) noun_lemma = lemmatizer.lemmatize(word, pos=wordnet.NOUN) print(f\u0026#34;{word}\\t\\t{verb_lemma}\\t\\t{noun_lemma}\u0026#34;) Lab 4: Complete Preprocessing Pipeline import nltk from nltk.tokenize import word_tokenize from nltk.corpus import stopwords from nltk.stem import WordNetLemmatizer import string def preprocess_text(text): # Lowercase text = text.lower() # Tokenize tokens = word_tokenize(text) # Remove punctuation and stop words stop_words = set(stopwords.words(\u0026#39;english\u0026#39;)) tokens = [word for word in tokens if word not in string.punctuation and word not in stop_words] # Lemmatize lemmatizer = WordNetLemmatizer() tokens = [lemmatizer.lemmatize(word) for word in tokens] return tokens sample = \u0026#34;The students are studying NLP concepts. They\u0026#39;re learning quickly!\u0026#34; processed = preprocess_text(sample) print(\u0026#34;Processed tokens:\u0026#34;, processed) Practice Exercises Compare preprocessing results with and without stop word removal Build a custom stop words list for a specific domain Analyze which stemmer works best for your use case Create a preprocessing pipeline function for tweets "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.9-week9/1.9.2-day42-2025-11-04/","title":"Day 42 - Advanced Text Classification","tags":[],"description":"","content":"Date: 2025-11-04\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Text Classification Revision Key Concepts from Week 8 Text classification assigns predefined categories to documents Feature extraction: TF-IDF, word embeddings, contextualized representations Common algorithms: Naive Bayes, SVM, Logistic Regression, Neural Networks Evaluation metrics: accuracy, precision, recall, F1-score Advanced Feature Engineering TF-IDF Deep Dive\nTerm Frequency: how often a word appears in a document Inverse Document Frequency: how rare a word is across all documents Formula: TF-IDF = TF × log(N/DF) Balances common and distinctive terms N-gram Features\nUnigrams: single words Bigrams: two consecutive words Trigrams: three consecutive words Character n-grams: useful for handling typos and morphology Advanced Classification Techniques Ensemble Methods Why Ensemble?\nCombine multiple models for better performance Reduce overfitting through diversity More robust to noise and outliers Common Approaches\nVoting: majority vote from multiple classifiers Bagging: Bootstrap Aggregating (e.g., Random Forest) Boosting: sequential model improvement (e.g., XGBoost, AdaBoost) Stacking: meta-learner combines base models Handling Imbalanced Data Problem:\nReal-world data often has unequal class distribution Models biased toward majority class Poor performance on minority class Solutions:\nOversampling: SMOTE (Synthetic Minority Over-sampling) Undersampling: reduce majority class samples Class weights: penalize errors on minority class more Ensemble methods: EasyEnsemble, BalancedRandomForest Cross-Validation Strategies K-Fold Cross-Validation\nSplit data into K folds Train on K-1 folds, validate on remaining fold Repeat K times, average results Reduces overfitting, better performance estimate Stratified K-Fold\nMaintains class distribution in each fold Critical for imbalanced datasets Ensures representative training/validation sets Key Insights Feature engineering often more important than algorithm choice Always baseline with simple models before complex ones Cross-validation essential for reliable performance estimates Domain knowledge crucial for feature selection Monitor both overall accuracy and per-class performance Hands-On Labs Lab 1: Advanced TF-IDF Features from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import classification_report import pandas as pd # Sample dataset texts = [ \u0026#34;Machine learning is fascinating\u0026#34;, \u0026#34;Deep learning neural networks\u0026#34;, \u0026#34;Natural language processing\u0026#34;, \u0026#34;Computer vision image recognition\u0026#34;, \u0026#34;Reinforcement learning agents\u0026#34;, \u0026#34;Supervised learning algorithms\u0026#34;, \u0026#34;Unsupervised clustering methods\u0026#34;, \u0026#34;Classification and regression\u0026#34;, ] labels = [\u0026#34;ML\u0026#34;, \u0026#34;DL\u0026#34;, \u0026#34;NLP\u0026#34;, \u0026#34;CV\u0026#34;, \u0026#34;RL\u0026#34;, \u0026#34;ML\u0026#34;, \u0026#34;ML\u0026#34;, \u0026#34;ML\u0026#34;] # Advanced TF-IDF with n-grams vectorizer = TfidfVectorizer( max_features=100, ngram_range=(1, 3), # unigrams, bigrams, trigrams min_df=1, max_df=0.8, sublinear_tf=True # apply sublinear tf scaling ) X = vectorizer.fit_transform(texts) print(f\u0026#34;Feature matrix shape: {X.shape}\u0026#34;) print(f\u0026#34;\\nTop features: {vectorizer.get_feature_names_out()[:20]}\u0026#34;) # Analyze feature importance feature_names = vectorizer.get_feature_names_out() tfidf_scores = X.toarray().sum(axis=0) feature_importance = pd.DataFrame({ \u0026#39;feature\u0026#39;: feature_names, \u0026#39;importance\u0026#39;: tfidf_scores }).sort_values(\u0026#39;importance\u0026#39;, ascending=False) print(\u0026#34;\\nTop 10 most important features:\u0026#34;) print(feature_importance.head(10)) Lab 2: Ensemble Classification from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier from sklearn.svm import SVC from sklearn.naive_bayes import MultinomialNB import numpy as np # Sample data preparation texts = [ \u0026#34;excellent product highly recommend\u0026#34;, \u0026#34;terrible waste of money\u0026#34;, \u0026#34;good value for price\u0026#34;, \u0026#34;worst purchase ever made\u0026#34;, \u0026#34;amazing quality very satisfied\u0026#34;, \u0026#34;disappointing not worth it\u0026#34;, \u0026#34;fantastic experience love it\u0026#34;, \u0026#34;poor quality broke quickly\u0026#34; ] * 10 # Repeat for more data labels = [\u0026#34;positive\u0026#34;, \u0026#34;negative\u0026#34;, \u0026#34;positive\u0026#34;, \u0026#34;negative\u0026#34;, \u0026#34;positive\u0026#34;, \u0026#34;negative\u0026#34;, \u0026#34;positive\u0026#34;, \u0026#34;negative\u0026#34;] * 10 X_train, X_test, y_train, y_test = train_test_split( texts, labels, test_size=0.2, random_state=42, stratify=labels ) # Vectorize vectorizer = TfidfVectorizer(max_features=50) X_train_vec = vectorizer.fit_transform(X_train) X_test_vec = vectorizer.transform(X_test) # Individual classifiers nb_clf = MultinomialNB() svm_clf = SVC(kernel=\u0026#39;linear\u0026#39;, probability=True) rf_clf = RandomForestClassifier(n_estimators=100, random_state=42) # Voting Ensemble voting_clf = VotingClassifier( estimators=[ (\u0026#39;nb\u0026#39;, nb_clf), (\u0026#39;svm\u0026#39;, svm_clf), (\u0026#39;rf\u0026#39;, rf_clf) ], voting=\u0026#39;soft\u0026#39; # use probability estimates ) # Train and evaluate each classifier classifiers = { \u0026#39;Naive Bayes\u0026#39;: nb_clf, \u0026#39;SVM\u0026#39;: svm_clf, \u0026#39;Random Forest\u0026#39;: rf_clf, \u0026#39;Voting Ensemble\u0026#39;: voting_clf } for name, clf in classifiers.items(): clf.fit(X_train_vec, y_train) score = clf.score(X_test_vec, y_test) print(f\u0026#34;{name} Accuracy: {score:.4f}\u0026#34;) Lab 3: Handling Imbalanced Data from imblearn.over_sampling import SMOTE from imblearn.under_sampling import RandomUnderSampler from imblearn.pipeline import Pipeline as ImbPipeline from collections import Counter # Create imbalanced dataset texts_imbalanced = [ \u0026#34;spam message win prize\u0026#34;, \u0026#34;spam free money click\u0026#34;, \u0026#34;spam urgent account update\u0026#34;, ] + [ \u0026#34;normal email from colleague\u0026#34;, \u0026#34;normal meeting invitation tomorrow\u0026#34;, \u0026#34;normal project update report\u0026#34;, \u0026#34;normal lunch plans friday\u0026#34;, \u0026#34;normal document attachment review\u0026#34;, \u0026#34;normal weekly team meeting\u0026#34;, \u0026#34;normal budget approval request\u0026#34;, ] * 5 labels_imbalanced = [\u0026#34;spam\u0026#34;] * 3 + [\u0026#34;normal\u0026#34;] * 35 print(f\u0026#34;Original distribution: {Counter(labels_imbalanced)}\u0026#34;) # Vectorize X_imb = vectorizer.fit_transform(texts_imbalanced) y_imb = labels_imbalanced # Strategy 1: Class Weights lr_weighted = LogisticRegression(class_weight=\u0026#39;balanced\u0026#39;, max_iter=1000) lr_weighted.fit(X_imb, y_imb) print(f\u0026#34;\\nWeighted Logistic Regression trained\u0026#34;) # Strategy 2: SMOTE smote = SMOTE(random_state=42) X_resampled, y_resampled = smote.fit_resample(X_imb.toarray(), y_imb) print(f\u0026#34;After SMOTE: {Counter(y_resampled)}\u0026#34;) lr_smote = LogisticRegression(max_iter=1000) lr_smote.fit(X_resampled, y_resampled) print(f\u0026#34;SMOTE Logistic Regression trained\u0026#34;) # Strategy 3: Undersampling undersample = RandomUnderSampler(random_state=42) X_under, y_under = undersample.fit_resample(X_imb.toarray(), y_imb) print(f\u0026#34;After Undersampling: {Counter(y_under)}\u0026#34;) Lab 4: Cross-Validation for Robust Evaluation from sklearn.model_selection import cross_val_score, StratifiedKFold from sklearn.metrics import make_scorer, f1_score # Prepare data texts_cv = [ \u0026#34;positive review great product\u0026#34;, \u0026#34;negative review bad quality\u0026#34;, \u0026#34;positive review excellent service\u0026#34;, \u0026#34;negative review poor experience\u0026#34;, ] * 20 labels_cv = [\u0026#34;positive\u0026#34;, \u0026#34;negative\u0026#34;, \u0026#34;positive\u0026#34;, \u0026#34;negative\u0026#34;] * 20 X_cv = vectorizer.fit_transform(texts_cv) y_cv = labels_cv # Stratified K-Fold skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) classifiers_cv = { \u0026#39;Logistic Regression\u0026#39;: LogisticRegression(max_iter=1000), \u0026#39;Random Forest\u0026#39;: RandomForestClassifier(n_estimators=100, random_state=42), \u0026#39;SVM\u0026#39;: SVC(kernel=\u0026#39;linear\u0026#39;) } for name, clf in classifiers_cv.items(): # Accuracy scores cv_scores = cross_val_score(clf, X_cv, y_cv, cv=skf, scoring=\u0026#39;accuracy\u0026#39;) # F1 scores f1_scores = cross_val_score(clf, X_cv, y_cv, cv=skf, scoring=make_scorer(f1_score, average=\u0026#39;weighted\u0026#39;)) print(f\u0026#34;\\n{name}:\u0026#34;) print(f\u0026#34; Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\u0026#34;) print(f\u0026#34; F1 Score: {f1_scores.mean():.4f} (+/- {f1_scores.std() * 2:.4f})\u0026#34;) print(f\u0026#34; Fold scores: {cv_scores}\u0026#34;) Lab 5: Complete Advanced Classification Pipeline from sklearn.pipeline import Pipeline from sklearn.model_selection import GridSearchCV # Build pipeline pipeline = Pipeline([ (\u0026#39;tfidf\u0026#39;, TfidfVectorizer()), (\u0026#39;clf\u0026#39;, RandomForestClassifier(random_state=42)) ]) # Parameter grid for tuning param_grid = { \u0026#39;tfidf__max_features\u0026#39;: [50, 100, 200], \u0026#39;tfidf__ngram_range\u0026#39;: [(1, 1), (1, 2), (1, 3)], \u0026#39;tfidf__min_df\u0026#39;: [1, 2], \u0026#39;clf__n_estimators\u0026#39;: [50, 100, 200], \u0026#39;clf__max_depth\u0026#39;: [None, 10, 20], \u0026#39;clf__min_samples_split\u0026#39;: [2, 5] } # Grid search with cross-validation grid_search = GridSearchCV( pipeline, param_grid, cv=5, scoring=\u0026#39;f1_weighted\u0026#39;, n_jobs=-1, verbose=1 ) # Sample data texts_full = texts_cv labels_full = labels_cv # Fit print(\u0026#34;Starting grid search...\u0026#34;) grid_search.fit(texts_full, labels_full) # Results print(f\u0026#34;\\nBest parameters: {grid_search.best_params_}\u0026#34;) print(f\u0026#34;Best cross-validation score: {grid_search.best_score_:.4f}\u0026#34;) # Test best model best_model = grid_search.best_estimator_ test_texts = [\u0026#34;amazing product\u0026#34;, \u0026#34;terrible service\u0026#34;] predictions = best_model.predict(test_texts) print(f\u0026#34;\\nTest predictions: {predictions}\u0026#34;) Practice Exercises Build a multi-class text classifier for news categories Implement a pipeline with custom feature extractors Compare different resampling strategies on imbalanced data Perform hyperparameter tuning with cross-validation Analyze feature importance and model interpretability "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.10-week10/1.10.2-day47-2025-11-11/","title":"Day 47 - Book Upload Flow &amp; Presigned URLs","tags":[],"description":"","content":"Date: 2025-11-11\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Upload Flow Architecture S3 Presigned URL Strategy Why Presigned URLs?\nDirect upload to S3 bypasses API Gateway payload limits (10MB) Reduces Lambda execution time and cost Better user experience with progress tracking Serverless architecture scales automatically Upload Flow:\nUser → Request Upload URL (API) → Lambda generates Presigned PUT URL → User uploads directly to S3 → S3 Event Notification → Lambda updates DynamoDB Security Considerations Presigned URL Configuration:\nShort expiration time (15 minutes for upload) Restrict file size via content-length-range condition Limit to specific MIME types (application/pdf, application/epub+zip) Upload to temporary uploads/ prefix, not public folder Validation Strategy:\nClient-side: check file size and type before requesting URL Server-side: Lambda validates metadata before generating URL S3 Event: post-upload Lambda validates actual file MIME type using magic bytes Status tracking: PENDING → VALIDATING → APPROVED/REJECTED Frontend Upload Implementation Multi-Step Upload Process Step 1: File Selection \u0026amp; Validation\nDrag-and-drop or file picker interface Client-side validation (size ≤50MB, type PDF/ePub) Preview file metadata (name, size, type) Step 2: Metadata Collection\nForm fields: title, author, description, cover image Genre/category selection Optional: ISBN, publication year, language Step 3: Request Upload URL\nPOST to /upload API endpoint with metadata Receive presigned URL and book ID Store book ID for status tracking Step 4: Direct S3 Upload\nPUT request to presigned URL with file content Track upload progress with XMLHttpRequest or fetch Show progress bar and estimated time remaining Step 5: Status Monitoring\nPoll /books/{id}/status endpoint Display current status (PENDING, VALIDATING, APPROVED, REJECTED) Show rejection reason if applicable Key Insights Presigned URLs are time-bound credentials - handle expiration gracefully Always validate on both client and server to prevent malicious uploads Progress tracking significantly improves user experience for large files S3 Event Notifications enable async validation without polling Clear status messages help users understand upload workflow Tasks Completed Upload UI Components\nCreated file upload component with drag-and-drop Built metadata form with validation (React Hook Form + Zod) Implemented upload progress indicator with percentage Added file type and size validation API Integration\nCreated API client methods for upload flow Implemented createUploadUrl API call with metadata Built direct S3 upload with progress tracking Added error handling for network failures Status Tracking\nCreated status polling mechanism with intervals Built status badge component (PENDING, APPROVED, REJECTED) Implemented real-time status updates Added notification system for status changes User Experience\nDesigned upload wizard with multi-step form Added file preview before upload Implemented success/error notifications Created \u0026ldquo;My Uploads\u0026rdquo; page showing all user uploads Error Handling\nHandled presigned URL expiration Implemented retry logic for failed uploads Added validation error messages Created fallback UI for upload failures Challenges \u0026amp; Solutions Challenge: Large file uploads timing out\nSolution: Implemented chunked upload with retry logic for failed chunks\nChallenge: Presigned URL expiring during slow uploads\nSolution: Request new URL if upload takes longer than 10 minutes\nChallenge: User closing browser during upload\nSolution: Added localStorage tracking to resume uploads and show warning before page unload\nChallenge: Progress tracking not accurate\nSolution: Used XMLHttpRequest upload.onprogress event for accurate byte-level tracking\n"},{"uri":"https://anquoc211.github.io/AWS_Internship/5-workshop/5.2-prerequisite/","title":"Prerequisites","tags":[],"description":"","content":"Account \u0026amp; Permissions An AWS account with permissions to create Cognito User Pools, IAM roles, and Amplify resources. No VPC or advanced networking permissions required for this workshop. Tools Node.js 18+ and npm installed on your local machine. AWS Amplify CLI: Install with npm install -g @aws-amplify/cli A code editor (VS Code, Sublime, or any preferred editor). Web browser for testing the authentication flow. Quick Setup Configure Amplify CLI:\namplify configure Follow the prompts to set up an IAM user with appropriate permissions.\nSelect a region close to you (e.g., us-east-1 or ap-southeast-1).\nVerify installations:\nnode --version # Should be 18+ npm --version amplify --version "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.2-week2/","title":"Week 2 - AWS Networking Services","tags":[],"description":"","content":"Week: 2025-09-15 đến 2025-09-19\nStatus: \u0026ldquo;Done\u0026rdquo;\nTổng quan tuần 2 Tuần này tập trung vào các dịch vụ mạng của AWS, từ VPC cơ bản đến các giải pháp kết nối nâng cao.\nNội dung chính Amazon VPC và Subnets Security Groups và NACLs Internet Gateway, NAT Gateway VPC Peering và Transit Gateway Elastic Load Balancing (ALB, NLB, GWLB) Labs thực hành Lab 03: Amazon VPC \u0026amp; Networking Basics Lab 10: Hybrid DNS (Route 53 Resolver) Lab 19: VPC Peering Lab 20: AWS Transit Gateway "},{"uri":"https://anquoc211.github.io/AWS_Internship/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - Supercharge Your Organization\u0026rsquo;s Productivity with Amazon Q Business Browser Extension This blog explores how Amazon Q Business browser extension brings generative AI capabilities directly into web browsers, enabling employees to receive context-aware AI assistance for their daily tasks. The article demonstrates how organizations can deploy this browser extension to help teams analyze web content, improve content quality, and access AI-driven insights seamlessly. It includes practical guidance on prerequisites, configuration steps, deployment options, customization possibilities, and real-world use cases showing how the extension increases workplace productivity by connecting company data and enterprise systems with browser-based workflows.\nBlog 2 - Building Agentic Workflows with OpenAI GPT OSS on Amazon SageMaker AI and Amazon Bedrock AgentCore This blog demonstrates how to build sophisticated agentic workflows using OpenAI\u0026rsquo;s open-weight GPT OSS models (120B and 20B parameters) deployed on Amazon SageMaker AI. The article focuses on creating a stock analysis agent assistant using LangGraph for workflow orchestration and state management, then deploying these agents to Amazon Bedrock AgentCore for scalable production operations. Readers will learn about the Mixture of Experts (MoE) architecture, multi-agent coordination patterns, and how to leverage SageMaker\u0026rsquo;s fully managed inference capabilities combined with Bedrock\u0026rsquo;s unified orchestration layer to build practical AI agent applications.\nBlog 3 - Getting Started with Healthcare Data Lakes: Using Microservices This blog introduces how to build healthcare data lakes using a microservices architecture to transform healthcare data into actionable business insights while protecting patient privacy. The article details the evolution from a monolithic service to loosely coupled microservices connected through a centralized pub/sub messaging hub, enabling better maintainability and flexibility when integrating diverse healthcare data formats. Readers will learn about architectural design decisions, handling HL7v2 message ingestion and parsing through REST interfaces, using Amazon Cognito with Attribute-Based Access Control (ABAC) for authentication and authorization, and breaking down data silos to combine different analytics types for better decision-making in healthcare environments.\n"},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.1-week1/1.1.3-day03-2025-09-10/","title":"Day 03 - AWS Management Tools","tags":[],"description":"","content":"Date: 2025-09-10\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes AWS Management Tools AWS Management Console Log in as Root User or IAM User (requires 12-digit Account ID). Search and access individual service dashboards. Support Center allows you to open support cases directly. AWS Command Line Interface (CLI) Open-source command-line tool for interacting with AWS services. Provides functionality equivalent to the Console. Key Features:\nCross-platform support (Windows, macOS, Linux) Scriptable and automatable Direct access to AWS service APIs Supports profiles for multiple accounts AWS SDK (Software Development Kit) Simplifies integration of AWS services within applications. Handles authentication, retries, and data serialization/deserialization automatically. Supported Languages:\nPython (Boto3) JavaScript/Node.js Java .NET Ruby, PHP, Go, and more "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.2-week2/1.2.3-day08-2025-09-17/","title":"Day 08 - VPC Security &amp; Flow Logs","tags":[],"description":"","content":"Date: 2025-09-17\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes VPC Security Security Group (SG) A stateful virtual firewall that controls inbound and outbound traffic to AWS resources. Rules are based on protocol, port, source, or another security group. Only allow rules are supported. Applied to Elastic Network Interfaces (ENIs). Security Group Characteristics:\nStateful: return traffic automatically allowed Supports allow rules only Evaluates all rules before deciding Applies to instance level (ENI) Network Access Control List (NACL) A stateless virtual firewall that operates at the subnet level. Rules control inbound and outbound traffic by protocol, port, and source. Default NACL allows all traffic. NACL Characteristics:\nStateless: must explicitly allow return traffic Supports both allow and deny rules Rules processed in number order Applies to subnet level VPC Flow Logs Capture metadata about IP traffic to and from network interfaces in your VPC. Logs can be delivered to Amazon CloudWatch Logs or S3. Flow Logs do not record packet payloads. Flow Log Use Cases:\nTroubleshoot connectivity issues Monitor traffic patterns Security analysis Compliance requirements Hands-On Labs Lab 03 – Amazon VPC \u0026amp; Networking (continued) Launch EC2 Instances in Subnets → 04-1 Test Connection Between Instances → 04-2 Create NAT Gateway (Private ↔ Internet) → 04-3 EC2 Instance Connect Endpoint (no bastion) → 04-5 "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.3-week3/1.3.3-day13-2025-09-24/","title":"Day 13 - Instance Store &amp; User Data","tags":[],"description":"","content":"Date: 2025-09-24\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes EC2 Advanced Features Instance Store Instance Store provides temporary block-level storage physically attached to the EC2 host. Characteristics\nVery high I/O and throughput Data lost when instance stops or terminates Cannot be detached or snapshotted Use Cases\nCaching or temporary data processing Applications with their own redundancy or replication Instance Store vs EBS:\nFeature Instance Store EBS Persistence Temporary Persistent Performance Very high High Snapshot No Yes Detachable No Yes Cost Included Additional User Data User Data scripts run automatically at instance launch (once per AMI provision). Linux – bash scripts Windows – PowerShell scripts User Data Examples:\n#!/bin/bash yum update -y yum install -y httpd systemctl start httpd systemctl enable httpd echo \u0026#34;\u0026lt;h1\u0026gt;Hello from $(hostname -f)\u0026lt;/h1\u0026gt;\u0026#34; \u0026gt; /var/www/html/index.html Metadata EC2 Instance Metadata provides details about the running instance such as private/public IP, hostname, and security groups. Often used in user data scripts for dynamic configuration. Accessing Metadata:\n# Get instance ID curl http://169.254.169.254/latest/meta-data/instance-id # Get public IP curl http://169.254.169.254/latest/meta-data/public-ipv4 # Get IAM role credentials curl http://169.254.169.254/latest/meta-data/iam/security-credentials/role-name Hands-On Labs Lab 07 – AWS Budgets \u0026amp; Cost Management Create Budget by Template → 07-01 Create Cost Budget Tutorial → 07-02 Create Usage Budget → 07-03 Create Reserved Instance Budget → 07-04 Create Savings Plans Budget → 07-05 Clean Up Budgets → 07-06 "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.4-week4/1.4.3-day18-2025-10-01/","title":"Day 18 - AWS Snow Family &amp; Hybrid Storage","tags":[],"description":"","content":"Date: 2025-10-01\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes AWS Snow Family Purpose-built devices and services to move large datasets into and out of AWS when networks are limited or data volumes are massive.\nAWS Snowcone: Small, rugged device (~8 TB). Suited for edge and remote sites. AWS Snowball: Snowball Edge Storage Optimized: Up to ~80 TB usable storage. Snowball Edge Compute Optimized: Adds powerful compute with ~42 TB storage. AWS Snowmobile: Exabyte-scale data transfer (up to 100 PB) in a secure containerized data center. Snow Family Comparison:\nDevice Storage Compute Use Case Snowcone 8 TB 2 vCPUs Edge, IoT Snowball Storage 80 TB 40 vCPUs Data migration Snowball Compute 42 TB 52 vCPUs Edge computing Snowmobile 100 PB N/A Datacenter migration When to Use Snow Family:\nLimited or expensive bandwidth Large data volumes (TB to PB) Remote or disconnected locations Edge computing requirements Regulatory data residency AWS Storage Gateway Hybrid cloud storage service that connects on-premises applications with cloud-backed storage.\nGateway Types File Gateway\nNFS/SMB file shares backed by S3 objects. Use cases: user shares, application backups, archives. Volume Gateway\niSCSI block storage backed by S3 with EBS snapshots. Modes: Cached volumes: Primary data in S3; local cache on-prem. Stored volumes: Primary data on-prem; async copy to S3. Use cases: on-prem block workloads with cloud backup/DR. Tape Gateway\nVirtual Tape Library (VTL) for existing backup apps (e.g., NetBackup, Veeam). Writes appear as tape but land in S3/Glacier. Use cases: tape replacement and archival modernization. Hands-On Labs Lab 24 – AWS Storage Gateway (On-Premises Integration) Create Storage Gateway → 24-2.1 Create File Shares → 24-2.2 Mount File Shares On-Prem → 24-2.3 Clean Up Resources → 24-3 Lab 14 – AWS VM Import/Export (Part 1) VMware Workstation → 14-01 Export Virtual Machine from On-Premises → 14-02.1 Upload Virtual Machine to AWS → 14-02.2 "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.5-week5/1.5.3-day23-2025-10-08/","title":"Day 23 - Amazon Cognito &amp; Organizations","tags":[],"description":"","content":"Date: 2025-10-08\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Amazon Cognito Managed authentication/authorization and user management for web \u0026amp; mobile apps. Components: User Pools: Sign-up/sign-in user directories. Identity Pools: Federated identities for temporary AWS credentials to access services. Cognito User Pools Features:\nSign-up and sign-in Social identity providers (Google, Facebook, Amazon) SAML identity providers Multi-factor authentication (MFA) Email and phone verification Custom authentication flows Lambda triggers for customization Cognito Identity Pools Features:\nTemporary AWS credentials Authenticated and unauthenticated access Role-based access control Integration with User Pools Support for external identity providers AWS Organizations Centrally manage multiple AWS accounts in a single organization. Benefits\nCentralized account management Consolidated Billing Hierarchies with Organizational Units (OUs) Guardrails with Service Control Policies (SCPs) Organizational Units (OUs) Group accounts by department, project, or environment; nest OUs for hierarchical policies. Example OU Structure:\nRoot ├── Production OU │ ├── Web App Account │ └── Database Account ├── Development OU │ ├── Dev Account │ └── Test Account └── Security OU └── Audit Account Consolidated Billing One invoice for all accounts; volume pricing benefits; no extra cost. Benefits:\nVolume discounts across accounts Easier tracking and reporting Simplified payment method Reserved Instance sharing Hands-On Labs Lab 28 – IAM Cross-Region Role \u0026amp; Policy (Part 2) Switch Roles → 28-5.1 Access EC2 Console – Tokyo → 28-5.2.1 Access EC2 Console – N. Virginia → 28-5.2.2 Create EC2 (No Qualified Tags) → 28-5.2.3 Edit EC2 Resource Tag → 28-5.2.4 Policy Check → 28-5.2.5 Lab 27 – AWS Resource Groups \u0026amp; Tagging (Part 1) Create EC2 Instance with Tag → 27-2.1.1 Manage Tags in AWS Resources → 27-2.1.2 Filter Resources by Tag → 27-2.1.3 "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.6-week6/1.6.3-day28-2025-10-15/","title":"Day 28 - Amazon Redshift","tags":[],"description":"","content":"Date: 2025-10-15\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Amazon Redshift Fully managed cloud data warehouse optimized for large-scale analytics (OLAP).\nColumnar storage, compression, MPP execution; scales from hundreds of GB to PB. Integrations: S3, Kinesis, DynamoDB, BI tools; strong security features. Concurrency Scaling adds capacity automatically during spikes. Architecture: cluster (leader node + compute nodes), each compute node has slices. Deployment options:\nRedshift Provisioned Redshift Serverless Redshift Spectrum (query S3 directly) Use cases: enterprise BI, data lake analytics, dashboards, trend analysis, forecasting.\nRedshift Features:\nColumnar Storage: Optimized for analytics queries Massively Parallel Processing (MPP): Distributes queries across nodes Result Caching: Speeds up repeated queries Automatic Compression: Reduces storage costs Workload Management (WLM): Query prioritization Concurrency Scaling: Handle burst workloads Redshift vs Traditional Data Warehouse:\nFeature Redshift Traditional DW Setup Minutes Weeks/Months Scaling Elastic Fixed capacity Cost Pay-as-you-go Large upfront Maintenance Managed Self-managed Redshift Spectrum:\nQuery data directly in S3 without loading Separate compute and storage Support for various file formats (Parquet, ORC, JSON) Cost-effective for infrequently accessed data Hands-On Labs Lab 43 – AWS Database Migration Service (DMS) (Part 2) MSSQL → Aurora MySQL Target Config → 43-07 MSSQL → Aurora MySQL Create Project → 43-08 MSSQL → Aurora MySQL Schema Conversion → 43-09 Oracle → MySQL Schema Conversion (1) → 43-10 Create Migration Task \u0026amp; Endpoints → 43-11 "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.7-week7/1.7.3-day33-2025-10-22/","title":"Day 33 - Next.js App Router","tags":[],"description":"","content":"Date: 2025-10-22\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Next.js 16 App Router Tận dụng Server Components để fetch data trực tiếp từ server và tránh bundle dư thừa. Route /app/books/[id]/page.tsx xử lý data fetching, trả UI đã được render sẵn. generateMetadata cung cấp SEO meta dựa trên dữ liệu sách. // app/books/[id]/page.tsx import { getBook } from \u0026#34;@/lib/api\u0026#34;; export default async function BookDetail({ params }) { const book = await getBook(params.id); if (!book) return notFound(); return \u0026lt;BookPage book={book} /\u0026gt;; } Error \u0026amp; Not Found Handling Chỉ cần not-found.tsx cho case không tìm được sách → coi như expected flow. Không dùng error.tsx để tránh double handling; các lỗi còn lại log ở backend. Giữ UX nhất quán: hiển thị CTA quay lại danh sách và hotline hỗ trợ. Environment \u0026amp; Config Sử dụng biến môi trường rõ ràng: NEXT_PUBLIC_API_URL cho frontend, API_URL cho route handlers. Đảm bảo .env.example cập nhật khi thêm biến mới. Chuyển logic build URL vào helper (lib/api.ts) để tránh lặp. Insight Server Components giảm latency đáng kể khi render detail page. App Router giúp cấu trúc thư mục rõ ràng, dễ chia nhỏ khi thêm slice mới. Khi mock API, dùng fetch trực tiếp từ Prism, chỉ cần đổi base URL là xong. Hands-On Labs Tạo not-found.tsx tùy biến với CTA điều hướng. Viết helper getBook(id) tái sử dụng cho server components và tests. Kiểm tra build npm run lint \u0026amp;\u0026amp; npm run build đảm bảo cấu hình Next.js sạch. "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.8-week8/1.8.3-day38-2025-10-29/","title":"Day 38 - Text Analysis &amp; Sentiment Analysis","tags":[],"description":"","content":"Date: 2025-10-29 Status: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Basic Text Analysis Word Frequency Analysis Counting how often each word appears in text. Identifies most common terms and themes. Foundation for many NLP applications. Useful for understanding document content quickly. N-grams Sequences of N consecutive words. Unigrams: single words (\u0026ldquo;machine\u0026rdquo;) Bigrams: two words (\u0026ldquo;machine learning\u0026rdquo;) Trigrams: three words (\u0026ldquo;natural language processing\u0026rdquo;) Captures context and common phrases. Text Statistics Document length (words, sentences, characters) Vocabulary size (unique words) Average word length Lexical diversity (unique words / total words) Sentiment Analysis What is Sentiment Analysis? Determining emotional tone of text: positive, negative, or neutral. Applications: customer reviews, social media monitoring, brand reputation. Approaches: lexicon-based, machine learning, deep learning. Sentiment Polarity \u0026amp; Subjectivity Polarity: ranges from -1 (negative) to +1 (positive) Subjectivity: ranges from 0 (objective) to 1 (subjective) Both metrics provide nuanced understanding of text. Challenges in Sentiment Analysis Sarcasm and irony are difficult to detect. Context dependency: \u0026ldquo;This movie was sick!\u0026rdquo; (positive slang) Negation handling: \u0026ldquo;not good\u0026rdquo; vs \u0026ldquo;good\u0026rdquo; Domain-specific language and cultural differences. Key Insights Simple frequency analysis reveals surprising insights about documents. N-grams capture meaning that individual words miss. Sentiment analysis is powerful but imperfect - always validate results. Combining multiple analysis techniques provides richer understanding. Hands-On Labs Lab 1: Word Frequency Analysis from collections import Counter from nltk.tokenize import word_tokenize from nltk.corpus import stopwords import string text = \u0026#34;\u0026#34;\u0026#34; Natural Language Processing is a subfield of artificial intelligence. It focuses on the interaction between computers and human language. NLP enables machines to understand and generate human language. \u0026#34;\u0026#34;\u0026#34; # Preprocess tokens = word_tokenize(text.lower()) stop_words = set(stopwords.words(\u0026#39;english\u0026#39;)) tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation] # Frequency analysis freq_dist = Counter(tokens) print(\u0026#34;Top 10 most common words:\u0026#34;) for word, count in freq_dist.most_common(10): print(f\u0026#34;{word}: {count}\u0026#34;) Lab 2: N-gram Analysis from nltk import ngrams from collections import Counter def get_ngrams(text, n): tokens = word_tokenize(text.lower()) n_grams = list(ngrams(tokens, n)) return n_grams text = \u0026#34;Natural language processing enables machine learning applications\u0026#34; # Generate bigrams bigrams = get_ngrams(text, 2) print(\u0026#34;Bigrams:\u0026#34;, bigrams) # Generate trigrams trigrams = get_ngrams(text, 3) print(\u0026#34;Trigrams:\u0026#34;, trigrams) # Most common bigrams bigram_freq = Counter(bigrams) print(\u0026#34;\\nMost common bigrams:\u0026#34;, bigram_freq.most_common(5)) Lab 3: Text Statistics def calculate_text_statistics(text): # Tokenize words = word_tokenize(text) sentences = sent_tokenize(text) # Calculate statistics stats = { \u0026#39;total_words\u0026#39;: len(words), \u0026#39;unique_words\u0026#39;: len(set(words)), \u0026#39;total_sentences\u0026#39;: len(sentences), \u0026#39;avg_word_length\u0026#39;: sum(len(word) for word in words) / len(words), \u0026#39;avg_sentence_length\u0026#39;: len(words) / len(sentences), \u0026#39;lexical_diversity\u0026#39;: len(set(words)) / len(words) } return stats sample_text = \u0026#34;\u0026#34;\u0026#34; Natural Language Processing is fascinating. It enables computers to understand human language. NLP has many practical applications in today\u0026#39;s technology-driven world. \u0026#34;\u0026#34;\u0026#34; stats = calculate_text_statistics(sample_text) for key, value in stats.items(): print(f\u0026#34;{key}: {value:.2f}\u0026#34;) Lab 4: Sentiment Analysis with TextBlob from textblob import TextBlob def analyze_sentiment(text): blob = TextBlob(text) sentiment = blob.sentiment # Determine sentiment category if sentiment.polarity \u0026gt; 0.1: category = \u0026#34;Positive\u0026#34; elif sentiment.polarity \u0026lt; -0.1: category = \u0026#34;Negative\u0026#34; else: category = \u0026#34;Neutral\u0026#34; return { \u0026#39;polarity\u0026#39;: sentiment.polarity, \u0026#39;subjectivity\u0026#39;: sentiment.subjectivity, \u0026#39;category\u0026#39;: category } # Test with different sentences texts = [ \u0026#34;I love this product! It\u0026#39;s amazing and works perfectly.\u0026#34;, \u0026#34;This is the worst experience I\u0026#39;ve ever had.\u0026#34;, \u0026#34;The item arrived on time and matches the description.\u0026#34;, \u0026#34;I\u0026#39;m not sure if I like this or not.\u0026#34; ] for text in texts: result = analyze_sentiment(text) print(f\u0026#34;\\nText: {text}\u0026#34;) print(f\u0026#34;Polarity: {result[\u0026#39;polarity\u0026#39;]:.2f}\u0026#34;) print(f\u0026#34;Subjectivity: {result[\u0026#39;subjectivity\u0026#39;]:.2f}\u0026#34;) print(f\u0026#34;Sentiment: {result[\u0026#39;category\u0026#39;]}\u0026#34;) Lab 5: Sentiment Analysis with VADER from nltk.sentiment import SentimentIntensityAnalyzer import nltk nltk.download(\u0026#39;vader_lexicon\u0026#39;) sia = SentimentIntensityAnalyzer() texts = [ \u0026#34;I absolutely love this! Best purchase ever!!!\u0026#34;, \u0026#34;This product is terrible. Complete waste of money.\u0026#34;, \u0026#34;It\u0026#39;s okay, nothing special.\u0026#34;, \u0026#34;Not bad, but could be better.\u0026#34; ] for text in texts: scores = sia.polarity_scores(text) print(f\u0026#34;\\nText: {text}\u0026#34;) print(f\u0026#34;Scores: {scores}\u0026#34;) # Determine overall sentiment if scores[\u0026#39;compound\u0026#39;] \u0026gt;= 0.05: sentiment = \u0026#34;Positive\u0026#34; elif scores[\u0026#39;compound\u0026#39;] \u0026lt;= -0.05: sentiment = \u0026#34;Negative\u0026#34; else: sentiment = \u0026#34;Neutral\u0026#34; print(f\u0026#34;Overall: {sentiment}\u0026#34;) Practice Exercises Analyze word frequency in your favorite book or article Extract and analyze bigrams/trigrams from social media posts Build a sentiment classifier for product reviews Compare TextBlob and VADER sentiment results Create visualizations for word frequency and sentiment distribution "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.9-week9/1.9.3-day43-2025-11-05/","title":"Day 43 - Production Sentiment Analysis Systems","tags":[],"description":"","content":"Date: 2025-11-05\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Sentiment Analysis Revision Key Concepts from Week 8 Sentiment analysis determines emotional tone: positive, negative, neutral Approaches: lexicon-based, machine learning, deep learning Metrics: polarity (-1 to +1), subjectivity (0 to 1) Challenges: sarcasm, context, negation, domain-specific language Beyond Basic Sentiment Aspect-Based Sentiment Analysis\nIdentify sentiments toward specific aspects/features Example: \u0026ldquo;Great camera but terrible battery life\u0026rdquo; Camera: positive Battery: negative More granular insights for product reviews Emotion Detection\nBeyond positive/negative: joy, anger, sadness, fear, surprise Multi-label classification problem Useful for customer support, mental health monitoring Production-Ready Systems Requirements for Production Scalability\nHandle high request volumes Real-time or near real-time processing Batch processing for large datasets Robustness\nHandle noisy, informal text (typos, slang, emojis) Graceful degradation on edge cases Clear confidence scores Monitoring \u0026amp; Maintenance\nTrack model performance over time Detect concept drift (language changes) A/B testing for model improvements Logging and error tracking API Design Best Practices Input Validation\nText length limits Character encoding handling Sanitization of malicious input Response Format\n{ \u0026#34;text\u0026#34;: \u0026#34;input text\u0026#34;, \u0026#34;sentiment\u0026#34;: \u0026#34;positive\u0026#34;, \u0026#34;confidence\u0026#34;: 0.92, \u0026#34;polarity\u0026#34;: 0.75, \u0026#34;subjectivity\u0026#34;: 0.65, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-11-05T10:00:00Z\u0026#34; } Error Handling\nMeaningful error messages HTTP status codes Rate limiting Request validation Advanced Techniques Transfer Learning Pre-trained models (BERT, RoBERTa) for better performance Fine-tuning on domain-specific data Reduces training data requirements State-of-the-art results Ensemble Approaches Combine lexicon-based and ML methods Voting or weighted averaging Improves robustness and accuracy Better handling of edge cases Key Insights Production systems require more than just good accuracy Domain adaptation crucial for real-world performance Continuous monitoring essential for maintaining quality API design affects both usability and system reliability Hands-On Labs Lab 1: Aspect-Based Sentiment Analysis import spacy from textblob import TextBlob import re nlp = spacy.load(\u0026#34;en_core_web_sm\u0026#34;) class AspectSentimentAnalyzer: \u0026#34;\u0026#34;\u0026#34;Analyze sentiment for specific aspects\u0026#34;\u0026#34;\u0026#34; def __init__(self): self.aspect_keywords = { \u0026#39;camera\u0026#39;: [\u0026#39;camera\u0026#39;, \u0026#39;photo\u0026#39;, \u0026#39;picture\u0026#39;, \u0026#39;lens\u0026#39;, \u0026#39;image\u0026#39;], \u0026#39;battery\u0026#39;: [\u0026#39;battery\u0026#39;, \u0026#39;charge\u0026#39;, \u0026#39;power\u0026#39;], \u0026#39;screen\u0026#39;: [\u0026#39;screen\u0026#39;, \u0026#39;display\u0026#39;, \u0026#39;brightness\u0026#39;], \u0026#39;performance\u0026#39;: [\u0026#39;speed\u0026#39;, \u0026#39;fast\u0026#39;, \u0026#39;slow\u0026#39;, \u0026#39;lag\u0026#39;, \u0026#39;performance\u0026#39;], \u0026#39;price\u0026#39;: [\u0026#39;price\u0026#39;, \u0026#39;cost\u0026#39;, \u0026#39;expensive\u0026#39;, \u0026#39;cheap\u0026#39;, \u0026#39;value\u0026#39;] } def extract_aspect_sentences(self, text): \u0026#34;\u0026#34;\u0026#34;Extract sentences mentioning each aspect\u0026#34;\u0026#34;\u0026#34; doc = nlp(text) aspect_sentences = {aspect: [] for aspect in self.aspect_keywords} for sent in doc.sents: sent_text = sent.text.lower() for aspect, keywords in self.aspect_keywords.items(): if any(keyword in sent_text for keyword in keywords): aspect_sentences[aspect].append(sent.text) return aspect_sentences def analyze_aspect_sentiment(self, text): \u0026#34;\u0026#34;\u0026#34;Analyze sentiment for each aspect\u0026#34;\u0026#34;\u0026#34; aspect_sentences = self.extract_aspect_sentences(text) results = {} for aspect, sentences in aspect_sentences.items(): if not sentences: continue sentiments = [] for sentence in sentences: blob = TextBlob(sentence) sentiments.append(blob.sentiment.polarity) avg_sentiment = sum(sentiments) / len(sentiments) results[aspect] = { \u0026#39;sentiment\u0026#39;: \u0026#39;positive\u0026#39; if avg_sentiment \u0026gt; 0.1 else \u0026#39;negative\u0026#39; if avg_sentiment \u0026lt; -0.1 else \u0026#39;neutral\u0026#39;, \u0026#39;polarity\u0026#39;: avg_sentiment, \u0026#39;mentions\u0026#39;: len(sentences), \u0026#39;examples\u0026#39;: sentences[:2] # First 2 examples } return results # Test aspect-based analysis analyzer = AspectSentimentAnalyzer() review = \u0026#34;\u0026#34;\u0026#34; This phone has an amazing camera! The photos are crystal clear and vibrant. However, the battery life is disappointing - it barely lasts a day. The screen is gorgeous with excellent brightness. Performance is good but not great, sometimes it lags. For the price, it\u0026#39;s a decent value. \u0026#34;\u0026#34;\u0026#34; aspects = analyzer.analyze_aspect_sentiment(review) print(\u0026#34;Aspect-Based Sentiment Analysis:\\n\u0026#34;) for aspect, info in aspects.items(): print(f\u0026#34;{aspect.upper()}:\u0026#34;) print(f\u0026#34; Sentiment: {info[\u0026#39;sentiment\u0026#39;]}\u0026#34;) print(f\u0026#34; Polarity: {info[\u0026#39;polarity\u0026#39;]:.2f}\u0026#34;) print(f\u0026#34; Mentions: {info[\u0026#39;mentions\u0026#39;]}\u0026#34;) print(f\u0026#34; Examples: {info[\u0026#39;examples\u0026#39;]}\u0026#34;) print() Lab 2: Multi-Class Emotion Detection from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.multioutput import MultiOutputClassifier from sklearn.linear_model import LogisticRegression from sklearn.metrics import classification_report import numpy as np # Sample dataset with multiple emotions texts = [ \u0026#34;I\u0026#39;m so happy and excited about this!\u0026#34;, \u0026#34;This makes me very angry and frustrated\u0026#34;, \u0026#34;I\u0026#39;m scared and worried about the future\u0026#34;, \u0026#34;What a pleasant surprise! I\u0026#39;m delighted\u0026#34;, \u0026#34;This is sad and disappointing news\u0026#34;, ] * 10 # Multi-label emotions (can have multiple emotions) emotions = [ {\u0026#39;joy\u0026#39;: 1, \u0026#39;anger\u0026#39;: 0, \u0026#39;fear\u0026#39;: 0, \u0026#39;surprise\u0026#39;: 0, \u0026#39;sadness\u0026#39;: 0}, {\u0026#39;joy\u0026#39;: 0, \u0026#39;anger\u0026#39;: 1, \u0026#39;fear\u0026#39;: 0, \u0026#39;surprise\u0026#39;: 0, \u0026#39;sadness\u0026#39;: 0}, {\u0026#39;joy\u0026#39;: 0, \u0026#39;anger\u0026#39;: 0, \u0026#39;fear\u0026#39;: 1, \u0026#39;surprise\u0026#39;: 0, \u0026#39;sadness\u0026#39;: 0}, {\u0026#39;joy\u0026#39;: 1, \u0026#39;anger\u0026#39;: 0, \u0026#39;fear\u0026#39;: 0, \u0026#39;surprise\u0026#39;: 1, \u0026#39;sadness\u0026#39;: 0}, {\u0026#39;joy\u0026#39;: 0, \u0026#39;anger\u0026#39;: 0, \u0026#39;fear\u0026#39;: 0, \u0026#39;surprise\u0026#39;: 0, \u0026#39;sadness\u0026#39;: 1}, ] * 10 # Convert to array format emotion_labels = [\u0026#39;joy\u0026#39;, \u0026#39;anger\u0026#39;, \u0026#39;fear\u0026#39;, \u0026#39;surprise\u0026#39;, \u0026#39;sadness\u0026#39;] y = np.array([[e[label] for label in emotion_labels] for e in emotions]) # Vectorize vectorizer = TfidfVectorizer(max_features=100) X = vectorizer.fit_transform(texts) # Multi-output classifier classifier = MultiOutputClassifier(LogisticRegression(max_iter=1000)) classifier.fit(X, y) # Test test_texts = [ \u0026#34;I\u0026#39;m thrilled and amazed!\u0026#34;, \u0026#34;This is terrifying and makes me anxious\u0026#34;, \u0026#34;So disappointed and upset\u0026#34; ] test_X = vectorizer.transform(test_texts) predictions = classifier.predict(test_X) print(\u0026#34;Emotion Detection Results:\\n\u0026#34;) for text, pred in zip(test_texts, predictions): detected_emotions = [emotion_labels[i] for i, val in enumerate(pred) if val == 1] print(f\u0026#34;Text: {text}\u0026#34;) print(f\u0026#34;Emotions: {\u0026#39;, \u0026#39;.join(detected_emotions) if detected_emotions else \u0026#39;neutral\u0026#39;}\\n\u0026#34;) Lab 3: Production-Ready API with FastAPI from fastapi import FastAPI, HTTPException from pydantic import BaseModel, validator from textblob import TextBlob from datetime import datetime import uvicorn app = FastAPI(title=\u0026#34;Sentiment Analysis API\u0026#34;) class SentimentRequest(BaseModel): text: str @validator(\u0026#39;text\u0026#39;) def validate_text(cls, v): if not v or len(v.strip()) == 0: raise ValueError(\u0026#39;Text cannot be empty\u0026#39;) if len(v) \u0026gt; 5000: raise ValueError(\u0026#39;Text too long (max 5000 characters)\u0026#39;) return v class SentimentResponse(BaseModel): text: str sentiment: str confidence: float polarity: float subjectivity: float timestamp: str @app.post(\u0026#34;/analyze\u0026#34;, response_model=SentimentResponse) async def analyze_sentiment(request: SentimentRequest): \u0026#34;\u0026#34;\u0026#34;Analyze sentiment of input text\u0026#34;\u0026#34;\u0026#34; try: blob = TextBlob(request.text) polarity = blob.sentiment.polarity subjectivity = blob.sentiment.subjectivity # Determine sentiment category if polarity \u0026gt; 0.1: sentiment = \u0026#34;positive\u0026#34; confidence = min(polarity, 1.0) elif polarity \u0026lt; -0.1: sentiment = \u0026#34;negative\u0026#34; confidence = min(abs(polarity), 1.0) else: sentiment = \u0026#34;neutral\u0026#34; confidence = 1.0 - abs(polarity) return SentimentResponse( text=request.text, sentiment=sentiment, confidence=round(confidence, 2), polarity=round(polarity, 2), subjectivity=round(subjectivity, 2), timestamp=datetime.utcnow().isoformat() + \u0026#34;Z\u0026#34; ) except Exception as e: raise HTTPException(status_code=500, detail=f\u0026#34;Analysis failed: {str(e)}\u0026#34;) @app.get(\u0026#34;/health\u0026#34;) async def health_check(): \u0026#34;\u0026#34;\u0026#34;Health check endpoint\u0026#34;\u0026#34;\u0026#34; return {\u0026#34;status\u0026#34;: \u0026#34;healthy\u0026#34;, \u0026#34;timestamp\u0026#34;: datetime.utcnow().isoformat()} # To run: uvicorn script_name:app --reload # Test with: curl -X POST \u0026#34;http://localhost:8000/analyze\u0026#34; -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;text\u0026#34;:\u0026#34;I love this product!\u0026#34;}\u0026#39; Lab 4: Ensemble Sentiment Analyzer from textblob import TextBlob from nltk.sentiment import SentimentIntensityAnalyzer import nltk nltk.download(\u0026#39;vader_lexicon\u0026#39;, quiet=True) class EnsembleSentimentAnalyzer: \u0026#34;\u0026#34;\u0026#34;Combine multiple sentiment analysis methods\u0026#34;\u0026#34;\u0026#34; def __init__(self): self.sia = SentimentIntensityAnalyzer() def textblob_sentiment(self, text): \u0026#34;\u0026#34;\u0026#34;TextBlob analysis\u0026#34;\u0026#34;\u0026#34; blob = TextBlob(text) polarity = blob.sentiment.polarity if polarity \u0026gt; 0.1: return \u0026#39;positive\u0026#39;, polarity elif polarity \u0026lt; -0.1: return \u0026#39;negative\u0026#39;, polarity else: return \u0026#39;neutral\u0026#39;, polarity def vader_sentiment(self, text): \u0026#34;\u0026#34;\u0026#34;VADER analysis\u0026#34;\u0026#34;\u0026#34; scores = self.sia.polarity_scores(text) compound = scores[\u0026#39;compound\u0026#39;] if compound \u0026gt;= 0.05: return \u0026#39;positive\u0026#39;, compound elif compound \u0026lt;= -0.05: return \u0026#39;negative\u0026#39;, compound else: return \u0026#39;neutral\u0026#39;, compound def ensemble_predict(self, text, method=\u0026#39;voting\u0026#39;): \u0026#34;\u0026#34;\u0026#34;Combine predictions from multiple methods\u0026#34;\u0026#34;\u0026#34; # Get predictions from both methods tb_sentiment, tb_score = self.textblob_sentiment(text) vader_sentiment, vader_score = self.vader_sentiment(text) if method == \u0026#39;voting\u0026#39;: # Majority voting sentiments = [tb_sentiment, vader_sentiment] final_sentiment = max(set(sentiments), key=sentiments.count) elif method == \u0026#39;weighted\u0026#39;: # Weighted average (give more weight to higher confidence) tb_weight = abs(tb_score) vader_weight = abs(vader_score) total_weight = tb_weight + vader_weight if total_weight == 0: return \u0026#39;neutral\u0026#39;, 0.0, {\u0026#39;textblob\u0026#39;: tb_sentiment, \u0026#39;vader\u0026#39;: vader_sentiment} # Weight the sentiments sentiment_scores = {\u0026#39;positive\u0026#39;: 0, \u0026#39;negative\u0026#39;: 0, \u0026#39;neutral\u0026#39;: 0} sentiment_scores[tb_sentiment] += tb_weight sentiment_scores[vader_sentiment] += vader_weight final_sentiment = max(sentiment_scores, key=sentiment_scores.get) confidence = sentiment_scores[final_sentiment] / total_weight return final_sentiment, confidence, { \u0026#39;textblob\u0026#39;: (tb_sentiment, tb_score), \u0026#39;vader\u0026#39;: (vader_sentiment, vader_score) } return final_sentiment, 0.0, {\u0026#39;textblob\u0026#39;: tb_sentiment, \u0026#39;vader\u0026#39;: vader_sentiment} # Test ensemble ensemble = EnsembleSentimentAnalyzer() test_sentences = [ \u0026#34;This product is absolutely amazing!\u0026#34;, \u0026#34;Worst purchase ever, total waste of money\u0026#34;, \u0026#34;It\u0026#39;s okay, nothing special\u0026#34;, \u0026#34;Not bad, but could be better\u0026#34;, \u0026#34;I love it so much!!!\u0026#34; ] print(\u0026#34;Ensemble Sentiment Analysis:\\n\u0026#34;) for sentence in test_sentences: sentiment, confidence, details = ensemble.ensemble_predict(sentence, method=\u0026#39;weighted\u0026#39;) print(f\u0026#34;Text: {sentence}\u0026#34;) print(f\u0026#34;Final Sentiment: {sentiment} (confidence: {confidence:.2f})\u0026#34;) print(f\u0026#34;TextBlob: {details[\u0026#39;textblob\u0026#39;]}\u0026#34;) print(f\u0026#34;VADER: {details[\u0026#39;vader\u0026#39;]}\u0026#34;) print() Lab 5: Sentiment Analysis with Performance Monitoring import time from collections import defaultdict from datetime import datetime class MonitoredSentimentAnalyzer: \u0026#34;\u0026#34;\u0026#34;Sentiment analyzer with built-in monitoring\u0026#34;\u0026#34;\u0026#34; def __init__(self): self.stats = defaultdict(int) self.response_times = [] self.errors = [] def analyze(self, text): \u0026#34;\u0026#34;\u0026#34;Analyze with monitoring\u0026#34;\u0026#34;\u0026#34; start_time = time.time() try: # Perform analysis blob = TextBlob(text) polarity = blob.sentiment.polarity if polarity \u0026gt; 0.1: sentiment = \u0026#34;positive\u0026#34; elif polarity \u0026lt; -0.1: sentiment = \u0026#34;negative\u0026#34; else: sentiment = \u0026#34;neutral\u0026#34; # Record stats self.stats[\u0026#39;total_requests\u0026#39;] += 1 self.stats[f\u0026#39;{sentiment}_count\u0026#39;] += 1 response_time = time.time() - start_time self.response_times.append(response_time) return { \u0026#39;sentiment\u0026#39;: sentiment, \u0026#39;polarity\u0026#39;: polarity, \u0026#39;response_time\u0026#39;: response_time } except Exception as e: self.stats[\u0026#39;errors\u0026#39;] += 1 self.errors.append({ \u0026#39;timestamp\u0026#39;: datetime.now().isoformat(), \u0026#39;error\u0026#39;: str(e), \u0026#39;text_length\u0026#39;: len(text) }) raise def get_metrics(self): \u0026#34;\u0026#34;\u0026#34;Get performance metrics\u0026#34;\u0026#34;\u0026#34; if not self.response_times: return {} return { \u0026#39;total_requests\u0026#39;: self.stats[\u0026#39;total_requests\u0026#39;], \u0026#39;positive_ratio\u0026#39;: self.stats[\u0026#39;positive_count\u0026#39;] / self.stats[\u0026#39;total_requests\u0026#39;], \u0026#39;negative_ratio\u0026#39;: self.stats[\u0026#39;negative_count\u0026#39;] / self.stats[\u0026#39;total_requests\u0026#39;], \u0026#39;neutral_ratio\u0026#39;: self.stats[\u0026#39;neutral_count\u0026#39;] / self.stats[\u0026#39;total_requests\u0026#39;], \u0026#39;avg_response_time\u0026#39;: sum(self.response_times) / len(self.response_times), \u0026#39;min_response_time\u0026#39;: min(self.response_times), \u0026#39;max_response_time\u0026#39;: max(self.response_times), \u0026#39;error_count\u0026#39;: self.stats[\u0026#39;errors\u0026#39;], \u0026#39;error_rate\u0026#39;: self.stats[\u0026#39;errors\u0026#39;] / self.stats[\u0026#39;total_requests\u0026#39;] if self.stats[\u0026#39;total_requests\u0026#39;] \u0026gt; 0 else 0 } # Test monitoring monitored = MonitoredSentimentAnalyzer() # Simulate requests test_texts = [ \u0026#34;Great product!\u0026#34;, \u0026#34;Terrible service\u0026#34;, \u0026#34;It\u0026#39;s okay\u0026#34;, \u0026#34;Love it!\u0026#34;, \u0026#34;Disappointed\u0026#34; ] * 20 for text in test_texts: result = monitored.analyze(text) # Get metrics metrics = monitored.get_metrics() print(\u0026#34;Performance Metrics:\\n\u0026#34;) for key, value in metrics.items(): if \u0026#39;time\u0026#39; in key: print(f\u0026#34;{key}: {value*1000:.2f}ms\u0026#34;) elif \u0026#39;ratio\u0026#39; in key or \u0026#39;rate\u0026#39; in key: print(f\u0026#34;{key}: {value*100:.2f}%\u0026#34;) else: print(f\u0026#34;{key}: {value}\u0026#34;) Practice Exercises Build an aspect-based sentiment analyzer for restaurant reviews Create a real-time sentiment dashboard using Streamlit Implement sentiment trend analysis over time Build a sentiment API with rate limiting and caching Compare different sentiment analysis libraries and approaches "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.10-week10/1.10.3-day48-2025-11-12/","title":"Day 48 - Admin Panel &amp; Approval Workflow","tags":[],"description":"","content":"Date: 2025-11-12\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Admin Panel Architecture Role-Based Access Control (RBAC) Cognito Groups Implementation:\nAdmin users assigned to Admins group in Cognito User Pool JWT token contains cognito:groups: [\u0026quot;Admins\u0026quot;] claim Frontend checks user groups before rendering admin routes API Gateway validates JWT, Lambda validates group membership Authorization Flow:\nAdmin Login → Cognito returns JWT with groups → Frontend checks groups → Show/Hide admin navigation → Admin action → API validates JWT + groups → Lambda executes Admin Dashboard Features Core Functionality:\nPending Books Review: List all books with status PENDING Approval/Rejection Actions: Approve or reject with reason Book Management: View all books, search, filter by status User Management: View uploaders, activity logs Analytics: Upload statistics, approval rates, storage usage Status Workflow:\nPENDING → Initial upload status VALIDATING → Server-side MIME validation in progress APPROVED → Admin approved, book publicly accessible REJECTED → Admin rejected with reason TAKEDOWN → Content removed due to copyright/violation Approval Workflow Implementation Backend Flow (Lambda) approveBook Lambda:\n1. Validate JWT and check Admins group 2. Verify book exists and status is PENDING 3. Copy file from uploads/ to public/books/ 4. Update DynamoDB: status=APPROVED, approverID, approvalTimestamp 5. Delete original file from uploads/ 6. Return success response rejectBook Lambda:\n1. Validate JWT and check Admins group 2. Update DynamoDB: status=REJECTED, reason, rejectionTimestamp 3. Optional: Move file to quarantine/ instead of deletion 4. Send notification to uploader (future enhancement) Frontend Admin Interface Pending Books List:\nDisplay: thumbnail, title, author, uploader, upload date Actions: Preview (download temp), Approve, Reject Filters: by date, by uploader, by file type Pagination for large datasets Approval Modal:\nConfirm action with book details Optional: add approval notes Loading state during processing Success/error notifications Rejection Modal:\nRequired rejection reason selection/textarea Predefined reasons: copyright, inappropriate, quality, other Custom message field Confirmation before submission Key Insights Admin panel is high-privilege area - strict access control essential Audit logging critical for accountability (who approved/rejected what and when) Soft delete (quarantine) better than hard delete for legal compliance Real-time status updates improve admin workflow efficiency Clear rejection reasons help uploaders improve future submissions Tasks Completed Admin Route Protection\nCreated admin-only route wrapper checking Cognito groups Implemented unauthorized access handling (redirect to dashboard) Added admin navigation menu (visible only to admins) Built \u0026ldquo;Access Denied\u0026rdquo; page for non-admin users Pending Books Interface\nCreated pending books list component with DataTable Implemented book preview modal (display metadata and cover) Added bulk selection for batch operations Built status filter (PENDING, VALIDATING, ALL) Approval System\nCreated approval confirmation modal Implemented approveBook API integration Added optimistic UI updates (instant status change) Built success/error toast notifications Rejection System\nCreated rejection modal with reason selection Built rejection reason dropdown (predefined + custom) Implemented rejectBook API integration Added rejection history view for uploaders Admin Dashboard\nCreated dashboard overview with statistics cards Built charts: uploads over time, approval rates, status distribution Implemented recent activity feed Added quick action buttons (pending count badge) Audit Logging Display\nCreated audit log viewer showing all actions Implemented filtering by action type, date range, admin Added export audit logs to CSV functionality Built detailed action view with metadata Challenges \u0026amp; Solutions Challenge: Real-time status updates without polling\nSolution: Implemented WebSocket connection for admin panel to receive instant status changes from backend\nChallenge: Large list of pending books causing performance issues\nSolution: Implemented virtual scrolling and server-side pagination with cursor-based navigation\nChallenge: Accidental approvals/rejections\nSolution: Added confirmation modals with 2-second countdown before action enabled\nChallenge: Admin actions not reflected immediately\nSolution: Implemented optimistic UI updates with rollback on error, plus cache invalidation\n"},{"uri":"https://anquoc211.github.io/AWS_Internship/5-workshop/5.3-module3/","title":"User Registration &amp; Verification","tags":[],"description":"","content":"Initialize React App Create a new React application and add Amplify:\nnpx create-react-app auth-workshop cd auth-workshop npm install aws-amplify @aws-amplify/ui-react Initialize Amplify Project amplify init # Follow prompts: # - Project name: authworkshop # - Environment: dev # - Default editor: Visual Studio Code # - App type: javascript # - Framework: react # - Source directory: src # - Distribution directory: build # - Build command: npm run build # - Start command: npm start Add Authentication amplify add auth # Choose: Default configuration # Sign-in method: Email # Advanced settings: No Deploy Backend amplify push Build Registration UI Create src/components/Register.js:\nimport { Auth } from \u0026#39;aws-amplify\u0026#39;; import { useState } from \u0026#39;react\u0026#39;; function Register() { const [email, setEmail] = useState(\u0026#39;\u0026#39;); const [password, setPassword] = useState(\u0026#39;\u0026#39;); const [code, setCode] = useState(\u0026#39;\u0026#39;); const [step, setStep] = useState(\u0026#39;register\u0026#39;); const handleRegister = async (e) =\u0026gt; { e.preventDefault(); try { await Auth.signUp({ username: email, password: password, attributes: { email } }); setStep(\u0026#39;verify\u0026#39;); } catch (error) { console.error(\u0026#39;Error:\u0026#39;, error); } }; const handleVerify = async (e) =\u0026gt; { e.preventDefault(); try { await Auth.confirmSignUp(email, code); alert(\u0026#39;Email verified successfully!\u0026#39;); } catch (error) { console.error(\u0026#39;Error:\u0026#39;, error); } }; return step === \u0026#39;register\u0026#39; ? ( \u0026lt;form onSubmit={handleRegister}\u0026gt; \u0026lt;input type=\u0026#34;email\u0026#34; value={email} onChange={(e) =\u0026gt; setEmail(e.target.value)} placeholder=\u0026#34;Email\u0026#34; required /\u0026gt; \u0026lt;input type=\u0026#34;password\u0026#34; value={password} onChange={(e) =\u0026gt; setPassword(e.target.value)} placeholder=\u0026#34;Password\u0026#34; required /\u0026gt; \u0026lt;button type=\u0026#34;submit\u0026#34;\u0026gt;Register\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; ) : ( \u0026lt;form onSubmit={handleVerify}\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; value={code} onChange={(e) =\u0026gt; setCode(e.target.value)} placeholder=\u0026#34;Verification Code\u0026#34; required /\u0026gt; \u0026lt;button type=\u0026#34;submit\u0026#34;\u0026gt;Verify\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; ); } export default Register; Test Registration Start the app: npm start Navigate to the registration page Enter email and password Check email for verification code Enter code to verify account "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.3-week3/","title":"Week 3 - AWS Compute Services","tags":[],"description":"","content":"Week: 2025-09-22 đến 2025-09-26\nStatus: \u0026ldquo;Done\u0026rdquo;\nTổng quan tuần 3 Tuần này tập trung vào các dịch vụ Compute của AWS, đặc biệt là Amazon EC2 và các dịch vụ liên quan.\nNội dung chính Amazon EC2 và Instance Types AMI và Backup Strategies EBS và Instance Store EC2 Auto Scaling EC2 Pricing Options Amazon Lightsail, EFS, FSx Labs thực hành Lab 01: AWS Account \u0026amp; IAM Setup Lab 07: AWS Budgets \u0026amp; Cost Management Lab 09: AWS Support Plans "},{"uri":"https://anquoc211.github.io/AWS_Internship/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"Throughout my internship, I attended two major AWS events that significantly expanded my cloud and AI knowledge while providing valuable networking opportunities.\nEvent 1: Vietnam Cloud Day 2025 When: September 18, 2025 at 9:00 AM\nWhere: AWS Vietnam Office (36F, 2 Hai Trieu St, District 1, HCMC)\nRole: Attendee\nOverview: Full-day AWS conference featuring government leaders, AWS executives, and industry innovators. Covered two main streams: live keynotes on GenAI transformation and executive leadership, plus hands-on breakout sessions on AI/data foundations, GenAI roadmaps, security best practices, and cloud modernization strategies.\nImpact: Gained enterprise-level insights into AI adoption strategies, learned AWS services for data infrastructure and GenAI deployment, and understood security frameworks for AI applications and legacy system modernization.\nEvent 2: AWS GenAI Builder Club When: October 3, 2025 at 2:00 PM\nWhere: AWS Event Hall (L26 Bitexco Tower, HCMC)\nRole: Attendee\nOverview: Technical workshop on AI-Driven Development Lifecycle (AI-DLC), featuring live demos of Amazon Q Developer and Kiro IDE. Explored how generative AI transforms software engineering from architecture through deployment, emphasizing AI as a collaborative partner rather than just a code assistant.\nImpact: Learned practical AI integration in development workflows, gained hands-on experience with Amazon Q Developer and Kiro, and understood how to leverage AI for productivity while maintaining code quality and developer oversight.\n"},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.1-week1/1.1.4-day04-2025-09-11/","title":"Day 04 - Cost Optimization on AWS","tags":[],"description":"","content":"Date: 2025-09-11\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Cost Optimization on AWS Cost Optimization Strategies Choose the right resource types and Regions. Use pricing models such as Reserved Instances, Savings Plans, and Spot Instances. Remove or schedule idle resources. Leverage serverless architectures. Continuously review and improve cost efficiency with AWS Budgets and Cost Explorer. Tag resources with Cost Allocation Tags for department-level tracking. AWS Pricing Calculator calculator.aws\nCreate and share cost estimates for common services. Pricing varies by Region. Key Features:\nEstimate costs before deployment Compare pricing across regions Export and share estimates Template-based estimation AWS Support Plans Four tiers: Basic, Developer, Business, and Enterprise. Plans can be upgraded temporarily during critical incidents. Support Plan Comparison Feature Basic Developer Business Enterprise Cost Free $29/month $100/month $15,000/month Response Time N/A 12-24 hours 1 hour (urgent) 15 min (critical) Technical Support Forums only Business hours 24/7 24/7 + TAM Hands-On Labs Lab 07 – AWS Budgets \u0026amp; Cost Management Create Budget by Template → 07-01 Create Cost Budget Tutorial → 07-02 Create Usage Budget → 07-03 Create Reserved Instance (RI) Budget → 07-04 Create Savings Plans Budget → 07-05 Clean Up Budgets → 07-06 Lab 09 – AWS Support Plans AWS Support Packages → 09-01 Types of Support Requests → 09-02 Change Support Package → 09-03 Manage Support Requests → 09-04 "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.2-week2/1.2.4-day09-2025-09-18/","title":"Day 09 - VPC Connectivity &amp; Load Balancing","tags":[],"description":"","content":"Date: 2025-09-18\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes VPC Peering \u0026amp; Transit Gateway VPC Peering Enables direct, private connectivity between two VPCs without traversing the Internet. Does not support transitive routing or overlapping CIDRs. VPC Peering Limitations:\nNo transitive peering No overlapping CIDR blocks Limited to 125 peering connections per VPC Cross-region peering supported AWS Transit Gateway (TGW) Acts as a hub to connect multiple VPCs and on-prem networks, simplifying complex mesh topologies. TGW Attachments associate subnets in specific AZs with a TGW. All subnets within the same AZ can reach the TGW once attached. Transit Gateway Benefits:\nCentralized connectivity hub Simplified network architecture Scalable to thousands of VPCs Supports inter-region peering VPN \u0026amp; Direct Connect Site-to-Site VPN Establishes a secure IPSec connection between an on-premises data center and AWS VPC. Consists of: Virtual Private Gateway (VGW): AWS-managed, multi-AZ endpoints. Customer Gateway (CGW): Customer-managed device or software appliance. AWS Direct Connect Provides a dedicated private network connection between an on-prem data center and AWS. Typical latency: 20–30 ms. In Vietnam, available through Hosted Connections (via partners). Bandwidth is adjustable. Hands-On Labs Lab 10 – Hybrid DNS (Route 53 Resolver) Generate Key Pair → 10-02.1 Initialize CloudFormation Template → 10-02.2 Configure Security Group → 10-02.3 Set up DNS System → 10-05 Create Route 53 Outbound Endpoint → 10-05.1 Create Resolver Rules → 10-05.2 Create Inbound Endpoints → 10-05.3 Lab 19 – VPC Peering Initialize CloudFormation Templates → 19-02.1 Create Security Group → 19-02.2 Create EC2 Instance (Test Peering) → 19-02.3 Create Peering Connection → 19-04 Configure Route Tables (Cross-VPC) → 19-05 Enable Cross-Peer DNS → 19-06 "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.3-week3/1.3.4-day14-2025-09-25/","title":"Day 14 - EC2 Auto Scaling","tags":[],"description":"","content":"Date: 2025-09-25\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Amazon EC2 Auto Scaling EC2 Auto Scaling automatically adjusts the number of EC2 instances based on demand. Benefits\nElastic capacity adjustment Increased application availability Cost optimization Components\nAuto Scaling Group (ASG) – logical group of EC2 instances Launch Template / Configuration – defines instance parameters Scaling Policies – rules for adding/removing instances Scaling Policies Simple / Step Scaling – add/remove instances when thresholds are met Target Tracking – maintain a metric (e.g., CPU = 50%) Scheduled Scaling – scale on a predefined schedule Predictive Scaling – uses ML to forecast and scale proactively Scaling Policy Examples:\n{ \u0026#34;TargetTrackingScalingPolicyConfiguration\u0026#34;: { \u0026#34;PredefinedMetricSpecification\u0026#34;: { \u0026#34;PredefinedMetricType\u0026#34;: \u0026#34;ASGAverageCPUUtilization\u0026#34; }, \u0026#34;TargetValue\u0026#34;: 50.0 } } Integration with Load Balancer ASGs often pair with Elastic Load Balancers (ELB). New instances automatically register; terminated instances deregister automatically. Auto Scaling Best Practices:\nUse multiple AZs for high availability Set appropriate cooldown periods Monitor CloudWatch metrics Use lifecycle hooks for custom actions Test scaling policies before production EC2 Pricing Options On-Demand: Pay per hour/second. Most expensive but flexible. Reserved Instances: 1- or 3-year commitment for discount; tied to specific instance type/family. Savings Plans: 1- or 3-year commitment; flexible across instance families. Spot Instances: Use spare capacity at up to 90% discount; can be terminated with 2-minute notice. Combine multiple pricing models within an Auto Scaling Group for cost optimization.\nPricing Comparison:\nModel Discount Flexibility Commitment On-Demand 0% High None Reserved 40-60% Low 1-3 years Savings Plans 40-60% Medium 1-3 years Spot 50-90% Low None Hands-On Labs Lab 09 – AWS Support Plans AWS Support Packages → 09-01 Types of Support Requests → 09-02 Change Support Package → 09-03 Manage Support Requests → 09-04 "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.4-week4/1.4.4-day19-2025-10-02/","title":"Day 19 - Disaster Recovery on AWS","tags":[],"description":"","content":"Date: 2025-10-02\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Disaster Recovery (DR) on AWS Disaster Recovery is about restoring IT services after major incidents (outages, disasters, hardware failures, cyberattacks).\nRTO (Recovery Time Objective): How quickly to restore service. RPO (Recovery Point Objective): How much data loss (time window) is acceptable. DR Strategies (ordered by complexity \u0026amp; cost) Backup \u0026amp; Restore\nMaintain backups only (EBS/RDS snapshots, S3/Glacier). Restore to new infrastructure during incidents. RTO: hours–days. RPO: depends on backup frequency. Cost: lowest. Pilot Light\nMinimal core services always running on AWS. Scale out to full production during DR. RTO: hours. RPO: minutes. Cost: moderate. Warm Standby\nFull system running at reduced scale on AWS. Scale up on failover. RTO: minutes–hours. RPO: seconds–minutes. Cost: higher. Multi-Site (Active/Active or Active/Passive)\nProduction running across on-prem and AWS, or multi-Region AWS. Traffic can be shifted instantly (Route 53, Global Accelerator). RTO/RPO: near zero. Cost: highest. DR Strategy Comparison:\nStrategy RTO RPO Cost Complexity Backup \u0026amp; Restore Hours-Days Hours $ Low Pilot Light Hours Minutes $$ Medium Warm Standby Minutes Seconds $$$ Medium-High Multi-Site Seconds Near-zero $$$$ High DR Best Practices Planning Define RTO and RPO requirements Document recovery procedures Identify critical systems and dependencies Establish communication plans Implementation Automate recovery processes Use multiple AZs and Regions Implement data replication Regular backup testing Testing Conduct DR drills regularly Test recovery procedures Measure actual RTO/RPO Update documentation Hands-On Labs Lab 14 – AWS VM Import/Export (Part 2) Import Virtual Machine to AWS → 14-02.3 Deploy Instance from AMI → 14-02.4 Set Up S3 Bucket ACL → 14-03.1 Export Virtual Machine from Instance → 14-03.2 Resource Cleanup on AWS → 14-05 "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.5-week5/1.5.4-day24-2025-10-09/","title":"Day 24 - SCPs, Identity Center &amp; KMS","tags":[],"description":"","content":"Date: 2025-10-09\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Service Control Policies (SCPs) Define maximum permissions for accounts; they limit but do not grant permissions. Apply to accounts or OUs; affect all users/roles, including root; Deny overrides Allow. Example SCP (deny bucket deletion)\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:DeleteBucket\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } SCP Use Cases:\nPrevent accounts from leaving the organization Restrict regions where resources can be created Enforce encryption requirements Prevent disabling security services Require specific tags on resources SCP Best Practices:\nStart with least privilege Test in non-production first Use explicit denies for critical controls Document SCP purposes Regular review and updates AWS Identity Center (formerly AWS SSO) Centralizes access to AWS accounts and external applications. Identity sources: built-in, AWS Managed Microsoft AD, on-prem AD (trust/AD Connector), or external IdPs. Permission Sets define what users/groups can do in target accounts (materialized as IAM roles). Multiple permission sets per user are supported. Identity Center Features:\nSingle sign-on to multiple AWS accounts Integration with Microsoft Active Directory SAML 2.0 support Multi-factor authentication Centralized permission management Audit logging with CloudTrail AWS Key Management Service (KMS) Managed keys for data protection with deep service integration and full auditability. Highlights\nCreate/manage keys without operating your own HSM infrastructure. Fine-grained access via IAM \u0026amp; key policies; usage logged in CloudTrail. Key categories\nCustomer-managed keys, AWS-managed keys, and AWS-owned keys. KMS Key Types:\nSymmetric: Single encryption key (AES-256) Asymmetric: Public/private key pair (RSA, ECC) KMS Features:\nAutomatic key rotation Key policies and grants Envelope encryption Integration with AWS services CloudTrail logging Multi-region keys Hands-On Labs Lab 33 – AWS KMS \u0026amp; CloudTrail Integration (Part 1) Create Policy and Role → 33-2.1 Create Group and User → 33-2.2 Create KMS Key → 33-3 Create S3 Bucket → 33-4.1 Upload Data to S3 → 33-4.2 Lab 30 – IAM Restriction Policy Create Restriction Policy → 30-3 Create IAM Limited User → 30-4 Test IAM User Limits → 30-5 Clean Up Resources → 30-6 "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.6-week6/1.6.4-day29-2025-10-16/","title":"Day 29 - Amazon ElastiCache","tags":[],"description":"","content":"Date: 2025-10-16\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Amazon ElastiCache Managed in-memory caching service for Redis and Memcached to reduce latency and offload databases.\nMicrosecond reads, Multi-AZ with failover, simple scaling, encryption/auth, automated ops. Redis: rich data structures, backups, replication, cluster mode. Memcached: simple, horizontally scalable cache with auto-discovery. Common uses: web/mobile acceleration, DB query caching, session stores, leaderboards, pub/sub, queues.\nElastiCache for Redis Features:\nData Structures: Strings, lists, sets, sorted sets, hashes, bitmaps, hyperloglogs Persistence: Snapshots and AOF (Append-Only File) Replication: Primary-replica with automatic failover Cluster Mode: Partition data across multiple shards Pub/Sub: Real-time messaging Lua Scripting: Server-side scripting Geospatial: Location-based queries ElastiCache for Memcached Features:\nMulti-threaded: Utilize multiple cores Auto Discovery: Automatic node discovery Horizontal Scaling: Add/remove nodes easily Simple: Easy to use, no persistence Redis vs Memcached:\nFeature Redis Memcached Data Structures Rich (lists, sets, etc.) Simple (key-value) Persistence Yes No Replication Yes No Multi-AZ Yes No Backup/Restore Yes No Pub/Sub Yes No Multi-threaded No Yes Caching Strategies Cache-Aside (Lazy Loading) Application checks cache first On miss, load from database and populate cache Pros: Only requested data is cached Cons: Cache miss penalty, stale data possible Write-Through Write to cache and database simultaneously Pros: Data always fresh, no cache misses on reads Cons: Write penalty, unused data may be cached Write-Behind (Write-Back) Write to cache immediately, async write to database Pros: Fast writes, reduced database load Cons: Risk of data loss, complexity Use Cases Session Store:\n# Store user session in Redis redis.setex(f\u0026#34;session:{user_id}\u0026#34;, 3600, session_data) # Retrieve session session = redis.get(f\u0026#34;session:{user_id}\u0026#34;) Leaderboard:\n# Add score to sorted set redis.zadd(\u0026#34;leaderboard\u0026#34;, {user_id: score}) # Get top 10 top_10 = redis.zrevrange(\u0026#34;leaderboard\u0026#34;, 0, 9, withscores=True) Rate Limiting:\n# Increment counter with expiry pipe = redis.pipeline() pipe.incr(f\u0026#34;rate:{user_id}\u0026#34;) pipe.expire(f\u0026#34;rate:{user_id}\u0026#34;, 60) count = pipe.execute()[0] if count \u0026gt; 100: raise RateLimitExceeded() Hands-On Labs Lab 43 – AWS Database Migration Service (DMS) (Part 3) Inspect S3 → 43-12 Create Serverless Migration → 43-13 Create Event Notification → 43-14 Logs → 43-15 Troubleshoot: Memory Pressure → 43-16 Troubleshoot: Table Error → 43-17 "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.7-week7/1.7.4-day34-2025-10-23/","title":"Day 34 - FastAPI Clean Architecture","tags":[],"description":"","content":"Date: 2025-10-23\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Clean Architecture Overview Tách rõ phần cấu hình, model, route và core logic để dễ mở rộng. Giữ main.py nhẹ: chỉ khởi tạo app, load config, mount router. Dùng Pydantic model để chuẩn hóa request/response, đảm bảo contract khớp OpenAPI. backend/ ├── main.py ├── core/ │ └── config.py ├── models/ │ └── book.py ├── routes/ │ └── books.py └── services/ └── books.py Cấu hình \u0026amp; Dependency core/config.py đọc biến môi trường, gom cấu hình CORS, API prefix, debug flag. Sử dụng dependency injection của FastAPI để truyền service layer vào router. Cho phép thay thế datasource (in-memory → PostgreSQL) mà không đổi giao diện hàm. CORS \u0026amp; API Stability CORS chỉ mở origin cần thiết (http://localhost:3000 khi dev). Bật allow_methods=[\u0026quot;GET\u0026quot;] cho slice đầu tiên để giảm bề mặt tấn công. Đảm bảo /openapi.json luôn trả về được để công cụ contract testing sử dụng. Start Simple, Refactor Later Bắt đầu bằng in-memory repository để demo nhanh, sau đó bổ sung DB thật. Document rõ TODO để tránh quên khi chuyển sang sprint tiếp theo. Logging tối giản, tập trung vào lỗi quan trọng (timeout, data mismatch). Hands-On Labs Refactor main.py chỉ giữ việc khởi tạo và router registration. Viết service layer get_book_detail(id) với dữ liệu giả lấy từ spec. Cấu hình CORSMiddleware khớp với URL của frontend mock và production. "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.8-week8/1.8.4-day39-2025-10-30/","title":"Day 39 - Named Entity Recognition (NER)","tags":[],"description":"","content":"Date: 2025-10-30 Status: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Named Entity Recognition (NER) What is NER? Identifying and classifying named entities in text. Entities: people, organizations, locations, dates, products, etc. Critical for information extraction and knowledge graphs. Foundation for question answering and search systems. Common Entity Types PERSON: Names of people ORG: Organizations, companies, institutions GPE: Geopolitical entities (countries, cities, states) DATE: Dates and time expressions MONEY: Monetary values LOCATION: Non-GPE locations (mountains, rivers) PRODUCT: Products and objects NER Approaches Rule-based: Pattern matching and dictionaries Machine Learning: Statistical models (CRF, HMM) Deep Learning: Neural networks (BiLSTM-CRF, Transformers) Pre-trained Models: spaCy, BERT-based models spaCy for NER Why spaCy? Fast and production-ready Pre-trained models for multiple languages Easy to use API Supports custom entity training Industrial-strength NLP library spaCy Pipeline Text → Tokenizer → Tagger → Parser → NER → Output Each component processes and adds annotations Can customize or disable components for performance Key Insights NER transforms unstructured text into structured data Accuracy varies by domain - finance NER differs from medical NER Pre-trained models work well for common entities Custom training needed for domain-specific entities Always validate NER results, especially for critical applications Hands-On Labs Lab 1: Basic NER with spaCy import spacy # Load pre-trained model nlp = spacy.load(\u0026#34;en_core_web_sm\u0026#34;) text = \u0026#34;\u0026#34;\u0026#34; Apple Inc. was founded by Steve Jobs in Cupertino, California. The company released the iPhone on January 9, 2007. Tim Cook became CEO in August 2011, succeeding Steve Jobs. \u0026#34;\u0026#34;\u0026#34; # Process text doc = nlp(text) # Extract entities print(\u0026#34;Entities found:\u0026#34;) for ent in doc.ents: print(f\u0026#34;{ent.text:20} - {ent.label_:15} - {spacy.explain(ent.label_)}\u0026#34;) Lab 2: Entity Visualization from spacy import displacy text = \u0026#34;\u0026#34;\u0026#34; Amazon, headquartered in Seattle, Washington, was founded by Jeff Bezos in 1994. The company started as an online bookstore and has grown into one of the world\u0026#39;s largest technology companies. In 2021, Andy Jassy became the CEO. \u0026#34;\u0026#34;\u0026#34; doc = nlp(text) # Visualize entities in Jupyter or save to HTML displacy.render(doc, style=\u0026#34;ent\u0026#34;, jupyter=False) # For saving to file: # html = displacy.render(doc, style=\u0026#34;ent\u0026#34;) # with open(\u0026#34;entities.html\u0026#34;, \u0026#34;w\u0026#34;) as f: # f.write(html) Lab 3: Entity Extraction and Analysis from collections import Counter def extract_entities_by_type(text, entity_type): doc = nlp(text) entities = [ent.text for ent in doc.ents if ent.label_ == entity_type] return entities def analyze_entities(text): doc = nlp(text) # Count entities by type entity_counts = Counter([ent.label_ for ent in doc.ents]) # Group entities by type entities_by_type = {} for ent in doc.ents: if ent.label_ not in entities_by_type: entities_by_type[ent.label_] = [] entities_by_type[ent.label_].append(ent.text) return entity_counts, entities_by_type news_text = \u0026#34;\u0026#34;\u0026#34; Microsoft announced on Monday that it would acquire GitHub for $7.5 billion. The deal was completed in October 2018. Satya Nadella, CEO of Microsoft, praised GitHub\u0026#39;s community of developers. GitHub, based in San Francisco, was founded in 2008 by Tom Preston-Werner, Chris Wanstrath, and PJ Hyett. \u0026#34;\u0026#34;\u0026#34; counts, entities = analyze_entities(news_text) print(\u0026#34;Entity Counts:\u0026#34;) for entity_type, count in counts.items(): print(f\u0026#34;{entity_type}: {count}\u0026#34;) print(\u0026#34;\\nEntities by Type:\u0026#34;) for entity_type, entity_list in entities.items(): print(f\u0026#34;\\n{entity_type}:\u0026#34;) for entity in set(entity_list): print(f\u0026#34; - {entity}\u0026#34;) Lab 4: Custom Entity Extraction def extract_all_people(text): \u0026#34;\u0026#34;\u0026#34;Extract all person names from text\u0026#34;\u0026#34;\u0026#34; doc = nlp(text) people = [ent.text for ent in doc.ents if ent.label_ == \u0026#34;PERSON\u0026#34;] return list(set(people)) # Remove duplicates def extract_all_organizations(text): \u0026#34;\u0026#34;\u0026#34;Extract all organization names from text\u0026#34;\u0026#34;\u0026#34; doc = nlp(text) orgs = [ent.text for ent in doc.ents if ent.label_ == \u0026#34;ORG\u0026#34;] return list(set(orgs)) def extract_all_locations(text): \u0026#34;\u0026#34;\u0026#34;Extract all locations from text\u0026#34;\u0026#34;\u0026#34; doc = nlp(text) locations = [ent.text for ent in doc.ents if ent.label_ in [\u0026#34;GPE\u0026#34;, \u0026#34;LOC\u0026#34;]] return list(set(locations)) article = \u0026#34;\u0026#34;\u0026#34; Elon Musk\u0026#39;s company Tesla is building a new factory in Austin, Texas. The factory will produce the Cybertruck and will employ thousands of workers. SpaceX, another company founded by Musk, is based in Hawthorne, California. Musk recently moved from California to Texas, citing better business conditions. \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;People:\u0026#34;, extract_all_people(article)) print(\u0026#34;Organizations:\u0026#34;, extract_all_organizations(article)) print(\u0026#34;Locations:\u0026#34;, extract_all_locations(article)) Lab 5: NER for Information Extraction def extract_structured_info(text): \u0026#34;\u0026#34;\u0026#34;Extract structured information from text\u0026#34;\u0026#34;\u0026#34; doc = nlp(text) info = { \u0026#39;people\u0026#39;: [], \u0026#39;organizations\u0026#39;: [], \u0026#39;locations\u0026#39;: [], \u0026#39;dates\u0026#39;: [], \u0026#39;money\u0026#39;: [] } for ent in doc.ents: if ent.label_ == \u0026#34;PERSON\u0026#34;: info[\u0026#39;people\u0026#39;].append(ent.text) elif ent.label_ == \u0026#34;ORG\u0026#34;: info[\u0026#39;organizations\u0026#39;].append(ent.text) elif ent.label_ in [\u0026#34;GPE\u0026#34;, \u0026#34;LOC\u0026#34;]: info[\u0026#39;locations\u0026#39;].append(ent.text) elif ent.label_ == \u0026#34;DATE\u0026#34;: info[\u0026#39;dates\u0026#39;].append(ent.text) elif ent.label_ == \u0026#34;MONEY\u0026#34;: info[\u0026#39;money\u0026#39;].append(ent.text) # Remove duplicates for key in info: info[key] = list(set(info[key])) return info business_news = \u0026#34;\u0026#34;\u0026#34; Google announced on March 15, 2023, that Sundar Pichai will continue as CEO. The company, based in Mountain View, California, reported revenue of $280 billion for the fiscal year. Google\u0026#39;s parent company, Alphabet Inc., also owns YouTube and has offices in New York, London, and Tokyo. \u0026#34;\u0026#34;\u0026#34; structured_data = extract_structured_info(business_news) print(\u0026#34;Extracted Information:\u0026#34;) for key, values in structured_data.items(): print(f\u0026#34;\\n{key.capitalize()}:\u0026#34;) for value in values: print(f\u0026#34; - {value}\u0026#34;) Practice Exercises Extract entities from news articles and categorize them Build a knowledge graph from extracted entities and their relationships Compare NER results from different spaCy models (small vs large) Create a function to extract company-executive relationships Analyze entity co-occurrence patterns in documents "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.9-week9/1.9.4-day44-2025-11-06/","title":"Day 44 - Advanced NER &amp; Custom Entity Training","tags":[],"description":"","content":"Date: 2025-11-06\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes NER Revision Key Concepts from Week 8 Named Entity Recognition identifies and classifies entities in text Common types: PERSON, ORG, GPE, DATE, MONEY, LOCATION Approaches: rule-based, machine learning, deep learning, pre-trained models spaCy provides production-ready NER capabilities Beyond Basic NER Custom Entity Types\nDomain-specific entities (product codes, medical terms, legal references) Training custom models for specialized domains Extending pre-trained models with new entity types Relation Extraction\nIdentifying relationships between entities Example: \u0026ldquo;Steve Jobs founded Apple\u0026rdquo; → (Steve Jobs, founded, Apple) Building knowledge graphs from text Custom NER Training Training Data Requirements Annotation Format\nTRAIN_DATA = [ (\u0026#34;Apple is looking at buying UK startup\u0026#34;, { \u0026#34;entities\u0026#34;: [(0, 5, \u0026#34;ORG\u0026#34;), (27, 29, \u0026#34;GPE\u0026#34;)] }) ] Quality Guidelines\nConsistent annotation across dataset Sufficient examples per entity type (100+ recommended) Balanced distribution of entity types Include negative examples (text without entities) Training Process Steps:\nPrepare annotated training data Initialize or load base model Add custom entity types if needed Train model with multiple iterations Evaluate on held-out test set Fine-tune hyperparameters Best Practices:\nUse transfer learning from pre-trained models Start with small learning rate Monitor for overfitting Save checkpoints during training Advanced Techniques Entity Linking Connect extracted entities to knowledge bases (Wikipedia, Wikidata) Disambiguate entities with same name Enrich entity information with external data Contextual Entity Recognition Use surrounding context for better accuracy Handle ambiguous entity mentions Consider document-level information Key Insights Custom NER essential for domain-specific applications Quality of training data directly impacts model performance Pre-trained models provide excellent starting point Continuous evaluation and retraining maintain accuracy Hands-On Labs Lab 1: Creating Custom Training Data import spacy from spacy.training import Example # Define custom training data TRAIN_DATA = [ (\u0026#34;Apple iPhone 15 Pro costs $999\u0026#34;, { \u0026#34;entities\u0026#34;: [(0, 5, \u0026#34;COMPANY\u0026#34;), (6, 19, \u0026#34;PRODUCT\u0026#34;), (26, 30, \u0026#34;MONEY\u0026#34;)] }), (\u0026#34;Samsung Galaxy S24 is priced at $899\u0026#34;, { \u0026#34;entities\u0026#34;: [(0, 7, \u0026#34;COMPANY\u0026#34;), (8, 18, \u0026#34;PRODUCT\u0026#34;), (33, 37, \u0026#34;MONEY\u0026#34;)] }), (\u0026#34;Google Pixel 8 available for $699\u0026#34;, { \u0026#34;entities\u0026#34;: [(0, 6, \u0026#34;COMPANY\u0026#34;), (7, 14, \u0026#34;PRODUCT\u0026#34;), (29, 33, \u0026#34;MONEY\u0026#34;)] }), (\u0026#34;Microsoft Surface Laptop starts at $1299\u0026#34;, { \u0026#34;entities\u0026#34;: [(0, 9, \u0026#34;COMPANY\u0026#34;), (10, 24, \u0026#34;PRODUCT\u0026#34;), (35, 40, \u0026#34;MONEY\u0026#34;)] }), (\u0026#34;Tesla Model 3 base price is $40000\u0026#34;, { \u0026#34;entities\u0026#34;: [(0, 5, \u0026#34;COMPANY\u0026#34;), (6, 13, \u0026#34;PRODUCT\u0026#34;), (28, 34, \u0026#34;MONEY\u0026#34;)] }) ] def create_training_examples(nlp, train_data): \u0026#34;\u0026#34;\u0026#34;Convert training data to spaCy Example format\u0026#34;\u0026#34;\u0026#34; examples = [] for text, annotations in train_data: doc = nlp.make_doc(text) example = Example.from_dict(doc, annotations) examples.append(example) return examples # Load blank model nlp = spacy.blank(\u0026#34;en\u0026#34;) # Create examples examples = create_training_examples(nlp, TRAIN_DATA) print(f\u0026#34;Created {len(examples)} training examples\u0026#34;) for i, example in enumerate(examples[:3]): print(f\u0026#34;\\nExample {i+1}:\u0026#34;) print(f\u0026#34;Text: {example.text}\u0026#34;) print(f\u0026#34;Entities: {[(ent.text, ent.label_) for ent in example.reference.ents]}\u0026#34;) Lab 2: Training Custom NER Model import random from spacy.training import Example from spacy.util import minibatch, compounding def train_ner_model(nlp, train_data, n_iter=30): \u0026#34;\u0026#34;\u0026#34;Train custom NER model\u0026#34;\u0026#34;\u0026#34; # Add NER pipeline if it doesn\u0026#39;t exist if \u0026#34;ner\u0026#34; not in nlp.pipe_names: ner = nlp.add_pipe(\u0026#34;ner\u0026#34;) else: ner = nlp.get_pipe(\u0026#34;ner\u0026#34;) # Add labels for _, annotations in train_data: for ent in annotations.get(\u0026#34;entities\u0026#34;): ner.add_label(ent[2]) # Disable other pipelines during training other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \u0026#34;ner\u0026#34;] with nlp.disable_pipes(*other_pipes): # Initialize optimizer optimizer = nlp.begin_training() # Training loop for iteration in range(n_iter): random.shuffle(train_data) losses = {} # Create batches batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001)) for batch in batches: examples = [] for text, annotations in batch: doc = nlp.make_doc(text) example = Example.from_dict(doc, annotations) examples.append(example) # Update model nlp.update(examples, drop=0.5, losses=losses) if iteration % 5 == 0: print(f\u0026#34;Iteration {iteration}, Loss: {losses[\u0026#39;ner\u0026#39;]:.4f}\u0026#34;) return nlp # Expanded training data EXPANDED_TRAIN_DATA = TRAIN_DATA + [ (\u0026#34;Amazon Echo Dot retails for $49\u0026#34;, { \u0026#34;entities\u0026#34;: [(0, 6, \u0026#34;COMPANY\u0026#34;), (7, 16, \u0026#34;PRODUCT\u0026#34;), (29, 32, \u0026#34;MONEY\u0026#34;)] }), (\u0026#34;Sony PlayStation 5 priced at $499\u0026#34;, { \u0026#34;entities\u0026#34;: [(0, 4, \u0026#34;COMPANY\u0026#34;), (5, 18, \u0026#34;PRODUCT\u0026#34;), (29, 33, \u0026#34;MONEY\u0026#34;)] }), (\u0026#34;Nintendo Switch OLED costs $349\u0026#34;, { \u0026#34;entities\u0026#34;: [(0, 8, \u0026#34;COMPANY\u0026#34;), (9, 20, \u0026#34;PRODUCT\u0026#34;), (27, 31, \u0026#34;MONEY\u0026#34;)] }), ] * 5 # Repeat for more training data # Train model nlp = spacy.blank(\u0026#34;en\u0026#34;) nlp = train_ner_model(nlp, EXPANDED_TRAIN_DATA, n_iter=30) # Test trained model test_text = \u0026#34;The new Apple MacBook Pro is available for $1999\u0026#34; doc = nlp(test_text) print(f\u0026#34;\\nTest Text: {test_text}\u0026#34;) print(\u0026#34;Extracted Entities:\u0026#34;) for ent in doc.ents: print(f\u0026#34; {ent.text:20} -\u0026gt; {ent.label_}\u0026#34;) Lab 3: Evaluating NER Model from spacy.scorer import Scorer def evaluate_ner_model(nlp, test_data): \u0026#34;\u0026#34;\u0026#34;Evaluate NER model performance\u0026#34;\u0026#34;\u0026#34; scorer = Scorer() examples = [] for text, annotations in test_data: doc = nlp.make_doc(text) example = Example.from_dict(doc, annotations) example.predicted = nlp(text) examples.append(example) # Calculate scores scores = scorer.score(examples) return scores # Test data TEST_DATA = [ (\u0026#34;HP Spectre x360 starts at $1199\u0026#34;, { \u0026#34;entities\u0026#34;: [(0, 2, \u0026#34;COMPANY\u0026#34;), (3, 15, \u0026#34;PRODUCT\u0026#34;), (26, 31, \u0026#34;MONEY\u0026#34;)] }), (\u0026#34;Dell XPS 13 available for $999\u0026#34;, { \u0026#34;entities\u0026#34;: [(0, 4, \u0026#34;COMPANY\u0026#34;), (5, 11, \u0026#34;PRODUCT\u0026#34;), (26, 30, \u0026#34;MONEY\u0026#34;)] }), (\u0026#34;Lenovo ThinkPad costs $1299\u0026#34;, { \u0026#34;entities\u0026#34;: [(0, 6, \u0026#34;COMPANY\u0026#34;), (7, 15, \u0026#34;PRODUCT\u0026#34;), (22, 27, \u0026#34;MONEY\u0026#34;)] }) ] # Evaluate scores = evaluate_ner_model(nlp, TEST_DATA) print(\u0026#34;\\nModel Evaluation:\u0026#34;) print(f\u0026#34;Precision: {scores[\u0026#39;ents_p\u0026#39;]:.4f}\u0026#34;) print(f\u0026#34;Recall: {scores[\u0026#39;ents_r\u0026#39;]:.4f}\u0026#34;) print(f\u0026#34;F1-Score: {scores[\u0026#39;ents_f\u0026#39;]:.4f}\u0026#34;) print(f\u0026#34;\\nPer-type scores:\u0026#34;) for label, metrics in scores[\u0026#39;ents_per_type\u0026#39;].items(): print(f\u0026#34; {label}: P={metrics[\u0026#39;p\u0026#39;]:.4f}, R={metrics[\u0026#39;r\u0026#39;]:.4f}, F={metrics[\u0026#39;f\u0026#39;]:.4f}\u0026#34;) Lab 4: Entity Relation Extraction class RelationExtractor: \u0026#34;\u0026#34;\u0026#34;Extract relationships between entities\u0026#34;\u0026#34;\u0026#34; def __init__(self, nlp): self.nlp = nlp def extract_relations(self, text): \u0026#34;\u0026#34;\u0026#34;Extract subject-relation-object triples\u0026#34;\u0026#34;\u0026#34; doc = self.nlp(text) relations = [] # Get all entities entities = list(doc.ents) # Simple pattern: ENTITY1 VERB ENTITY2 for i, ent1 in enumerate(entities): for ent2 in entities[i+1:]: # Find tokens between entities start = ent1.end end = ent2.start if end \u0026gt; start: between_tokens = doc[start:end] # Look for verbs verbs = [token.lemma_ for token in between_tokens if token.pos_ == \u0026#34;VERB\u0026#34;] if verbs: relations.append({ \u0026#39;subject\u0026#39;: ent1.text, \u0026#39;subject_type\u0026#39;: ent1.label_, \u0026#39;relation\u0026#39;: \u0026#39; \u0026#39;.join(verbs), \u0026#39;object\u0026#39;: ent2.text, \u0026#39;object_type\u0026#39;: ent2.label_ }) return relations # Load spaCy model with dependency parsing nlp_full = spacy.load(\u0026#34;en_core_web_sm\u0026#34;) extractor = RelationExtractor(nlp_full) # Test texts texts = [ \u0026#34;Steve Jobs founded Apple in Cupertino\u0026#34;, \u0026#34;Tim Cook became CEO of Apple in 2011\u0026#34;, \u0026#34;Microsoft acquired GitHub for $7.5 billion\u0026#34;, \u0026#34;Elon Musk leads Tesla and SpaceX\u0026#34; ] print(\u0026#34;Relation Extraction:\\n\u0026#34;) for text in texts: print(f\u0026#34;Text: {text}\u0026#34;) relations = extractor.extract_relations(text) for rel in relations: print(f\u0026#34; ({rel[\u0026#39;subject\u0026#39;]} [{rel[\u0026#39;subject_type\u0026#39;]}]) \u0026#34; f\u0026#34;--[{rel[\u0026#39;relation\u0026#39;]}]--\u0026gt; \u0026#34; f\u0026#34;({rel[\u0026#39;object\u0026#39;]} [{rel[\u0026#39;object_type\u0026#39;]}])\u0026#34;) print() Lab 5: Production NER Pipeline import json from pathlib import Path class ProductionNERSystem: \u0026#34;\u0026#34;\u0026#34;Complete NER system for production use\u0026#34;\u0026#34;\u0026#34; def __init__(self, model_path=None): if model_path and Path(model_path).exists(): self.nlp = spacy.load(model_path) else: self.nlp = spacy.load(\u0026#34;en_core_web_sm\u0026#34;) def extract_entities(self, text, return_format=\u0026#39;dict\u0026#39;): \u0026#34;\u0026#34;\u0026#34;Extract entities with multiple return formats\u0026#34;\u0026#34;\u0026#34; doc = self.nlp(text) if return_format == \u0026#39;dict\u0026#39;: return { \u0026#39;text\u0026#39;: text, \u0026#39;entities\u0026#39;: [ { \u0026#39;text\u0026#39;: ent.text, \u0026#39;label\u0026#39;: ent.label_, \u0026#39;start\u0026#39;: ent.start_char, \u0026#39;end\u0026#39;: ent.end_char } for ent in doc.ents ] } elif return_format == \u0026#39;json\u0026#39;: result = { \u0026#39;text\u0026#39;: text, \u0026#39;entities\u0026#39;: [ { \u0026#39;text\u0026#39;: ent.text, \u0026#39;label\u0026#39;: ent.label_, \u0026#39;start\u0026#39;: ent.start_char, \u0026#39;end\u0026#39;: ent.end_char } for ent in doc.ents ] } return json.dumps(result, indent=2) elif return_format == \u0026#39;list\u0026#39;: return [(ent.text, ent.label_, ent.start_char, ent.end_char) for ent in doc.ents] def batch_extract(self, texts): \u0026#34;\u0026#34;\u0026#34;Process multiple texts efficiently\u0026#34;\u0026#34;\u0026#34; results = [] for doc in self.nlp.pipe(texts): entities = [ { \u0026#39;text\u0026#39;: ent.text, \u0026#39;label\u0026#39;: ent.label_, \u0026#39;start\u0026#39;: ent.start_char, \u0026#39;end\u0026#39;: ent.end_char } for ent in doc.ents ] results.append({ \u0026#39;text\u0026#39;: doc.text, \u0026#39;entities\u0026#39;: entities }) return results def save_results(self, results, output_file): \u0026#34;\u0026#34;\u0026#34;Save extraction results to file\u0026#34;\u0026#34;\u0026#34; with open(output_file, \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: json.dump(results, f, indent=2, ensure_ascii=False) # Test production system ner_system = ProductionNERSystem() # Single extraction text = \u0026#34;Apple Inc. CEO Tim Cook announced the new iPhone 15 at an event in Cupertino, California on September 12, 2023.\u0026#34; result = ner_system.extract_entities(text, return_format=\u0026#39;dict\u0026#39;) print(\u0026#34;Single Extraction:\u0026#34;) print(json.dumps(result, indent=2)) # Batch extraction texts = [ \u0026#34;Amazon CEO Andy Jassy leads the company from Seattle.\u0026#34;, \u0026#34;Microsoft acquired LinkedIn for $26.2 billion in 2016.\u0026#34;, \u0026#34;Google was founded by Larry Page and Sergey Brin in 1998.\u0026#34; ] batch_results = ner_system.batch_extract(texts) print(\u0026#34;\\nBatch Extraction:\u0026#34;) for i, result in enumerate(batch_results): print(f\u0026#34;\\nText {i+1}: {result[\u0026#39;text\u0026#39;]}\u0026#34;) print(\u0026#34;Entities:\u0026#34;) for ent in result[\u0026#39;entities\u0026#39;]: print(f\u0026#34; - {ent[\u0026#39;text\u0026#39;]} ({ent[\u0026#39;label\u0026#39;]})\u0026#34;) Practice Exercises Create a custom NER model for a specific domain (medical, legal, finance) Build an entity linking system connecting entities to Wikipedia Implement active learning for efficient annotation Create a web interface for NER annotation and testing Build a knowledge graph from extracted entities and relations "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.10-week10/1.10.4-day49-2025-11-13/","title":"Day 49 - Reader Interface &amp; Search Implementation","tags":[],"description":"","content":"Date: 2025-11-13\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Book Reading Experience CloudFront Signed URLs for Content Delivery Security Strategy:\nBooks stored in S3 public/books/ but not publicly accessible CloudFront configured with Origin Access Control (OAC) S3 bucket policy allows only CloudFront to access objects Lambda generates signed GET URLs with short TTL (15 minutes) Users access content through signed URLs only Signed URL Flow:\nUser clicks \u0026#34;Read\u0026#34; → Frontend calls /books/{id}/read API → Lambda validates user authentication → Lambda checks book status (APPROVED) → Lambda generates CloudFront signed URL → Returns URL to frontend → Frontend loads PDF/ePub via signed URL PDF/ePub Rendering Implementation Options:\nreact-pdf: Render PDF in browser with canvas PDF.js: Mozilla\u0026rsquo;s PDF viewer (more features) epub.js: ePub reader with customization iframe: Simple but less control Reader Features:\nPage navigation (previous/next) Zoom in/out Fullscreen mode Bookmark support (store in DynamoDB) Reading progress tracking Search \u0026amp; Discovery DynamoDB GSI-Based Search Search Strategy:\nCreate Global Secondary Indexes (GSI) for searchable fields GSI1: PK=TITLE#{normalizedTitle}, SK=BOOK#{bookId} GSI2: PK=AUTHOR#{normalizedAuthor}, SK=BOOK#{bookId} Query GSI instead of expensive Scan operations Search Flow:\nUser types search query → Frontend calls /search API with query params → Lambda determines which GSI(s) to query → Query GSI1 (title) and/or GSI2 (author) → Intersection of results if multiple params → BatchGetItem to fetch full book metadata → Return results to frontend Frontend Search Implementation Search Components:\nSearch bar with autocomplete Filter panel (by author, genre, upload date) Sort options (newest, oldest, popular) Results grid/list view toggle Pagination or infinite scroll Search Optimization:\nDebounce search input (wait 300ms after typing) Cache recent search results (5 minutes) Show loading skeleton during search Display \u0026ldquo;No results\u0026rdquo; with suggestions Key Insights Signed URLs essential for content security - never expose S3 directly Short TTL on signed URLs prevents sharing/abuse GSI queries much cheaper and faster than Scan Normalized fields (lowercase, no special chars) improve search accuracy Client-side caching reduces API calls and improves UX Tasks Completed Book Detail Page\nCreated book detail view with metadata display Implemented cover image loading from S3 Added \u0026ldquo;Read Now\u0026rdquo; button (only for APPROVED books) Built related books section PDF Reader Integration\nIntegrated react-pdf library for PDF rendering Created reader component with navigation controls Implemented zoom controls (fit-width, fit-page, custom zoom) Added fullscreen mode toggle ePub Reader Integration\nIntegrated epub.js library for ePub rendering Created ePub reader with page turn animations Implemented text selection and note-taking Added adjustable font size and theme Signed URL Implementation\nCreated getReadUrl API client method Implemented signed URL fetching on \u0026ldquo;Read\u0026rdquo; click Added URL expiration handling (re-fetch on expire) Built loading state while fetching URL Search Functionality\nCreated search bar component with autocomplete Implemented search API integration (/search endpoint) Built search results page with grid/list view Added filter sidebar (author, genre, status) Search Optimization\nImplemented debounced search input (300ms delay) Added client-side result caching with React Query Created search history (stored in localStorage) Built \u0026ldquo;Recent Searches\u0026rdquo; dropdown Discovery Features\nCreated \u0026ldquo;Browse Books\u0026rdquo; page with all approved books Implemented category/genre filtering Added sort options (newest, title A-Z, author A-Z) Built \u0026ldquo;Recommended for You\u0026rdquo; section (future: ML-based) Challenges \u0026amp; Solutions Challenge: PDF rendering performance issues with large files\nSolution: Implemented lazy loading of pages (render visible pages only) and used Web Workers for PDF parsing\nChallenge: Signed URLs expiring during reading session\nSolution: Implemented auto-refresh mechanism that requests new URL 2 minutes before expiration\nChallenge: Search returning too many results causing slow UI\nSolution: Implemented server-side pagination with cursor-based navigation and virtual scrolling\nChallenge: ePub text selection interfering with page turns\nSolution: Added touch/click handlers with gesture detection to differentiate between selection and navigation\n"},{"uri":"https://anquoc211.github.io/AWS_Internship/5-workshop/5.4-module4/","title":"Login &amp; Session Management","tags":[],"description":"","content":"Build Login Component Create src/components/Login.js:\nimport { Auth } from \u0026#39;aws-amplify\u0026#39;; import { useState } from \u0026#39;react\u0026#39;; function Login() { const [email, setEmail] = useState(\u0026#39;\u0026#39;); const [password, setPassword] = useState(\u0026#39;\u0026#39;); const [user, setUser] = useState(null); const handleLogin = async (e) =\u0026gt; { e.preventDefault(); try { const user = await Auth.signIn(email, password); setUser(user); console.log(\u0026#39;Login successful:\u0026#39;, user); } catch (error) { console.error(\u0026#39;Error signing in:\u0026#39;, error); alert(error.message); } }; const handleLogout = async () =\u0026gt; { try { await Auth.signOut(); setUser(null); console.log(\u0026#39;Logged out successfully\u0026#39;); } catch (error) { console.error(\u0026#39;Error signing out:\u0026#39;, error); } }; return ( \u0026lt;div\u0026gt; {!user ? ( \u0026lt;form onSubmit={handleLogin}\u0026gt; \u0026lt;h2\u0026gt;Login\u0026lt;/h2\u0026gt; \u0026lt;input type=\u0026#34;email\u0026#34; value={email} onChange={(e) =\u0026gt; setEmail(e.target.value)} placeholder=\u0026#34;Email\u0026#34; required /\u0026gt; \u0026lt;input type=\u0026#34;password\u0026#34; value={password} onChange={(e) =\u0026gt; setPassword(e.target.value)} placeholder=\u0026#34;Password\u0026#34; required /\u0026gt; \u0026lt;button type=\u0026#34;submit\u0026#34;\u0026gt;Login\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; ) : ( \u0026lt;div\u0026gt; \u0026lt;h2\u0026gt;Welcome, {user.attributes.email}\u0026lt;/h2\u0026gt; \u0026lt;button onClick={handleLogout}\u0026gt;Logout\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; )} \u0026lt;/div\u0026gt; ); } export default Login; Check Authentication State Add to src/App.js:\nimport { Amplify } from \u0026#39;aws-amplify\u0026#39;; import { Auth } from \u0026#39;aws-amplify\u0026#39;; import { useEffect, useState } from \u0026#39;react\u0026#39;; import awsconfig from \u0026#39;./aws-exports\u0026#39;; import Login from \u0026#39;./components/Login\u0026#39;; Amplify.configure(awsconfig); function App() { const [user, setUser] = useState(null); const [loading, setLoading] = useState(true); useEffect(() =\u0026gt; { checkUser(); }, []); const checkUser = async () =\u0026gt; { try { const user = await Auth.currentAuthenticatedUser(); setUser(user); } catch (error) { setUser(null); } setLoading(false); }; if (loading) return \u0026lt;div\u0026gt;Loading...\u0026lt;/div\u0026gt;; return ( \u0026lt;div className=\u0026#34;App\u0026#34;\u0026gt; {user ? \u0026lt;h1\u0026gt;Welcome back!\u0026lt;/h1\u0026gt; : \u0026lt;Login /\u0026gt;} \u0026lt;/div\u0026gt; ); } export default App; Session Management Cognito automatically manages sessions:\nAccess Token: Valid for 1 hour, used for API authorization ID Token: Contains user attributes, expires after 1 hour Refresh Token: Valid for 30 days (configurable), refreshes access/ID tokens Auto-refresh Sessions Amplify automatically refreshes tokens:\n// Get current session const session = await Auth.currentSession(); console.log(\u0026#39;Access token:\u0026#39;, session.getAccessToken().getJwtToken()); // Amplify handles token refresh automatically // No manual refresh needed Protected Routes Create protected routes in your app:\nimport { Route, Navigate } from \u0026#39;react-router-dom\u0026#39;; function ProtectedRoute({ children }) { const [user, setUser] = useState(null); const [loading, setLoading] = useState(true); useEffect(() =\u0026gt; { checkAuth(); }, []); const checkAuth = async () =\u0026gt; { try { await Auth.currentAuthenticatedUser(); setUser(true); } catch { setUser(false); } setLoading(false); }; if (loading) return \u0026lt;div\u0026gt;Đang tải...\u0026lt;/div\u0026gt;; return user ? children : \u0026lt;Navigate to=\u0026#34;/login\u0026#34; /\u0026gt;; } export default ProtectedRoute; "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.4-week4/","title":"Week 4 - AWS Storage Services","tags":[],"description":"","content":"Week: 2025-09-29 đến 2025-10-03\nStatus: \u0026ldquo;Done\u0026rdquo;\nTổng quan tuần 4 Tuần này tập trung vào các dịch vụ lưu trữ của AWS, từ S3 object storage đến các giải pháp hybrid storage.\nNội dung chính Amazon S3 và Storage Classes S3 Static Website Hosting S3 Glacier for Archival AWS Snow Family AWS Storage Gateway Disaster Recovery Strategies AWS Backup Labs thực hành Lab 13: AWS Backup Lab 14: AWS VM Import/Export Lab 24: AWS Storage Gateway Lab 25: Amazon FSx Lab 57: Amazon S3 \u0026amp; CloudFront "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.1-week1/1.1.5-day05-2025-09-12/","title":"Day 05 - AWS Well-Architected Framework","tags":[],"description":"","content":"Date: 2025-09-12\nStatus: \u0026ldquo;Done\u0026rdquo;\nExploration AWS Well-Architected Framework A set of design principles and best practices for building reliable, secure, efficient, and cost-effective cloud architectures. The Well-Architected Tool in the Console provides self-assessments and improvement guidance. Six Pillars of Well-Architected Framework 1. Operational Excellence Focus on running and monitoring systems Continuous improvement of processes Automation of changes Response to events 2. Security Protect information and systems Identity and access management Detective controls Infrastructure protection Data protection 3. Reliability Recover from failures automatically Scale horizontally for resilience Test recovery procedures Manage change through automation 4. Performance Efficiency Use computing resources efficiently Select the right resource types Monitor performance Make informed decisions 5. Cost Optimization Avoid unnecessary costs Understand spending patterns Select appropriate services Optimize over time 6. Sustainability Minimize environmental impact Understand your impact Maximize utilization Use managed services Best Practices Review Design Principles Stop guessing capacity needs: Use auto-scaling Test at production scale: Clone environments easily Automate architecture experimentation: Use IaC Allow for evolutionary architectures: Design for change Drive architectures using data: Monitor and measure Improve through game days: Practice failure scenarios Week 1 Summary Tuần này đã hoàn thành các kiến thức nền tảng về AWS:\nHiểu về Cloud Computing và lợi ích\nNắm được AWS Global Infrastructure\nBiết cách sử dụng AWS Management Tools\nHọc về Cost Optimization strategies\nTìm hiểu AWS Well-Architected Framework\nLabs completed: 3 labs (IAM Setup, Budgets, Support Plans)\n"},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.2-week2/1.2.5-day10-2025-09-19/","title":"Day 10 - Elastic Load Balancing","tags":[],"description":"","content":"Date: 2025-09-19\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Elastic Load Balancing (ELB) Overview A fully managed service distributing traffic across multiple targets (EC2, containers, etc.). Supports HTTP, HTTPS, TCP, TLS. Can be deployed in public or private subnets. Provides DNS names; only NLB supports static IPs. Includes health checks and access logs (to S3). Supports sticky sessions (session affinity). Types: Application, Network, Classic, and Gateway Load Balancer. Application Load Balancer (ALB) Operates at Layer 7 (HTTP/HTTPS). Supports path-based routing (e.g., /mobile vs /desktop). Targets: EC2, Lambda, IP addresses, containers (ECS/EKS). ALB Features:\nHost-based routing Path-based routing HTTP header-based routing Query string parameter-based routing WebSocket support HTTP/2 support Network Load Balancer (NLB) Operates at Layer 4 (TCP/TLS). Supports static IPs and handles millions of requests per second. Targets: EC2, IP addresses, containers (ECS/EKS). NLB Features:\nUltra-low latency Static IP addresses Preserve source IP Long-lived TCP connections TLS termination Gateway Load Balancer (GWLB) Operates at Layer 3 (IP packets). Uses the GENEVE protocol on port 6081. Routes traffic to virtual appliances such as firewalls or monitoring tools. Partner list: aws.amazon.com/elasticloadbalancing/partners Exploration AWS Advanced Networking – Specialty Study Guide Official study guide covering exam topics, AWS network design principles, and real-world architecture scenarios. Hands-On Labs Lab 20 – AWS Transit Gateway Preparation Steps → 20-02 Create Transit Gateway → 20-03 Create TGW Attachments → 20-04 Create TGW Route Tables → 20-05 Add TGW Routes to VPC Route Tables → 20-06 Week 2 Summary Tuần này đã hoàn thành kiến thức về AWS Networking:\nAmazon VPC và Subnets\nSecurity Groups và NACLs\nVPC Peering và Transit Gateway\nVPN và Direct Connect\nElastic Load Balancing (ALB, NLB, GWLB)\nLabs completed: 4 labs (VPC Basics, Hybrid DNS, VPC Peering, Transit Gateway)\n"},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.3-week3/1.3.5-day15-2025-09-26/","title":"Day 15 - Lightsail, EFS &amp; FSx","tags":[],"description":"","content":"Date: 2025-09-26\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Amazon Lightsail Simplified compute service with predictable monthly pricing (starting ~$3.5/month). Includes bundled data transfer at lower rates than EC2. Ideal for small workloads, development, or testing environments. Supports snapshots for backups. Runs inside a managed VPC and can connect to standard VPCs via one-click peering. Lightsail Use Cases:\nSimple web applications WordPress sites Development/test environments Small business applications Learning and experimentation Lightsail vs EC2:\nFeature Lightsail EC2 Pricing Fixed monthly Pay-as-you-go Complexity Simple Advanced Scalability Limited Unlimited Target Small projects Enterprise Amazon EFS (Elastic File System) Fully managed NFSv4 file system mountable by multiple EC2 instances simultaneously. Scales automatically to petabytes. Pay only for the storage used (unlike EBS provisioned size). Can be mounted from on-prem via VPN or Direct Connect. EFS Features:\nConcurrent access from multiple instances Automatic scaling Regional service (multi-AZ) Lifecycle management Encryption at rest and in transit EFS Storage Classes:\nStandard: Frequently accessed files Infrequent Access (IA): Lower cost for rarely accessed files One Zone: Single AZ for cost savings Amazon FSx Managed, scalable file systems for Windows, Lustre, and NetApp ONTAP. AWS handles setup, scaling, and backups. Accessible from EC2, on-prem servers, or users via SMB or NFS protocols. FSx Variants:\nFSx for Windows File Server Native Windows file system SMB protocol support Active Directory integration DFS namespaces FSx for Lustre High-performance computing Machine learning workloads Sub-millisecond latencies S3 integration FSx for NetApp ONTAP Multi-protocol (NFS, SMB, iSCSI) Data deduplication and compression Snapshots and replication AWS Application Migration Service (MGN) Used for migrating or replicating physical/virtual servers to AWS for DR or modernization. Continuously replicates source machines to lightweight staging instances on EC2. During cut-over, MGN launches fully functional EC2 instances from the replicated data. Migration Phases:\nInstall agent on source servers Continuous replication to AWS Testing with non-disruptive test instances Cutover to production Exploration Microsoft Workloads on AWS A curated series covering deployment, optimization, and best practices for running Microsoft workloads on AWS. Week 3 Summary Tuần này đã hoàn thành kiến thức về AWS Compute:\nAmazon EC2 và Instance Types\nAMI, EBS, Instance Store\nEC2 Auto Scaling\nEC2 Pricing Options\nLightsail, EFS, FSx\nLabs completed: 3 labs (IAM Setup, Budgets, Support Plans)\n"},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.4-week4/1.4.5-day20-2025-10-03/","title":"Day 20 - AWS Backup &amp; FSx","tags":[],"description":"","content":"Date: 2025-10-03\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes AWS Backup Centralized backup service for automating and governing data protection at scale.\nKey Capabilities Central management: Define and apply backup policies across services. Multi-service support: EC2, EBS, RDS, DynamoDB, EFS, Storage Gateway, S3, and more. Scheduling \u0026amp; lifecycle: Automate backups and retention. Compliance: Support for governance and audit requirements. Benefits Operational simplicity: No custom scripts or disparate tools. Time savings: Automated, policy-driven protection. Reporting \u0026amp; audit: Visibility into backup status and compliance. Backup Vault Lock Immutability controls to prevent modifications or deletions of protected backups for strict compliance. AWS Backup Features:\nCross-region backup copy Cross-account backup Backup policies (plans) Lifecycle management Encryption at rest Tag-based backup policies Backup Plan Example:\n{ \u0026#34;BackupPlanName\u0026#34;: \u0026#34;DailyBackups\u0026#34;, \u0026#34;Rules\u0026#34;: [{ \u0026#34;RuleName\u0026#34;: \u0026#34;DailyRule\u0026#34;, \u0026#34;ScheduleExpression\u0026#34;: \u0026#34;cron(0 5 ? * * *)\u0026#34;, \u0026#34;StartWindowMinutes\u0026#34;: 60, \u0026#34;CompletionWindowMinutes\u0026#34;: 120, \u0026#34;Lifecycle\u0026#34;: { \u0026#34;DeleteAfterDays\u0026#34;: 30, \u0026#34;MoveToColdStorageAfterDays\u0026#34;: 7 } }] } Exploration AWS Skill Builder Curated learning plans and deep-dive content for storage specialists: Storage Learning Plan: Block Storage Storage Learning Plan: Object Storage Hands-On Labs Lab 13 – AWS Backup Create S3 Bucket → 13-02.1 Deploy Infrastructure → 13-02.2 Create Backup Plan → 13-03 Set Up Notifications → 13-04 Test Restore → 13-05 Clean Up Resources → 13-06 Lab 25 – Amazon FSx (File Systems) Create SSD Multi-AZ File System → 25-2.2 Create HDD Multi-AZ File System → 25-2.3 Create New File Shares → 25-3 Test Performance → 25-4 Monitor Performance → 25-5 Enable Data Deduplication → 25-6 Enable Shadow Copies → 25-7 Manage User Sessions and Open Files → 25-8 Enable User Storage Quotas → 25-9 Scale Throughput Capacity → 25-11 Scale Storage Capacity → 25-12 Delete Environment → 25-13 Week 4 Summary Tuần này đã hoàn thành kiến thức về AWS Storage:\nAmazon S3 và Storage Classes\nS3 Static Website và CORS\nAWS Snow Family\nAWS Storage Gateway\nDisaster Recovery Strategies\nAWS Backup\nLabs completed: 5 labs (Backup, VM Import/Export, Storage Gateway, FSx, S3 \u0026amp; CloudFront)\n"},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.5-week5/1.5.5-day25-2025-10-10/","title":"Day 25 - AWS Security Hub &amp; Automation","tags":[],"description":"","content":"Date: 2025-10-10\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes AWS Security Hub Aggregates and prioritizes security findings and posture across accounts/services. Capabilities\nAutomated checks, normalized findings, prioritized remediation workflows. Compliance standards: CIS AWS Foundations, PCI DSS, AWS Foundational Security Best Practices. Integrations\nGuardDuty, Inspector, Macie, Firewall Manager, IAM Access Analyzer, plus partner tools. Outcomes\nLess time aggregating, more time fixing; unified visibility and improved security hygiene. Security Hub Features:\nContinuous security posture monitoring Automated compliance checks Centralized findings across accounts Integration with 50+ AWS and partner services Custom insights and dashboards Automated remediation with EventBridge Security Standards:\nAWS Foundational Security Best Practices: 50+ controls CIS AWS Foundations Benchmark: Industry best practices PCI DSS: Payment card industry standards NIST: National Institute of Standards framework Security Automation AWS Services for Automation:\nAWS Config: Track resource configuration changes Amazon EventBridge: Event-driven automation AWS Lambda: Serverless remediation functions AWS Systems Manager: Automated patching and compliance Common Automation Patterns:\nAuto-remediate non-compliant resources Automated incident response Security group rule validation Encryption enforcement Tag compliance Exploration AWS Certified Security – Specialty: All-in-One Exam Guide (SCS-C01) Comprehensive preparation material for the Security Specialty certification. Hands-On Labs Lab 18 – AWS Security Hub Enable Security Hub → 18-02 Score for Each Set of Criteria → 18-03 Clean Up Resources → 18-04 Lab 22 – AWS Lambda Automation with Slack Create VPC → 22-2.1 Create Security Group → 22-2.2 Create EC2 Instance → 22-2.3 Incoming Webhooks (Slack) → 22-2.4 Create Tag for Instance → 22-3 Create Role for Lambda → 22-4 Function: Stop Instance → 22-5.1 Function: Start Instance → 22-5.2 Check Result → 22-6 Clean Up Resources → 22-7 Lab 27 – AWS Resource Groups \u0026amp; Tagging (Part 2) Use Tags with CLI → 27-2.2 Create a Resource Group → 27-3 Clean Up Resources → 27-4 Lab 33 – AWS KMS \u0026amp; CloudTrail Integration (Part 2) Create CloudTrail → 33-5.1 Log to CloudTrail → 33-5.2 Create Amazon Athena → 33-5.3 Query with Athena → 33-5.4 Test \u0026amp; Share Encrypted S3 Data → 33-6 Resource Cleanup → 33-7 Lab 44 – IAM Advanced Role Control Create IAM Group → 44-2 Create IAM Users → 44-3.1 Check Permissions → 44-3.2 Create Admin IAM Role → 44-4.1 Configure Switch Role → 44-4.2 Restrict Switch Role by IP → 44-4.3.1 Restrict Switch Role by Time → 44-4.3.2 Clean Up Resources → 44-5 Week 5 Summary Tuần này đã hoàn thành kiến thức về AWS Security:\nShared Responsibility Model\nAWS IAM (Users, Groups, Roles, Policies)\nAmazon Cognito\nAWS Organizations \u0026amp; SCPs\nAWS Identity Center\nAWS KMS\nAWS Security Hub\nLabs completed: 8 labs (Security Hub, Lambda Automation, Resource Groups, IAM Policies, KMS \u0026amp; CloudTrail, Advanced Role Control)\n"},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.6-week6/1.6.5-day30-2025-10-17/","title":"Day 30 - Database Migration &amp; Best Practices","tags":[],"description":"","content":"Date: 2025-10-17\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes AWS Database Migration Service (DMS) AWS DMS helps migrate databases to AWS quickly and securely with minimal downtime.\nKey Features:\nHomogeneous Migrations: Same database engine (e.g., Oracle to Oracle) Heterogeneous Migrations: Different engines (e.g., Oracle to Aurora) Continuous Replication: Keep source and target in sync Schema Conversion: AWS Schema Conversion Tool (SCT) Migration Types:\nFull Load: One-time migration of existing data Full Load + CDC: Initial load plus ongoing changes CDC Only: Replicate only ongoing changes Supported Sources:\nOracle, SQL Server, MySQL, PostgreSQL, MongoDB, SAP ASE, IBM Db2 Amazon RDS, Amazon Aurora, Amazon S3 Supported Targets:\nAmazon RDS, Amazon Aurora, Amazon Redshift, Amazon DynamoDB Amazon S3, Amazon Elasticsearch, Amazon Kinesis Data Streams Database Best Practices Performance Optimization RDS/Aurora:\nUse appropriate instance types Enable Enhanced Monitoring Optimize queries and indexes Use Read Replicas for read-heavy workloads Enable Performance Insights Redshift:\nChoose appropriate distribution keys Use sort keys for frequently filtered columns Vacuum and analyze tables regularly Use columnar compression Implement workload management (WLM) ElastiCache:\nChoose appropriate node types Use cluster mode for Redis scalability Implement proper cache eviction policies Monitor cache hit rates Use connection pooling Security Best Practices Encryption at Rest: Enable for all databases Encryption in Transit: Use SSL/TLS connections Network Isolation: Deploy in private subnets IAM Authentication: Use for RDS/Aurora when possible Secrets Manager: Store database credentials securely Security Groups: Restrict access to minimum required Audit Logging: Enable CloudWatch Logs and CloudTrail High Availability \u0026amp; Disaster Recovery RDS/Aurora:\nEnable Multi-AZ for production workloads Configure automated backups Test restore procedures regularly Use Aurora Global Database for multi-region DR Implement read replicas in different regions Redshift:\nEnable automated snapshots Copy snapshots to other regions Use Redshift Spectrum for data lake integration Implement cross-region snapshot copy ElastiCache:\nEnable Multi-AZ with automatic failover (Redis) Configure backup and restore (Redis) Use cluster mode for Redis scalability Implement application-level retry logic Cost Optimization Right-sizing: Choose appropriate instance types Reserved Instances: Commit for 1-3 years for discounts Aurora Serverless: For variable workloads Redshift Serverless: For intermittent analytics Storage Optimization: Use appropriate storage types Lifecycle Policies: Archive old data to S3/Glacier Monitor Usage: Use Cost Explorer and Budgets Exploration The Data Warehouse Toolkit Canonical reference for dimensional modeling and DW design patterns. Week 6 Summary Tuần này đã hoàn thành kiến thức về AWS Database Services:\nDatabase Fundamentals (RDBMS, NoSQL, OLTP vs OLAP)\nAmazon RDS \u0026amp; Aurora\nAmazon Redshift\nAmazon ElastiCache\nAWS Database Migration Service\nLabs completed: 2 labs (RDS \u0026amp; EC2 Integration, Database Migration Service)\nTổng kết 6 tuần đầu (8/9 - 17/10/2025) 30 ngày làm việc đã hoàn thành:\nWeek 1: Cloud Computing Fundamentals AWS basics, infrastructure, management tools, cost optimization Week 2: AWS Networking Services VPC, subnets, security groups, load balancing, hybrid connectivity Week 3: AWS Compute Services EC2, AMI, storage, auto scaling, pricing models Week 4: AWS Storage Services S3, Glacier, Snow Family, Storage Gateway, backup \u0026amp; DR Week 5: AWS Security \u0026amp; Identity IAM, Cognito, Organizations, KMS, Security Hub Week 6: AWS Database Services RDS, Aurora, Redshift, ElastiCache, DMS Tổng số labs hoàn thành: 25+ labs\nTiếp theo: Tuần 7-8 sẽ bắt đầu từ ngày 20/10/2025\n"},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.7-week7/1.7.5-day35-2025-10-24/","title":"Day 35 - Contract Testing &amp; Retrospective","tags":[],"description":"","content":"Date: 2025-10-24\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Contract Testing với Schemathesis Dùng lệnh schemathesis run --checks all --workers 4 --url http://127.0.0.1:8000/openapi.yaml để tạo test tự động. Schemathesis sinh nhiều tình huống ngẫu nhiên (happy path, edge cases, missing fields). Giúp đảm bảo backend không trả sai schema khi frontend chuyển sang gọi API thật. Lợi ích Không cần viết tay test case phức tạp. Giảm rủi ro mismatch sau khi refactor. Hoạt động như quality gate trong pipeline CI. Mistakes \u0026amp; Fixes Mistake Nguyên nhân Cách khắc phục Tạo cả error.tsx và not-found.tsx Redundant Giữ lại not-found.tsx thôi Dùng --base-url trong Schemathesis Lệnh sai Dùng --url đúng chuẩn Timeout khi load /openapi.json CORS hoặc chậm Dùng file YAML trực tiếp Over-engineer backend Quá nhiều file sớm Bắt đầu đơn giản, refactor sau Workflow Chuẩn Đã Validate 1. Define Contract (OpenAPI) 2. Mock API (Prism) 3. Build Frontend với mock data 4. Implement Backend theo spec 5. Switch sang real API 6. Contract Testing (Schemathesis) Hỗ trợ phát triển song song giữa team frontend và backend. Giảm xung đột, tăng tốc demo và giữ chất lượng ổn định. Key Insights Contract-first development giữ spec đồng bộ và giảm lỗi integration. Vertical slice cho phép release từng phần, nhận feedback sớm. Automation (Prism, Schemathesis) giúp giảm effort test thủ công. Bắt đầu đơn giản, refactor dần khi cần mở rộng. Hands-On Labs Chạy Schemathesis với spec mới nhất và ghi nhận kết quả. Cập nhật README workflow cho team tham khảo. Chuẩn bị backlog cho vertical slice tiếp theo dựa trên feedback demo. "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.8-week8/1.8.5-day40-2025-10-31/","title":"Day 40 - Building NLP Projects","tags":[],"description":"","content":"Date: 2025-10-31 Status: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Building Production NLP Systems NLP Project Workflow Data Collection → Preprocessing → Feature Engineering → Model Training → Evaluation → Deployment → Monitoring Each stage requires careful consideration and iteration Data quality determines model quality Continuous monitoring essential for production systems Common NLP Project Types Text Classification: Categorizing documents into predefined classes Chatbots: Conversational AI for customer service or information retrieval Information Extraction: Extracting structured data from unstructured text Text Summarization: Creating concise summaries of longer documents Machine Translation: Translating text between languages Best Practices Start simple: baseline models before complex solutions Version control your data and models Document preprocessing decisions Test on real-world data, not just clean datasets Monitor model performance in production Have fallback strategies for model failures Text Classification Fundamentals Approach Data preparation: Label collection and cleaning Feature extraction: TF-IDF, word embeddings, or contextual embeddings Model selection: Naive Bayes, SVM, or neural networks Evaluation: Accuracy, precision, recall, F1-score Iteration: Improve based on error analysis Popular Algorithms Naive Bayes: Fast, works well with limited data SVM: Effective for text with clear margins Logistic Regression: Simple and interpretable Random Forest: Handles non-linear patterns Neural Networks: Best performance but requires more data Simple Chatbot Architecture Components Intent Recognition: Understanding user\u0026rsquo;s goal Entity Extraction: Identifying key information Dialogue Management: Maintaining conversation context Response Generation: Creating appropriate replies Implementation Approaches Rule-based: Pattern matching for simple queries Retrieval-based: Select from predefined responses Generative: Generate responses using language models Hybrid: Combine multiple approaches Key Insights Start with simple approaches and iterate Real-world data is messy - robust preprocessing is critical Model interpretability matters for production systems User experience is as important as model accuracy Always have a human-in-the-loop option for critical applications Hands-On Labs Lab 1: Text Classification - Spam Detection from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.model_selection import train_test_split from sklearn.naive_bayes import MultinomialNB from sklearn.metrics import classification_report, confusion_matrix import pandas as pd # Sample data (in practice, use real dataset) emails = [ (\u0026#34;Win a free iPhone now!\u0026#34;, \u0026#34;spam\u0026#34;), (\u0026#34;Meeting tomorrow at 3pm\u0026#34;, \u0026#34;ham\u0026#34;), (\u0026#34;Claim your prize money today\u0026#34;, \u0026#34;spam\u0026#34;), (\u0026#34;Project update attached\u0026#34;, \u0026#34;ham\u0026#34;), (\u0026#34;Get rich quick! Click here!\u0026#34;, \u0026#34;spam\u0026#34;), (\u0026#34;Lunch plans for Friday?\u0026#34;, \u0026#34;ham\u0026#34;), # Add more examples... ] # Prepare data texts = [email[0] for email in emails] labels = [email[1] for email in emails] # Split data X_train, X_test, y_train, y_test = train_test_split( texts, labels, test_size=0.2, random_state=42 ) # Feature extraction vectorizer = TfidfVectorizer(max_features=1000, stop_words=\u0026#39;english\u0026#39;) X_train_vec = vectorizer.fit_transform(X_train) X_test_vec = vectorizer.transform(X_test) # Train model classifier = MultinomialNB() classifier.fit(X_train_vec, y_train) # Predict y_pred = classifier.predict(X_test_vec) # Evaluate print(\u0026#34;Classification Report:\u0026#34;) print(classification_report(y_test, y_pred)) # Test new email new_email = [\u0026#34;Congratulations! You won $1000000\u0026#34;] new_email_vec = vectorizer.transform(new_email) prediction = classifier.predict(new_email_vec) print(f\u0026#34;\\nNew email prediction: {prediction[0]}\u0026#34;) Lab 2: Sentiment Classifier from sklearn.linear_model import LogisticRegression from nltk.tokenize import word_tokenize from nltk.corpus import stopwords import string # Preprocessing function def preprocess(text): # Lowercase and tokenize tokens = word_tokenize(text.lower()) # Remove stopwords and punctuation stop_words = set(stopwords.words(\u0026#39;english\u0026#39;)) tokens = [w for w in tokens if w not in stop_words and w not in string.punctuation] return \u0026#39; \u0026#39;.join(tokens) # Sample movie reviews reviews = [ (\u0026#34;This movie was amazing! Loved every minute.\u0026#34;, \u0026#34;positive\u0026#34;), (\u0026#34;Terrible film, waste of time and money.\u0026#34;, \u0026#34;negative\u0026#34;), (\u0026#34;Great acting and storyline, highly recommend!\u0026#34;, \u0026#34;positive\u0026#34;), (\u0026#34;Boring and predictable, wouldn\u0026#39;t watch again.\u0026#34;, \u0026#34;negative\u0026#34;), (\u0026#34;Masterpiece! One of the best films I\u0026#39;ve seen.\u0026#34;, \u0026#34;positive\u0026#34;), # Add more examples... ] # Preprocess data texts = [preprocess(review[0]) for review in reviews] labels = [review[1] for review in reviews] # Split and vectorize X_train, X_test, y_train, y_test = train_test_split( texts, labels, test_size=0.2, random_state=42 ) vectorizer = TfidfVectorizer() X_train_vec = vectorizer.fit_transform(X_train) X_test_vec = vectorizer.transform(X_test) # Train model = LogisticRegression(max_iter=1000) model.fit(X_train_vec, y_train) # Evaluate accuracy = model.score(X_test_vec, y_test) print(f\u0026#34;Accuracy: {accuracy:.2f}\u0026#34;) # Test test_review = \u0026#34;This movie exceeded my expectations, truly wonderful!\u0026#34; test_vec = vectorizer.transform([preprocess(test_review)]) prediction = model.predict(test_vec) print(f\u0026#34;\\nReview: {test_review}\u0026#34;) print(f\u0026#34;Sentiment: {prediction[0]}\u0026#34;) Lab 3: Simple Rule-Based Chatbot import re import random class SimpleChatbot: def __init__(self): self.patterns = { r\u0026#39;hi|hello|hey\u0026#39;: [ \u0026#34;Hello! How can I help you?\u0026#34;, \u0026#34;Hi there! What can I do for you?\u0026#34;, \u0026#34;Hey! How are you doing?\u0026#34; ], r\u0026#39;how are you\u0026#39;: [ \u0026#34;I\u0026#39;m doing great, thanks for asking!\u0026#34;, \u0026#34;I\u0026#39;m well, how about you?\u0026#34;, \u0026#34;Doing fine! How can I assist you?\u0026#34; ], r\u0026#39;what is your name\u0026#39;: [ \u0026#34;I\u0026#39;m a simple chatbot created to assist you!\u0026#34;, \u0026#34;You can call me ChatBot!\u0026#34;, \u0026#34;I\u0026#39;m your friendly assistant bot!\u0026#34; ], r\u0026#39;bye|goodbye|see you\u0026#39;: [ \u0026#34;Goodbye! Have a great day!\u0026#34;, \u0026#34;See you later!\u0026#34;, \u0026#34;Bye! Come back soon!\u0026#34; ], r\u0026#39;thank you|thanks\u0026#39;: [ \u0026#34;You\u0026#39;re welcome!\u0026#34;, \u0026#34;Happy to help!\u0026#34;, \u0026#34;Anytime!\u0026#34; ] } self.default_responses = [ \u0026#34;I\u0026#39;m not sure I understand. Can you rephrase that?\u0026#34;, \u0026#34;Could you please elaborate?\u0026#34;, \u0026#34;Interesting! Tell me more.\u0026#34; ] def get_response(self, user_input): user_input = user_input.lower() # Check each pattern for pattern, responses in self.patterns.items(): if re.search(pattern, user_input): return random.choice(responses) # Default response return random.choice(self.default_responses) def chat(self): print(\u0026#34;Chatbot: Hello! I\u0026#39;m a simple chatbot. Type \u0026#39;quit\u0026#39; to exit.\u0026#34;) while True: user_input = input(\u0026#34;You: \u0026#34;) if user_input.lower() == \u0026#39;quit\u0026#39;: print(\u0026#34;Chatbot: Goodbye!\u0026#34;) break response = self.get_response(user_input) print(f\u0026#34;Chatbot: {response}\u0026#34;) # Test the chatbot bot = SimpleChatbot() # Interactive mode # bot.chat() # Test individual messages test_messages = [ \u0026#34;Hello!\u0026#34;, \u0026#34;How are you?\u0026#34;, \u0026#34;What is your name?\u0026#34;, \u0026#34;Thanks for your help!\u0026#34;, \u0026#34;Tell me about NLP\u0026#34; ] for message in test_messages: print(f\u0026#34;You: {message}\u0026#34;) print(f\u0026#34;Bot: {bot.get_response(message)}\\n\u0026#34;) Lab 4: Intent-Based Chatbot class IntentChatbot: def __init__(self): self.intents = { \u0026#39;greeting\u0026#39;: { \u0026#39;patterns\u0026#39;: [\u0026#39;hello\u0026#39;, \u0026#39;hi\u0026#39;, \u0026#39;hey\u0026#39;, \u0026#39;good morning\u0026#39;], \u0026#39;responses\u0026#39;: [\u0026#39;Hello!\u0026#39;, \u0026#39;Hi there!\u0026#39;, \u0026#39;Greetings!\u0026#39;] }, \u0026#39;weather\u0026#39;: { \u0026#39;patterns\u0026#39;: [\u0026#39;weather\u0026#39;, \u0026#39;temperature\u0026#39;, \u0026#39;forecast\u0026#39;], \u0026#39;responses\u0026#39;: [ \u0026#39;I cannot check the weather, but you can try a weather app!\u0026#39;, \u0026#39;For weather info, please check a weather service.\u0026#39; ] }, \u0026#39;time\u0026#39;: { \u0026#39;patterns\u0026#39;: [\u0026#39;time\u0026#39;, \u0026#39;clock\u0026#39;, \u0026#39;what time\u0026#39;], \u0026#39;responses\u0026#39;: [\u0026#39;Please check your system clock for the current time.\u0026#39;] }, \u0026#39;capabilities\u0026#39;: { \u0026#39;patterns\u0026#39;: [\u0026#39;what can you do\u0026#39;, \u0026#39;your features\u0026#39;, \u0026#39;help\u0026#39;], \u0026#39;responses\u0026#39;: [ \u0026#39;I can chat with you, answer simple questions, and assist with basic queries!\u0026#39;, \u0026#39;I\\\u0026#39;m a simple chatbot that can have basic conversations.\u0026#39; ] } } def classify_intent(self, text): text = text.lower() for intent, data in self.intents.items(): for pattern in data[\u0026#39;patterns\u0026#39;]: if pattern in text: return intent return \u0026#39;unknown\u0026#39; def get_response(self, text): intent = self.classify_intent(text) if intent == \u0026#39;unknown\u0026#39;: return \u0026#34;I\u0026#39;m not sure how to respond to that. Try asking about weather, time, or what I can do!\u0026#34; return random.choice(self.intents[intent][\u0026#39;responses\u0026#39;]) # Test intent_bot = IntentChatbot() queries = [ \u0026#34;Hello!\u0026#34;, \u0026#34;What\u0026#39;s the weather like?\u0026#34;, \u0026#34;What time is it?\u0026#34;, \u0026#34;What can you do?\u0026#34;, \u0026#34;Tell me a joke\u0026#34; ] for query in queries: print(f\u0026#34;User: {query}\u0026#34;) print(f\u0026#34;Bot: {intent_bot.get_response(query)}\u0026#34;) print(f\u0026#34;Intent: {intent_bot.classify_intent(query)}\\n\u0026#34;) Lab 5: Complete NLP Pipeline Project import spacy from textblob import TextBlob from collections import Counter class TextAnalyzer: def __init__(self): self.nlp = spacy.load(\u0026#34;en_core_web_sm\u0026#34;) def analyze(self, text): \u0026#34;\u0026#34;\u0026#34;Complete text analysis pipeline\u0026#34;\u0026#34;\u0026#34; # Basic stats doc = self.nlp(text) word_count = len([token for token in doc if not token.is_punct]) sentence_count = len(list(doc.sents)) # Entities entities = [(ent.text, ent.label_) for ent in doc.ents] # Sentiment blob = TextBlob(text) sentiment = { \u0026#39;polarity\u0026#39;: blob.sentiment.polarity, \u0026#39;subjectivity\u0026#39;: blob.sentiment.subjectivity } # Keywords (most common meaningful words) keywords = Counter([token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct and token.pos_ in [\u0026#39;NOUN\u0026#39;, \u0026#39;VERB\u0026#39;, \u0026#39;ADJ\u0026#39;]]) return { \u0026#39;word_count\u0026#39;: word_count, \u0026#39;sentence_count\u0026#39;: sentence_count, \u0026#39;entities\u0026#39;: entities, \u0026#39;sentiment\u0026#39;: sentiment, \u0026#39;top_keywords\u0026#39;: keywords.most_common(5) } # Test analyzer = TextAnalyzer() sample_article = \u0026#34;\u0026#34;\u0026#34; Apple Inc. announced today that Tim Cook will keynote their annual conference in Cupertino, California. The event will showcase innovative products including the new iPhone and revolutionary software updates. Industry experts are excited about the potential impact on technology markets worldwide. \u0026#34;\u0026#34;\u0026#34; results = analyzer.analyze(sample_article) print(\u0026#34;=== Text Analysis Results ===\\n\u0026#34;) print(f\u0026#34;Word Count: {results[\u0026#39;word_count\u0026#39;]}\u0026#34;) print(f\u0026#34;Sentence Count: {results[\u0026#39;sentence_count\u0026#39;]}\u0026#34;) print(\u0026#34;\\nNamed Entities:\u0026#34;) for entity, label in results[\u0026#39;entities\u0026#39;]: print(f\u0026#34; {entity} ({label})\u0026#34;) print(f\u0026#34;\\nSentiment:\u0026#34;) print(f\u0026#34; Polarity: {results[\u0026#39;sentiment\u0026#39;][\u0026#39;polarity\u0026#39;]:.2f}\u0026#34;) print(f\u0026#34; Subjectivity: {results[\u0026#39;sentiment\u0026#39;][\u0026#39;subjectivity\u0026#39;]:.2f}\u0026#34;) print(\u0026#34;\\nTop Keywords:\u0026#34;) for word, count in results[\u0026#39;top_keywords\u0026#39;]: print(f\u0026#34; {word}: {count}\u0026#34;) Practice Exercises Build a news article classifier for different categories Create a FAQ chatbot for a specific domain Implement a text summarization tool Build a keyword extraction system Create a complete sentiment analysis dashboard Week 8 Summary This week covered:\nNLP fundamentals and text preprocessing Advanced preprocessing (stemming, lemmatization) Text analysis and sentiment analysis Named Entity Recognition with spaCy Building practical NLP applications Key Skills Acquired:\nText preprocessing pipelines Sentiment analysis implementation Entity extraction and classification Text classification model building Simple chatbot development "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.9-week9/1.9.5-day45-2025-11-07/","title":"Day 45 - NLP Integration &amp; Final Project","tags":[],"description":"","content":"Date: 2025-11-07\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Week 9 Recap Concepts Covered This Week Day 41: NLP fundamentals revision - tokenization, preprocessing pipelines Day 42: Advanced text classification - ensemble methods, imbalanced data Day 43: Production sentiment analysis - aspect-based, emotion detection, APIs Day 44: Advanced NER - custom training, entity linking, relation extraction Day 45: Integration \u0026amp; complete NLP systems Key Learnings Building production-ready NLP systems requires more than good models Integration of multiple NLP components creates powerful applications Monitoring, evaluation, and continuous improvement are essential Domain adaptation crucial for real-world performance Integrating NLP Components Multi-Component Pipeline Text Input → Preprocessing → Classification → NER → Sentiment → Output Benefits of Integration:\nSingle API for multiple NLP tasks Shared preprocessing reduces computation Consistent error handling across components Easier maintenance and updates Design Patterns Pipeline Pattern\nSequential processing of text through multiple stages Each stage can be independently developed and tested Easy to add/remove stages Microservices Pattern\nEach NLP task as separate service Scalable and independently deployable Communication via REST API or message queues Monolithic Pattern\nAll NLP tasks in single application Simpler deployment Suitable for smaller applications Building Complete NLP Applications Requirements Analysis Functional Requirements:\nWhat NLP tasks are needed? What accuracy is acceptable? Real-time or batch processing? Input/output formats? Non-Functional Requirements:\nPerformance (latency, throughput) Scalability (concurrent users, data volume) Reliability (uptime, error rates) Security (data privacy, authentication) System Architecture Layers:\nAPI Layer: REST/GraphQL endpoints, request validation Business Logic Layer: NLP pipeline orchestration Processing Layer: Individual NLP components Data Layer: Model storage, caching, logging Best Practices Code Organization Separate concerns: preprocessing, models, API, utilities Configuration management (environment variables, config files) Dependency injection for flexibility Clear naming conventions Testing Strategy Unit tests for individual components Integration tests for pipelines End-to-end tests for complete workflows Performance/load testing Deployment Containerization (Docker) for consistency CI/CD pipelines for automated deployment Blue-green or canary deployments for safety Health checks and monitoring Key Insights Start simple, iterate based on real usage Log everything for debugging and improvement Plan for failure - graceful degradation User feedback loop essential for improvement Documentation as important as code Hands-On Labs Lab 1: Multi-Component NLP Pipeline import spacy from textblob import TextBlob from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.naive_bayes import MultinomialNB import pickle class ComprehensiveNLPPipeline: \u0026#34;\u0026#34;\u0026#34;Integrated NLP pipeline with multiple components\u0026#34;\u0026#34;\u0026#34; def __init__(self): # Load models self.nlp = spacy.load(\u0026#34;en_core_web_sm\u0026#34;) # Initialize components self.vectorizer = TfidfVectorizer(max_features=100) self.classifier = MultinomialNB() # Train simple classifier (in production, load pre-trained) sample_texts = [\u0026#34;tech product review\u0026#34;, \u0026#34;political news article\u0026#34;, \u0026#34;sports game summary\u0026#34;] * 10 sample_labels = [\u0026#34;technology\u0026#34;, \u0026#34;politics\u0026#34;, \u0026#34;sports\u0026#34;] * 10 X = self.vectorizer.fit_transform(sample_texts) self.classifier.fit(X, sample_labels) def preprocess(self, text): \u0026#34;\u0026#34;\u0026#34;Clean and normalize text\u0026#34;\u0026#34;\u0026#34; # Lowercase text = text.lower() # Remove extra whitespace text = \u0026#39; \u0026#39;.join(text.split()) return text def extract_entities(self, text): \u0026#34;\u0026#34;\u0026#34;Extract named entities\u0026#34;\u0026#34;\u0026#34; doc = self.nlp(text) entities = [ { \u0026#39;text\u0026#39;: ent.text, \u0026#39;label\u0026#39;: ent.label_, \u0026#39;start\u0026#39;: ent.start_char, \u0026#39;end\u0026#39;: ent.end_char } for ent in doc.ents ] return entities def classify_text(self, text): \u0026#34;\u0026#34;\u0026#34;Classify text into category\u0026#34;\u0026#34;\u0026#34; X = self.vectorizer.transform([text]) category = self.classifier.predict(X)[0] probabilities = self.classifier.predict_proba(X)[0] confidence = max(probabilities) return category, confidence def analyze_sentiment(self, text): \u0026#34;\u0026#34;\u0026#34;Analyze sentiment\u0026#34;\u0026#34;\u0026#34; blob = TextBlob(text) polarity = blob.sentiment.polarity if polarity \u0026gt; 0.1: sentiment = \u0026#34;positive\u0026#34; elif polarity \u0026lt; -0.1: sentiment = \u0026#34;negative\u0026#34; else: sentiment = \u0026#34;neutral\u0026#34; return sentiment, polarity def extract_keywords(self, text, top_n=5): \u0026#34;\u0026#34;\u0026#34;Extract key terms\u0026#34;\u0026#34;\u0026#34; doc = self.nlp(text) # Get noun phrases and important words keywords = [] for chunk in doc.noun_chunks: keywords.append(chunk.text) # Get named entities as keywords for ent in doc.ents: keywords.append(ent.text) # Remove duplicates and return top N keywords = list(set(keywords)) return keywords[:top_n] def analyze(self, text): \u0026#34;\u0026#34;\u0026#34;Complete analysis pipeline\u0026#34;\u0026#34;\u0026#34; # Preprocess clean_text = self.preprocess(text) # Run all analyses entities = self.extract_entities(clean_text) category, confidence = self.classify_text(clean_text) sentiment, polarity = self.analyze_sentiment(clean_text) keywords = self.extract_keywords(clean_text) return { \u0026#39;original_text\u0026#39;: text, \u0026#39;preprocessed_text\u0026#39;: clean_text, \u0026#39;category\u0026#39;: category, \u0026#39;category_confidence\u0026#39;: round(confidence, 3), \u0026#39;sentiment\u0026#39;: sentiment, \u0026#39;sentiment_polarity\u0026#39;: round(polarity, 3), \u0026#39;entities\u0026#39;: entities, \u0026#39;keywords\u0026#39;: keywords } # Test comprehensive pipeline pipeline = ComprehensiveNLPPipeline() test_text = \u0026#34;\u0026#34;\u0026#34; Apple Inc. announced the new iPhone 15 today in Cupertino, California. CEO Tim Cook praised the innovative features and excellent performance. The product received overwhelmingly positive reviews from tech journalists. \u0026#34;\u0026#34;\u0026#34; result = pipeline.analyze(test_text) print(\u0026#34;=== Comprehensive NLP Analysis ===\\n\u0026#34;) print(f\u0026#34;Original Text: {result[\u0026#39;original_text\u0026#39;]}\\n\u0026#34;) print(f\u0026#34;Category: {result[\u0026#39;category\u0026#39;]} (confidence: {result[\u0026#39;category_confidence\u0026#39;]})\u0026#34;) print(f\u0026#34;Sentiment: {result[\u0026#39;sentiment\u0026#39;]} (polarity: {result[\u0026#39;sentiment_polarity\u0026#39;]})\u0026#34;) print(f\u0026#34;\\nEntities:\u0026#34;) for ent in result[\u0026#39;entities\u0026#39;]: print(f\u0026#34; - {ent[\u0026#39;text\u0026#39;]} ({ent[\u0026#39;label\u0026#39;]})\u0026#34;) print(f\u0026#34;\\nKeywords: {\u0026#39;, \u0026#39;.join(result[\u0026#39;keywords\u0026#39;])}\u0026#34;) Lab 2: RESTful NLP API with Error Handling from fastapi import FastAPI, HTTPException, Request from fastapi.responses import JSONResponse from pydantic import BaseModel, validator from datetime import datetime import logging import time # Configure logging logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) app = FastAPI( title=\u0026#34;Comprehensive NLP API\u0026#34;, description=\u0026#34;Multi-component NLP analysis service\u0026#34;, version=\u0026#34;1.0.0\u0026#34; ) # Initialize pipeline nlp_pipeline = ComprehensiveNLPPipeline() class AnalysisRequest(BaseModel): text: str include_entities: bool = True include_sentiment: bool = True include_classification: bool = True include_keywords: bool = True @validator(\u0026#39;text\u0026#39;) def validate_text(cls, v): if not v or len(v.strip()) == 0: raise ValueError(\u0026#39;Text cannot be empty\u0026#39;) if len(v) \u0026gt; 10000: raise ValueError(\u0026#39;Text too long (max 10000 characters)\u0026#39;) return v class AnalysisResponse(BaseModel): text: str category: str = None category_confidence: float = None sentiment: str = None sentiment_polarity: float = None entities: list = [] keywords: list = [] processing_time: float timestamp: str # Middleware for logging @app.middleware(\u0026#34;http\u0026#34;) async def log_requests(request: Request, call_next): start_time = time.time() # Log request logger.info(f\u0026#34;Request: {request.method} {request.url.path}\u0026#34;) # Process request response = await call_next(request) # Log response process_time = time.time() - start_time logger.info(f\u0026#34;Completed in {process_time:.2f}s with status {response.status_code}\u0026#34;) return response @app.exception_handler(Exception) async def global_exception_handler(request: Request, exc: Exception): logger.error(f\u0026#34;Unhandled exception: {exc}\u0026#34;) return JSONResponse( status_code=500, content={ \u0026#34;error\u0026#34;: \u0026#34;Internal server error\u0026#34;, \u0026#34;message\u0026#34;: str(exc), \u0026#34;timestamp\u0026#34;: datetime.utcnow().isoformat() } ) @app.post(\u0026#34;/analyze\u0026#34;, response_model=AnalysisResponse) async def analyze_text(request: AnalysisRequest): \u0026#34;\u0026#34;\u0026#34;Comprehensive text analysis\u0026#34;\u0026#34;\u0026#34; try: start_time = time.time() # Run analysis result = nlp_pipeline.analyze(request.text) # Build response based on requested components response_data = { \u0026#39;text\u0026#39;: request.text, \u0026#39;processing_time\u0026#39;: time.time() - start_time, \u0026#39;timestamp\u0026#39;: datetime.utcnow().isoformat() + \u0026#34;Z\u0026#34; } if request.include_classification: response_data[\u0026#39;category\u0026#39;] = result[\u0026#39;category\u0026#39;] response_data[\u0026#39;category_confidence\u0026#39;] = result[\u0026#39;category_confidence\u0026#39;] if request.include_sentiment: response_data[\u0026#39;sentiment\u0026#39;] = result[\u0026#39;sentiment\u0026#39;] response_data[\u0026#39;sentiment_polarity\u0026#39;] = result[\u0026#39;sentiment_polarity\u0026#39;] if request.include_entities: response_data[\u0026#39;entities\u0026#39;] = result[\u0026#39;entities\u0026#39;] if request.include_keywords: response_data[\u0026#39;keywords\u0026#39;] = result[\u0026#39;keywords\u0026#39;] return response_data except Exception as e: logger.error(f\u0026#34;Analysis failed: {e}\u0026#34;) raise HTTPException(status_code=500, detail=f\u0026#34;Analysis failed: {str(e)}\u0026#34;) @app.get(\u0026#34;/health\u0026#34;) async def health_check(): \u0026#34;\u0026#34;\u0026#34;Health check endpoint\u0026#34;\u0026#34;\u0026#34; return { \u0026#34;status\u0026#34;: \u0026#34;healthy\u0026#34;, \u0026#34;timestamp\u0026#34;: datetime.utcnow().isoformat(), \u0026#34;components\u0026#34;: { \u0026#34;spacy\u0026#34;: \u0026#34;loaded\u0026#34;, \u0026#34;classifier\u0026#34;: \u0026#34;ready\u0026#34;, \u0026#34;sentiment\u0026#34;: \u0026#34;ready\u0026#34; } } @app.get(\u0026#34;/metrics\u0026#34;) async def get_metrics(): \u0026#34;\u0026#34;\u0026#34;System metrics\u0026#34;\u0026#34;\u0026#34; return { \u0026#34;timestamp\u0026#34;: datetime.utcnow().isoformat(), \u0026#34;uptime\u0026#34;: \u0026#34;available\u0026#34;, \u0026#34;requests_processed\u0026#34;: \u0026#34;tracked in production\u0026#34; } # To run: uvicorn script_name:app --reload Lab 3: Batch Processing System import json import time from concurrent.futures import ThreadPoolExecutor, as_completed from pathlib import Path class BatchNLPProcessor: \u0026#34;\u0026#34;\u0026#34;Process multiple texts efficiently\u0026#34;\u0026#34;\u0026#34; def __init__(self, pipeline, max_workers=4): self.pipeline = pipeline self.max_workers = max_workers def process_single(self, text_id, text): \u0026#34;\u0026#34;\u0026#34;Process single text\u0026#34;\u0026#34;\u0026#34; try: result = self.pipeline.analyze(text) return { \u0026#39;id\u0026#39;: text_id, \u0026#39;status\u0026#39;: \u0026#39;success\u0026#39;, \u0026#39;result\u0026#39;: result } except Exception as e: return { \u0026#39;id\u0026#39;: text_id, \u0026#39;status\u0026#39;: \u0026#39;error\u0026#39;, \u0026#39;error\u0026#39;: str(e) } def process_batch(self, texts, show_progress=True): \u0026#34;\u0026#34;\u0026#34;Process multiple texts in parallel\u0026#34;\u0026#34;\u0026#34; results = [] total = len(texts) with ThreadPoolExecutor(max_workers=self.max_workers) as executor: # Submit all tasks future_to_id = { executor.submit(self.process_single, i, text): i for i, text in enumerate(texts) } # Collect results completed = 0 for future in as_completed(future_to_id): result = future.result() results.append(result) completed += 1 if show_progress: print(f\u0026#34;Progress: {completed}/{total} ({completed/total*100:.1f}%)\u0026#34;) return results def process_file(self, input_file, output_file): \u0026#34;\u0026#34;\u0026#34;Process texts from file\u0026#34;\u0026#34;\u0026#34; # Read input with open(input_file, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: data = json.load(f) texts = data if isinstance(data, list) else [data] # Process results = self.process_batch(texts) # Write output with open(output_file, \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: json.dump(results, f, indent=2, ensure_ascii=False) return len(results) # Test batch processing batch_processor = BatchNLPProcessor(pipeline, max_workers=4) test_texts = [ \u0026#34;Apple released new products today.\u0026#34;, \u0026#34;The election results surprised many analysts.\u0026#34;, \u0026#34;The team won the championship game.\u0026#34;, \u0026#34;Stock markets reached new highs.\u0026#34;, \u0026#34;Scientists discovered a new species.\u0026#34; ] * 5 # 25 texts total print(\u0026#34;Starting batch processing...\u0026#34;) start_time = time.time() results = batch_processor.process_batch(test_texts) elapsed = time.time() - start_time print(f\u0026#34;\\nCompleted {len(results)} texts in {elapsed:.2f}s\u0026#34;) print(f\u0026#34;Average: {elapsed/len(results):.3f}s per text\u0026#34;) # Show sample results print(\u0026#34;\\nSample Results:\u0026#34;) for result in results[:3]: if result[\u0026#39;status\u0026#39;] == \u0026#39;success\u0026#39;: print(f\u0026#34;\\nText {result[\u0026#39;id\u0026#39;]}:\u0026#34;) print(f\u0026#34; Category: {result[\u0026#39;result\u0026#39;][\u0026#39;category\u0026#39;]}\u0026#34;) print(f\u0026#34; Sentiment: {result[\u0026#39;result\u0026#39;][\u0026#39;sentiment\u0026#39;]}\u0026#34;) Lab 4: Complete NLP Application with Web Interface from fastapi import FastAPI from fastapi.staticfiles import StaticFiles from fastapi.responses import HTMLResponse import uvicorn app = FastAPI() # HTML template for web interface HTML_TEMPLATE = \u0026#34;\u0026#34;\u0026#34; \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;NLP Analysis Tool\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { font-family: Arial, sans-serif; max-width: 800px; margin: 50px auto; padding: 20px; } textarea { width: 100%; height: 150px; padding: 10px; font-size: 14px; } button { background-color: #4CAF50; color: white; padding: 10px 20px; border: none; cursor: pointer; font-size: 16px; margin-top: 10px; } button:hover { background-color: #45a049; } #results { margin-top: 20px; padding: 20px; background-color: #f5f5f5; border-radius: 5px; } .result-item { margin: 10px 0; } .label { font-weight: bold; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;📝 NLP Analysis Tool\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Enter text below to analyze:\u0026lt;/p\u0026gt; \u0026lt;textarea id=\u0026#34;textInput\u0026#34; placeholder=\u0026#34;Enter your text here...\u0026#34;\u0026gt;\u0026lt;/textarea\u0026gt; \u0026lt;div\u0026gt; \u0026lt;label\u0026gt;\u0026lt;input type=\u0026#34;checkbox\u0026#34; id=\u0026#34;entities\u0026#34; checked\u0026gt; Named Entities\u0026lt;/label\u0026gt; \u0026lt;label\u0026gt;\u0026lt;input type=\u0026#34;checkbox\u0026#34; id=\u0026#34;sentiment\u0026#34; checked\u0026gt; Sentiment\u0026lt;/label\u0026gt; \u0026lt;label\u0026gt;\u0026lt;input type=\u0026#34;checkbox\u0026#34; id=\u0026#34;classification\u0026#34; checked\u0026gt; Classification\u0026lt;/label\u0026gt; \u0026lt;label\u0026gt;\u0026lt;input type=\u0026#34;checkbox\u0026#34; id=\u0026#34;keywords\u0026#34; checked\u0026gt; Keywords\u0026lt;/label\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;button onclick=\u0026#34;analyzeText()\u0026#34;\u0026gt;Analyze\u0026lt;/button\u0026gt; \u0026lt;div id=\u0026#34;results\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;script\u0026gt; async function analyzeText() { const text = document.getElementById(\u0026#39;textInput\u0026#39;).value; if (!text.trim()) { alert(\u0026#39;Please enter some text\u0026#39;); return; } const request = { text: text, include_entities: document.getElementById(\u0026#39;entities\u0026#39;).checked, include_sentiment: document.getElementById(\u0026#39;sentiment\u0026#39;).checked, include_classification: document.getElementById(\u0026#39;classification\u0026#39;).checked, include_keywords: document.getElementById(\u0026#39;keywords\u0026#39;).checked }; try { const response = await fetch(\u0026#39;/analyze\u0026#39;, { method: \u0026#39;POST\u0026#39;, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; }, body: JSON.stringify(request) }); const data = await response.json(); displayResults(data); } catch (error) { document.getElementById(\u0026#39;results\u0026#39;).innerHTML = \u0026#39;\u0026lt;p style=\u0026#34;color: red;\u0026#34;\u0026gt;Error: \u0026#39; + error.message + \u0026#39;\u0026lt;/p\u0026gt;\u0026#39;; } } function displayResults(data) { let html = \u0026#39;\u0026lt;h2\u0026gt;Analysis Results\u0026lt;/h2\u0026gt;\u0026#39;; if (data.category) { html += `\u0026lt;div class=\u0026#34;result-item\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;label\u0026#34;\u0026gt;Category:\u0026lt;/span\u0026gt; ${data.category} (${(data.category_confidence * 100).toFixed(1)}% confidence) \u0026lt;/div\u0026gt;`; } if (data.sentiment) { html += `\u0026lt;div class=\u0026#34;result-item\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;label\u0026#34;\u0026gt;Sentiment:\u0026lt;/span\u0026gt; ${data.sentiment} (polarity: ${data.sentiment_polarity.toFixed(2)}) \u0026lt;/div\u0026gt;`; } if (data.entities \u0026amp;\u0026amp; data.entities.length \u0026gt; 0) { html += \u0026#39;\u0026lt;div class=\u0026#34;result-item\u0026#34;\u0026gt;\u0026lt;span class=\u0026#34;label\u0026#34;\u0026gt;Entities:\u0026lt;/span\u0026gt;\u0026lt;ul\u0026gt;\u0026#39;; data.entities.forEach(ent =\u0026gt; { html += `\u0026lt;li\u0026gt;${ent.text} (${ent.label})\u0026lt;/li\u0026gt;`; }); html += \u0026#39;\u0026lt;/ul\u0026gt;\u0026lt;/div\u0026gt;\u0026#39;; } if (data.keywords \u0026amp;\u0026amp; data.keywords.length \u0026gt; 0) { html += `\u0026lt;div class=\u0026#34;result-item\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;label\u0026#34;\u0026gt;Keywords:\u0026lt;/span\u0026gt; ${data.keywords.join(\u0026#39;, \u0026#39;)} \u0026lt;/div\u0026gt;`; } html += `\u0026lt;div class=\u0026#34;result-item\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;label\u0026#34;\u0026gt;Processing Time:\u0026lt;/span\u0026gt; ${(data.processing_time * 1000).toFixed(2)}ms \u0026lt;/div\u0026gt;`; document.getElementById(\u0026#39;results\u0026#39;).innerHTML = html; } \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; \u0026#34;\u0026#34;\u0026#34; @app.get(\u0026#34;/\u0026#34;, response_class=HTMLResponse) async def get_interface(): \u0026#34;\u0026#34;\u0026#34;Serve web interface\u0026#34;\u0026#34;\u0026#34; return HTML_TEMPLATE # Include analysis endpoint from Lab 2 # ... (paste the analyze endpoint here) if __name__ == \u0026#34;__main__\u0026#34;: uvicorn.run(app, host=\u0026#34;0.0.0.0\u0026#34;, port=8000) Lab 5: Week 9 Final Project - Complete NLP System import json from datetime import datetime from pathlib import Path class NLPSystemManager: \u0026#34;\u0026#34;\u0026#34;Manage complete NLP system with all components\u0026#34;\u0026#34;\u0026#34; def __init__(self, config_file=None): self.pipeline = ComprehensiveNLPPipeline() self.batch_processor = BatchNLPProcessor(self.pipeline) self.stats = { \u0026#39;total_processed\u0026#39;: 0, \u0026#39;successful\u0026#39;: 0, \u0026#39;failed\u0026#39;: 0, \u0026#39;start_time\u0026#39;: datetime.now().isoformat() } if config_file: self.load_config(config_file) def load_config(self, config_file): \u0026#34;\u0026#34;\u0026#34;Load configuration\u0026#34;\u0026#34;\u0026#34; with open(config_file, \u0026#39;r\u0026#39;) as f: self.config = json.load(f) def process_request(self, text, options=None): \u0026#34;\u0026#34;\u0026#34;Process single request\u0026#34;\u0026#34;\u0026#34; try: result = self.pipeline.analyze(text) self.stats[\u0026#39;total_processed\u0026#39;] += 1 self.stats[\u0026#39;successful\u0026#39;] += 1 return {\u0026#39;status\u0026#39;: \u0026#39;success\u0026#39;, \u0026#39;data\u0026#39;: result} except Exception as e: self.stats[\u0026#39;total_processed\u0026#39;] += 1 self.stats[\u0026#39;failed\u0026#39;] += 1 return {\u0026#39;status\u0026#39;: \u0026#39;error\u0026#39;, \u0026#39;error\u0026#39;: str(e)} def get_statistics(self): \u0026#34;\u0026#34;\u0026#34;Get system statistics\u0026#34;\u0026#34;\u0026#34; success_rate = (self.stats[\u0026#39;successful\u0026#39;] / self.stats[\u0026#39;total_processed\u0026#39;] * 100 if self.stats[\u0026#39;total_processed\u0026#39;] \u0026gt; 0 else 0) return { \u0026#39;total_processed\u0026#39;: self.stats[\u0026#39;total_processed\u0026#39;], \u0026#39;successful\u0026#39;: self.stats[\u0026#39;successful\u0026#39;], \u0026#39;failed\u0026#39;: self.stats[\u0026#39;failed\u0026#39;], \u0026#39;success_rate\u0026#39;: f\u0026#34;{success_rate:.2f}%\u0026#34;, \u0026#39;uptime_since\u0026#39;: self.stats[\u0026#39;start_time\u0026#39;] } def export_report(self, output_file): \u0026#34;\u0026#34;\u0026#34;Export system report\u0026#34;\u0026#34;\u0026#34; report = { \u0026#39;timestamp\u0026#39;: datetime.now().isoformat(), \u0026#39;statistics\u0026#39;: self.get_statistics(), \u0026#39;configuration\u0026#39;: getattr(self, \u0026#39;config\u0026#39;, {}) } with open(output_file, \u0026#39;w\u0026#39;) as f: json.dump(report, f, indent=2) # Initialize complete system system = NLPSystemManager() # Process some texts test_texts = [ \u0026#34;Apple announced new products\u0026#34;, \u0026#34;Political debate heats up\u0026#34;, \u0026#34;Team wins championship\u0026#34; ] print(\u0026#34;=== NLP System Final Project ===\\n\u0026#34;) print(\u0026#34;Processing texts...\u0026#34;) for i, text in enumerate(test_texts, 1): result = system.process_request(text) print(f\u0026#34;\\n{i}. {text}\u0026#34;) if result[\u0026#39;status\u0026#39;] == \u0026#39;success\u0026#39;: data = result[\u0026#39;data\u0026#39;] print(f\u0026#34; Category: {data[\u0026#39;category\u0026#39;]}\u0026#34;) print(f\u0026#34; Sentiment: {data[\u0026#39;sentiment\u0026#39;]}\u0026#34;) print(f\u0026#34; Entities: {len(data[\u0026#39;entities\u0026#39;])}\u0026#34;) else: print(f\u0026#34; Error: {result[\u0026#39;error\u0026#39;]}\u0026#34;) # Show statistics print(\u0026#34;\\n\u0026#34; + \u0026#34;=\u0026#34;*50) print(\u0026#34;System Statistics:\u0026#34;) stats = system.get_statistics() for key, value in stats.items(): print(f\u0026#34; {key}: {value}\u0026#34;) print(\u0026#34;\\nWeek 9 Complete - NLP Revision \u0026amp; Integration Successful!\u0026#34;) Practice Exercises Build a complete NLP service with Docker deployment Create a multi-language support system Implement caching for improved performance Build a monitoring dashboard for NLP metrics Create comprehensive documentation and user guide Week 9 Summary Topics Covered Day 41: NLP fundamentals revision and advanced tokenization Day 42: Advanced text classification with ensemble methods Day 43: Production sentiment analysis systems Day 44: Custom NER training and entity linking Day 45: Complete NLP system integration Skills Acquired Deep understanding of NLP preprocessing techniques\nBuilding production-ready classification systems\nImplementing multi-component sentiment analysis\nTraining custom NER models for specific domains\nIntegrating NLP components into cohesive systems\nAPI design and deployment best practices\nPerformance optimization and monitoring\nKey Achievements Mastered complete NLP development lifecycle Built production-ready NLP applications Integrated multiple NLP components effectively Implemented robust error handling and monitoring Created comprehensive testing strategies "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.10-week10/1.10.5-day50-2025-11-14/","title":"Day 50 - Testing, Deployment &amp; Project Completion","tags":[],"description":"","content":"Date: 2025-11-14\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Testing Strategy Frontend Testing Approach Unit Testing:\nTest individual components in isolation Mock API calls and external dependencies Focus on business logic and user interactions Tools: Jest, React Testing Library Integration Testing:\nTest component interactions and data flow Verify API integration with mock servers Test authentication flows end-to-end Tools: Cypress, Playwright E2E Testing:\nSimulate real user workflows Test critical paths (upload, approve, read) Verify production environment behavior Tools: Cypress E2E, manual QA Testing Checklist Authentication Flow:\n✓ User can register with email ✓ Email verification works correctly ✓ User can login and receive JWT ✓ Protected routes redirect to login ✓ Admin role detection works ✓ Token refresh happens automatically Upload Flow:\n✓ File validation (size, type) works ✓ Presigned URL generation succeeds ✓ Direct S3 upload with progress tracking ✓ Status updates correctly (PENDING → VALIDATING) ✓ Error handling for failed uploads Admin Approval:\n✓ Only admins can access approval page ✓ Pending books list displays correctly ✓ Approve action moves file and updates status ✓ Reject action updates status with reason ✓ Audit logs record all actions Reader Experience:\n✓ Book list displays approved books only ✓ Search functionality returns accurate results ✓ Signed URL generation works ✓ PDF/ePub renders correctly in reader ✓ URL expiration handled gracefully Deployment Process Pre-Deployment Checklist Code Quality:\nAll TypeScript errors resolved ESLint warnings addressed Build succeeds without errors No console.log statements in production Environment Variables:\nAll required env vars set in Amplify Console Production API endpoint configured Cognito User Pool IDs correct CloudFront distribution ID set Security Review:\nS3 bucket policies restrict public access CloudFront OAC configured correctly API Gateway JWT validation enabled CORS configured for production domain only Amplify Deployment Automatic Deployment:\nGitHub push (main branch) → Amplify webhook triggers → Install dependencies → Build Next.js app → Deploy to CDN → Live at custom domain Build Settings:\nNode.js version specified (18.x) Build commands configured Environment variables injected Output directory set correctly Post-Deployment Verification:\nSite loads successfully SSL certificate active All routes accessible API calls working Authentication functional Performance Optimization Frontend Optimization Code Splitting:\nLazy load heavy components (PDF reader, ePub viewer) Dynamic imports for admin pages Route-based code splitting Image Optimization:\nNext.js Image component for book covers Lazy loading images below fold WebP format with fallbacks Caching Strategy:\nReact Query caching for API responses Service worker for offline capability (future) LocalStorage for user preferences Monitoring Setup CloudWatch Integration:\nFrontend error tracking via CloudWatch RUM API Gateway access logs Lambda execution logs Custom metrics for key actions Performance Metrics:\nPage load time tracking API response time monitoring Upload success/failure rates User engagement metrics Key Insights Comprehensive testing catches issues before users encounter them Automated CI/CD reduces deployment friction and human error Performance optimization is ongoing - monitor and iterate User feedback post-launch is invaluable for prioritizing improvements Documentation essential for onboarding and maintenance Tasks Completed Unit Testing\nWrote tests for authentication components Tested upload form validation logic Created tests for API client methods Tested custom hooks (useUser, useUpload) Integration Testing\nSet up Cypress test suite Wrote E2E tests for auth flow Created tests for upload workflow Tested admin approval process Bug Fixes\nFixed token refresh timing issue Resolved file upload progress accuracy Fixed admin panel permission checks Corrected PDF rendering on mobile Performance Optimization\nImplemented code splitting for reader components Added React Query caching with stale-while-revalidate Optimized image loading with Next.js Image Reduced bundle size by 30% Production Deployment\nConfigured Amplify build settings Set all environment variables in Amplify Console Deployed to production domain Verified SSL certificate and HTTPS Monitoring Setup\nEnabled CloudWatch RUM for frontend errors Configured CloudWatch alarms for critical metrics Set up API Gateway logging Created CloudWatch dashboard for monitoring Documentation\nUpdated README with setup instructions Documented API endpoints and contracts Created user guide for uploaders Wrote admin manual for approval workflow Challenges \u0026amp; Solutions Challenge: Cypress tests failing in CI/CD pipeline\nSolution: Configured proper base URL and added retry logic for network-dependent tests\nChallenge: Build failing due to environment variable issues\nSolution: Used NEXT_PUBLIC_ prefix for client-side env vars and documented all required variables\nChallenge: PDF rendering causing memory issues on mobile\nSolution: Implemented page-by-page rendering and disabled pre-caching on mobile devices\nChallenge: Race condition between token refresh and API calls\nSolution: Implemented request queue that waits for token refresh before proceeding\nWeek 10 Summary Project Completion Status Authentication \u0026amp; User Management\nCognito integration with Amplify UI JWT-based authentication Role-based access control (User vs Admin) Protected routes and authorization Upload Flow\nFile validation and presigned URL generation Direct S3 upload with progress tracking Status tracking (PENDING, VALIDATING, APPROVED, REJECTED) User upload history Admin Approval Workflow\nAdmin-only access to approval interface Pending books review with preview Approve/reject with audit logging Status badge and notification system Reader Interface\nBook detail pages with metadata Signed URL generation for secure access PDF and ePub rendering Fullscreen reader mode Search \u0026amp; Discovery\nSearch by title and author using DynamoDB GSI Filter and sort options Browse all approved books Related books suggestions Testing \u0026amp; Quality Assurance\nUnit tests for critical components Integration tests for main workflows E2E tests with Cypress Bug fixes and error handling Deployment\nCI/CD with Amplify Hosting Production environment configured Custom domain with SSL Monitoring and logging setup Technical Achievements Serverless Architecture: Built entirely on AWS managed services Cost-Effective: Estimated $9.80/month for MVP (100 users) Secure: Presigned URLs, CloudFront OAC, JWT authentication Scalable: Can handle 5,000-50,000 users with minimal changes Modern Stack: Next.js 14, TypeScript, TailwindCSS, React Query Lessons Learned Start with vertical slices - Building feature-by-feature enabled faster feedback Security first - Implementing presigned URLs and OAC early prevented vulnerabilities Test continuously - Automated tests caught issues before production Monitor everything - CloudWatch integration essential for troubleshooting Document as you build - Documentation saved time during testing and deployment Next Steps (Future Enhancements) Implement bookmark and reading progress tracking Add book categories and advanced filtering Build recommendation system based on reading history Add commenting and rating features Implement batch upload for admins Add analytics dashboard for usage statistics Mobile app version (React Native) Online Library Project Completed Successfully!\nTotal Development Time: 2 weeks (Days 46-50)\nLines of Code: ~5,000+ frontend + backend configurations\nFeatures Delivered: 15+ user-facing features\nAWS Services Used: 10 services (Amplify, Cognito, API Gateway, Lambda, S3, CloudFront, DynamoDB, Route 53, CloudWatch, CodePipeline)\n"},{"uri":"https://anquoc211.github.io/AWS_Internship/5-workshop/5.5-module5/","title":"Password Management","tags":[],"description":"","content":"Forgot Password Flow Create src/components/ForgotPassword.js:\nimport { Auth } from \u0026#39;aws-amplify\u0026#39;; import { useState } from \u0026#39;react\u0026#39;; function ForgotPassword() { const [email, setEmail] = useState(\u0026#39;\u0026#39;); const [code, setCode] = useState(\u0026#39;\u0026#39;); const [newPassword, setNewPassword] = useState(\u0026#39;\u0026#39;); const [step, setStep] = useState(\u0026#39;request\u0026#39;); const handleRequestReset = async (e) =\u0026gt; { e.preventDefault(); try { await Auth.forgotPassword(email); setStep(\u0026#39;reset\u0026#39;); alert(\u0026#39;Verification code sent to your email\u0026#39;); } catch (error) { console.error(\u0026#39;Error:\u0026#39;, error); alert(error.message); } }; const handleResetPassword = async (e) =\u0026gt; { e.preventDefault(); try { await Auth.forgotPasswordSubmit(email, code, newPassword); alert(\u0026#39;Password reset successfully!\u0026#39;); setStep(\u0026#39;complete\u0026#39;); } catch (error) { console.error(\u0026#39;Error:\u0026#39;, error); alert(error.message); } }; if (step === \u0026#39;request\u0026#39;) { return ( \u0026lt;form onSubmit={handleRequestReset}\u0026gt; \u0026lt;h2\u0026gt;Forgot Password\u0026lt;/h2\u0026gt; \u0026lt;input type=\u0026#34;email\u0026#34; value={email} onChange={(e) =\u0026gt; setEmail(e.target.value)} placeholder=\u0026#34;Email\u0026#34; required /\u0026gt; \u0026lt;button type=\u0026#34;submit\u0026#34;\u0026gt;Send Code\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; ); } if (step === \u0026#39;reset\u0026#39;) { return ( \u0026lt;form onSubmit={handleResetPassword}\u0026gt; \u0026lt;h2\u0026gt;Reset Password\u0026lt;/h2\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; value={code} onChange={(e) =\u0026gt; setCode(e.target.value)} placeholder=\u0026#34;Verification Code\u0026#34; required /\u0026gt; \u0026lt;input type=\u0026#34;password\u0026#34; value={newPassword} onChange={(e) =\u0026gt; setNewPassword(e.target.value)} placeholder=\u0026#34;New Password\u0026#34; required /\u0026gt; \u0026lt;button type=\u0026#34;submit\u0026#34;\u0026gt;Reset Password\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; ); } return \u0026lt;div\u0026gt;Password reset complete! \u0026lt;a href=\u0026#34;/login\u0026#34;\u0026gt;Go to Login\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt;; } export default ForgotPassword; Change Password Create src/components/ChangePassword.js:\nimport { Auth } from \u0026#39;aws-amplify\u0026#39;; import { useState } from \u0026#39;react\u0026#39;; function ChangePassword() { const [oldPassword, setOldPassword] = useState(\u0026#39;\u0026#39;); const [newPassword, setNewPassword] = useState(\u0026#39;\u0026#39;); const handleChangePassword = async (e) =\u0026gt; { e.preventDefault(); try { const user = await Auth.currentAuthenticatedUser(); await Auth.changePassword(user, oldPassword, newPassword); alert(\u0026#39;Password changed successfully!\u0026#39;); setOldPassword(\u0026#39;\u0026#39;); setNewPassword(\u0026#39;\u0026#39;); } catch (error) { console.error(\u0026#39;Error:\u0026#39;, error); alert(error.message); } }; return ( \u0026lt;form onSubmit={handleChangePassword}\u0026gt; \u0026lt;h2\u0026gt;Change Password\u0026lt;/h2\u0026gt; \u0026lt;input type=\u0026#34;password\u0026#34; value={oldPassword} onChange={(e) =\u0026gt; setOldPassword(e.target.value)} placeholder=\u0026#34;Current Password\u0026#34; required /\u0026gt; \u0026lt;input type=\u0026#34;password\u0026#34; value={newPassword} onChange={(e) =\u0026gt; setNewPassword(e.target.value)} placeholder=\u0026#34;New Password\u0026#34; required /\u0026gt; \u0026lt;button type=\u0026#34;submit\u0026#34;\u0026gt;Change Password\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; ); } export default ChangePassword; Password Policy Configure password requirements in Amplify:\namplify update auth # Choose: Walkthrough all the auth configurations # Password policy: Custom # Minimum length: 8 # Require lowercase: Yes # Require uppercase: Yes # Require numbers: Yes # Require symbols: Yes Deploy changes:\namplify push Password Validation Add client-side validation:\nconst validatePassword = (password) =\u0026gt; { const minLength = 8; const hasUpperCase = /[A-Z]/.test(password); const hasLowerCase = /[a-z]/.test(password); const hasNumbers = /\\d/.test(password); const hasSymbols = /[!@#$%^\u0026amp;*(),.?\u0026#34;:{}|\u0026lt;\u0026gt;]/.test(password); if (password.length \u0026lt; minLength) { return \u0026#39;Password must be at least 8 characters\u0026#39;; } if (!hasUpperCase) { return \u0026#39;Password must contain uppercase letter\u0026#39;; } if (!hasLowerCase) { return \u0026#39;Password must contain lowercase letter\u0026#39;; } if (!hasNumbers) { return \u0026#39;Password must contain number\u0026#39;; } if (!hasSymbols) { return \u0026#39;Password must contain special character\u0026#39;; } return null; }; "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.5-week5/","title":"Week 5 - AWS Security &amp; Identity","tags":[],"description":"","content":"Week: 2025-10-06 đến 2025-10-10\nStatus: \u0026ldquo;Done\u0026rdquo;\nTổng quan tuần 5 Tuần này tập trung vào bảo mật và quản lý danh tính trên AWS.\nNội dung chính Shared Responsibility Model AWS IAM (Users, Groups, Roles, Policies) Amazon Cognito AWS Organizations \u0026amp; SCPs AWS Identity Center (SSO) AWS KMS AWS Security Hub Labs thực hành Lab 18: AWS Security Hub Lab 22: AWS Lambda Automation with Slack Lab 27: AWS Resource Groups \u0026amp; Tagging Lab 28: IAM Cross-Region Role \u0026amp; Policy Lab 30: IAM Restriction Policy Lab 33: AWS KMS \u0026amp; CloudTrail Integration Lab 44: IAM Advanced Role Control Lab 48: IAM Access Keys \u0026amp; Roles "},{"uri":"https://anquoc211.github.io/AWS_Internship/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Authentication Workshop: AWS Amplify \u0026amp; Amazon Cognito Overview Hands-on workshop introducing user authentication fundamentals with AWS Amplify and Amazon Cognito. Participants build a complete authentication system including user registration, email verification, login/logout, password management, multi-factor authentication (MFA), and social sign-in. Focus on implementing secure authentication quickly without managing infrastructure, following AWS best practices.\nAgenda Authentication foundations: why Amplify \u0026amp; Cognito, security best practices, use cases. Getting started: install Amplify CLI, initialize React app, add authentication category. User registration: build sign-up flow, email verification, custom attributes, password policies. Login \u0026amp; sessions: implement authentication, handle JWT tokens, manage user sessions, logout properly. Password management: forgot password flow, password reset, change password, security requirements. Advanced features: enable MFA (TOTP), add social sign-in (Google/Facebook), user groups \u0026amp; roles. Deployment: host with Amplify Hosting, configure CI/CD, custom domains, monitoring. What You\u0026rsquo;ll Build React application with complete authentication UI (registration, login, profile). Amazon Cognito User Pool with email verification and custom password policies. Multi-factor authentication using TOTP (Time-based One-Time Password). Social sign-in integration with Google and Facebook. Protected routes with role-based access control (RBAC). Prerequisites AWS account with console access. Node.js 18+ and npm installed. Basic familiarity with JavaScript/React. Amplify CLI installed (npm install -g @aws-amplify/cli). References Amplify CLI configuration steps Cognito User Pool setup guide Social identity provider integration Cleanup checklist "},{"uri":"https://anquoc211.github.io/AWS_Internship/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✔️ 2 Ability to learn Ability to absorb new knowledge and learn quickly ✔️ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✔️ 4 Sense of responsibility Completing tasks on time and ensuring quality ✔️ 5 Discipline Adhering to schedules, rules, and work processes ✔️ 6 Progressive mindset Willingness to receive feedback and improve oneself ✔️ 7 Communication Presenting ideas and reporting work clearly ✔️ 8 Teamwork Working effectively with colleagues and participating in teams ✔️ 9 Professional conduct Respecting colleagues, partners, and the work environment ✔️ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ✔️ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✔️ 12 Overall General evaluation of the entire internship period ✔️ Areas for Improvement Technical Skills (Average):\nDeepen expertise through additional courses and certifications Apply knowledge to more complex real-world scenarios Increase proficiency with industry-standard tools Learning Speed (Fair):\nDevelop more efficient learning strategies Seek challenging projects outside comfort zone Build stronger self-directed learning habits Initiative (Average):\nTake more ownership without waiting for direction Proactively identify improvement opportunities Volunteer for additional responsibilities Adaptability (Fair):\nBe more receptive to constructive feedback Develop growth mindset for continuous improvement Actively seek input to identify blind spots Communication (Fair):\nImprove clarity when presenting technical concepts Enhance written documentation skills Practice active listening and asking clarifying questions Problem-Solving (Fair):\nStrengthen analytical and systematic thinking Develop creative approaches to challenges Build problem-solving frameworks toolkit Team Impact (Fair):\nIncrease quality and impact of deliverables Contribute more innovative ideas to discussions Take ownership of larger project components Action Plan To address these areas, I commit to:\nWeekly dedicated time for skill development through online courses Seeking mentorship from senior colleagues Setting specific, measurable improvement goals Regular progress reflection and strategy adjustment Active participation in team discussions with solutions "},{"uri":"https://anquoc211.github.io/AWS_Internship/5-workshop/5.6-module6/","title":"Advanced Features &amp; Cleanup","tags":[],"description":"","content":"Multi-Factor Authentication (MFA) Enable TOTP-based MFA:\namplify update auth # Choose: Walkthrough all the auth configurations # MFA: Optional # MFA type: TOTP (Time-based One-Time Password) amplify push Implement MFA in your app:\nimport { Auth } from \u0026#39;aws-amplify\u0026#39;; // Setup MFA for user async function setupMFA() { try { const user = await Auth.currentAuthenticatedUser(); const code = await Auth.setupTOTP(user); // Display QR code to user using code const qrCodeUrl = `otpauth://totp/AWSCognito:${user.username}?secret=${code}\u0026amp;issuer=AuthWorkshop`; console.log(\u0026#39;QR Code URL:\u0026#39;, qrCodeUrl); // Verify TOTP token const token = prompt(\u0026#39;Enter TOTP code from authenticator app:\u0026#39;); await Auth.verifyTotpToken(user, token); await Auth.setPreferredMFA(user, \u0026#39;TOTP\u0026#39;); alert(\u0026#39;MFA enabled successfully!\u0026#39;); } catch (error) { console.error(\u0026#39;Error setting up MFA:\u0026#39;, error); } } Social Sign-In Add Google/Facebook authentication:\namplify update auth # Choose: Walkthrough all the auth configurations # Social providers: Google, Facebook # Follow prompts to configure OAuth amplify push Add social login buttons:\nimport { Auth } from \u0026#39;aws-amplify\u0026#39;; function SocialLogin() { const handleGoogleLogin = () =\u0026gt; { Auth.federatedSignIn({ provider: \u0026#39;Google\u0026#39; }); }; const handleFacebookLogin = () =\u0026gt; { Auth.federatedSignIn({ provider: \u0026#39;Facebook\u0026#39; }); }; return ( \u0026lt;div\u0026gt; \u0026lt;button onClick={handleGoogleLogin}\u0026gt;Sign in with Google\u0026lt;/button\u0026gt; \u0026lt;button onClick={handleFacebookLogin}\u0026gt;Sign in with Facebook\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; ); } User Groups \u0026amp; Roles Create user groups in Cognito console:\nGo to Cognito User Pools Select your user pool Navigate to \u0026ldquo;Users and groups\u0026rdquo; \u0026ldquo;Groups\u0026rdquo; Create groups: Admin, User Assign users to groups via code:\n// Server-side code (Lambda) const AWS = require(\u0026#39;aws-sdk\u0026#39;); const cognito = new AWS.CognitoIdentityServiceProvider(); async function addUserToGroup(username, groupName, userPoolId) { const params = { GroupName: groupName, UserPoolId: userPoolId, Username: username }; await cognito.adminAddUserToGroup(params).promise(); } Deploy to Amplify Hosting Deploy your React app:\namplify add hosting # Choose: Amplify Console # Type: Manual deployment amplify publish Your app will be deployed to a live URL.\nCleanup Resources Remove all AWS resources:\n# Delete Amplify backend amplify delete # Confirm deletion # This will remove: # - Cognito User Pool # - All authentication resources # - CloudFormation stacks Remove local files:\n# Delete Amplify project files Remove-Item -Recurse -Force amplify Remove-Item -Force aws-exports.js Cost Optimization Cognito User Pools: Free tier covers 50,000 MAUs (Monthly Active Users) Amplify Hosting: Build minutes and data transfer have free tier limits Best Practice: Always delete unused resources Congratulations! You\u0026rsquo;ve completed the Authentication Workshop covering:\nUser registration and email verification Login and session management Password reset flows MFA with TOTP Social authentication User groups and roles Production deployment "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.6-week6/","title":"Week 6 - AWS Database Services","tags":[],"description":"","content":"Week: 2025-10-13 đến 2025-10-17\nStatus: \u0026ldquo;Done\u0026rdquo;\nTổng quan tuần 6 Tuần này tập trung vào các dịch vụ cơ sở dữ liệu của AWS, từ RDBMS đến NoSQL và Data Warehouse.\nNội dung chính Database Fundamentals (RDBMS, NoSQL, OLTP vs OLAP) Amazon RDS \u0026amp; Aurora Amazon Redshift Amazon ElastiCache AWS Database Migration Service (DMS) Labs thực hành Lab 05: Amazon RDS \u0026amp; EC2 Integration Lab 43: AWS Database Migration Service (DMS) "},{"uri":"https://anquoc211.github.io/AWS_Internship/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help when I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better.\n2. Support from Mentor / Team Admin\nThe mentor provides detailed guidance, explains clearly when I don\u0026rsquo;t understand, and always encourages questions. The admin team supports administrative tasks and provides necessary documents. I especially appreciate that the mentor allows me to try solving problems myself instead of just giving answers.\n3. Relevance of Work to Academic Major\nThe assigned tasks align well with university knowledge while introducing new areas. This allowed me to strengthen my foundation and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as project management tools, teamwork, and professional communication. The mentor also shared valuable real-world experiences that helped me plan my career path better.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but keeps things enjoyable. When projects are urgent, everyone works together regardless of position. This made me feel like a real team member, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. Additionally, joining internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship?\nI was most satisfied with the open learning environment and dedicated mentorship. Working on real projects and receiving detailed feedback helped me progress significantly. Especially, the culture of encouraging questions and experimentation helped me confidently develop my skills.\nIf recommending to a friend, would you suggest they intern here? Why or why not?\nI would definitely recommend friends to intern here. The main reasons are the practical learning environment and enthusiastic team. Interns don\u0026rsquo;t just do simple tasks but participate in real projects, receive detailed guidance, and have opportunities to develop both technical and soft skills. This is an ideal place for those who want to learn and develop seriously.\n"},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.7-week7/","title":"Week 7 - Vertical Slice Delivery","tags":[],"description":"","content":"Week: 2025-10-20 đến 2025-10-24\nStatus: \u0026ldquo;Done\u0026rdquo;\nTổng quan tuần 7 Tuần này hoàn thiện vertical slice 0 cho dự án Ebook Demo, tập trung vào contract-first development và tự động hóa kiểm thử.\nNội dung chính Vertical Slice Architecture \u0026amp; phạm vi slice 0 Contract-first development với OpenAPI + Prism mock Next.js 16 App Router \u0026amp; Server Components FastAPI clean architecture và cấu hình CORS Schemathesis contract testing \u0026amp; retrospective Labs thực hành Checklist demo vertical slice 0 Mock API với Prism và kết nối Next.js Refactor backend FastAPI theo clean architecture Chạy Schemathesis và cập nhật workflow chuẩn "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.8-week8/","title":"Week 8 - Learning Natural Language Processing","tags":[],"description":"","content":"Week: 2025-10-27 to 2025-10-31\nStatus: \u0026ldquo;Done\u0026rdquo;\nWeek 8 overview This week focuses on learning Natural Language Processing (NLP)\nMain topics Introduction to NLP: Understanding what Natural Language Processing is and its applications in real-world scenarios Text Preprocessing: Learning tokenization, stemming, lemmatization, and stop word removal Basic Text Analysis: Exploring word frequency, n-grams, and simple text statistics Sentiment Analysis: Introduction to analyzing emotions and opinions in text data Named Entity Recognition (NER): Identifying and classifying entities like names, locations, and organizations Popular NLP Libraries: Getting started with NLTK, spaCy, and basic usage examples Simple NLP Project: Building a basic text classifier or chatbot as a hands-on exercise "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.9-week9/","title":"Week 9 - NLP Revision &amp; Advanced Concepts","tags":[],"description":"","content":"Week: 2025-11-03 to 2025-11-07\nStatus: \u0026ldquo;Done\u0026rdquo;\nWeek 9 Overview This week focuses on revising Natural Language Processing concepts from Week 8 and exploring advanced applications.\nMain Topics NLP Fundamentals Review: Revisiting core concepts including tokenization, preprocessing techniques, and text normalization Text Classification Deep Dive: Advanced techniques for document classification and feature engineering Sentiment Analysis Applications: Building production-ready sentiment analysis systems NER Advanced Techniques: Custom entity training and domain-specific entity recognition NLP Pipeline Integration: Combining multiple NLP components into cohesive systems Real-world NLP Projects: Building end-to-end applications including chatbots and text analyzers Performance Optimization: Improving speed and accuracy of NLP models "},{"uri":"https://anquoc211.github.io/AWS_Internship/1-worklog/1.10-week10/","title":"Week 10 - Online Library Project Completion (Frontend)","tags":[],"description":"","content":"Week: 2025-11-10 to 2025-11-14\nStatus: \u0026ldquo;Done\u0026rdquo;\nWeek 10 Overview This week focuses on completing the Online Library frontend implementation, integrating with AWS services, and finalizing the serverless application.\nMain Topics Authentication Integration: Implementing Cognito-based login, registration, and user management with Amplify UI Upload Flow Development: Building the book upload interface with S3 presigned URLs and real-time status tracking Admin Panel Creation: Developing admin approval workflow, book management, and moderation features Reader Interface: Implementing the book reading experience with CloudFront signed URLs and PDF rendering Search \u0026amp; Discovery: Building search functionality using DynamoDB GSI and optimizing user experience Testing \u0026amp; Deployment: Final integration testing, bug fixes, and production deployment via Amplify CI/CD Performance Optimization: Implementing caching strategies and optimizing load times "},{"uri":"https://anquoc211.github.io/AWS_Internship/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://anquoc211.github.io/AWS_Internship/tags/","title":"Tags","tags":[],"description":"","content":""}]